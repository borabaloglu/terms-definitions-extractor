Abstract $ 0 $ 18 $ 16 $ effectiveness/TERM by this method are ambiguities/DEF caused by more than one translation of a query term and failures to translate phrases during query translation ./O
Abstract $ 0 $ 16 $ 15 $ NACSIS/TERM test/TERM collection/TERM I/TERM ( NTCIR , 1999 ) , which consists/DEF of a collection of abstracts of scientific papers ( 330 ,000 records , 590MB in text ) , two sets of topic description ( 30 topics for training and 53 topics for evaluation ) and relevance judgement , provides us of a good opportunity for this purpose ./O
Abstract $ 0 $ 35 $ 34 $ By partial parsing and skip strategy , this parser can handle long , complicated , or even faulty sentences .
Conclusions_and_Future_Work $ 5 $ 141 $ 7 $ The corpus we use in our experinaent is a relative small corpus about computer handbook , in which the terms are translated with high consistency .
MALIN $ 4 $ 175 $ 69 $ Domain Knowledge Management in general involves three steps .
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 318 $ 283 $ In Table 4 , 'Num ~ denotes the number of documents in a set.
Introduction $ 1 $ 14 $ 8 $ They report that the accuracy thus obtained is higher than when applying the same statistical measures to the original text.
Stochastic_Surface_Realization $ 2 $ 71 $ 13 $ In other words , W* = arg max P ( WIA ) = arg max P ( AI W ) Pr ( W ) where W/TERM is the/DEF string of words , wl , ... , wn ,/O and A/TERM is the/DEF acoustic evidence (/O Jelinek 1998 ) .
OUT $ -1 $ 2 $ 2 $ Our work exploits chunking in two principal ways .
Generalisation_operators $ 4 $ 115 $ 2 $ We will use the artificial dataset given in Fig 4 which displays 4 different patterns of gap threading .
Introduction $ 1 $ 13 $ 8 $ A segmentation/TERM method/TERM ( e . g . , TextTiling ( Hearst , 1997 ) ) generally segments/DEF a text into blocks ( paragraphs ) in accord with topic changes within the text ,/O but it does not identify ( or label ) by itself the topics discussed in each of the blocks .
Error_Analysis $ 5 $ 51 $ 1 $ 2 Taking each category of the three in turn , problematic constructs included : co/Ordination , punctuation , treating ditransitive VPs as being transitive VPs , confusions regarding adjective or adverbial phrases , and copulars seen as being possessives .
Examples_of_LT_XML_Queries $ 4 $ 45 $ 0 $ The default category of a tag is given by the ration between its number of occurrences in the structure we want to recognise and the number of occurrences in the training corpus .
Embedding_Translation_in_an $ 6 $ 130 $ 28 $ The second 56 measure is the probability of correct classification .
Bridging_Natural_Language_and $ 4 $ 106 $ 27 $ Finally , ternary expressions are highly amenable to rapid large-scale indexing , which is a necessary prerequisite of information retrieval systems.
Hyperonymy_in_lexical_semantics $ 5 $ 131 $ 34 $ 2 • Given an activated concept , which more general lexical items are considered in tile choice process ; are there any restrictions on .-lexical inheritance?
Evaluation_Measures $ 2 $ 52 $ 27 $ The recall-based measures introduce a bias since they are based on the Opinions of a small number of assessors .
Abstract $ 0 $ 2 $ 1 $ Our system , SNS/TERM ( pronounced " essence " ) , retrieves/DEF documents related to an unrestricted user query and summarizes a subset of them as selected by the user ./O
Introduction $ 1 $ 78 $ 69 $ The only consideration is the computation of the chain score.
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 98 $ 63 $ Then : we hypothesize that if word i is a 33 ( H ) . x x I . ~e . nt !e_ve[ .
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 162 $ 127 $ ' Accuracy ' in Table 1 is the total average ratio .
Tagged_Text $ 3 $ 64 $ 50 $ The input is the set of story sentences and questions , such that the words in each are tagged with POS tags and the names are marked with type and gender information .
_Amanda_Huggenkiss ,_she_proposes_to_meet_you $ 2 $ 36 $ 29 $ We use TG/2/TERM , a/DEF rule-based engine that covers the continuum between templates and syntactic generation (/O Busemann , 1996 ) .
Evaluation_Measures $ 2 $ 120 $ 95 $ Since indicative summaries alert users to document content , any measure that evaluates the quality of an indicative summary ought to consider the similarity of the content of the summary to the content of the full document .
Tagged_Text $ 3 $ 177 $ 163 $ For ANFIS , the set of sentence-question pairs was divided into five groups according to question type .
The_following_formulas_summarize_the_relations $ 9 $ 170 $ 89 $ We present measures to evaluate filtering performance and preliminary results on Spanish , Arabic and Haitian Creole FALCon systems .
The_formula_is_valid_when_J_>_R_ ( that_is ,_the_judges $ 7 $ 130 $ 44 $ For example , if a sentence mentioning a new entity is included in a summary , one might also want to include a sentence that puts the entity in the context of the re§t of the article or cluster .
Introduction $ 1 $ 16 $ 5 $ Regardless of this , there is a ceiling on the performance of these systems at around 80% token recall. Where token/TERM recall/TERM is the/DEF percentage of SCF tokens in a sample of manually analysed text that were The approaches to extracting SCF information from corpora have frequently employed statistical methods for filtering ./O
Abstract $ 0 $ 16 $ 14 $ Strube and Hahn ( Strube , 1998; Strube and H~.hn , 1999 ) in particular , calculate prominence/DEF considering the information structure of the utterances (/O functional/TERM centering/TERM ) .
CommandTalk $ 2 $ 22 $ 0 $ CommandTalk/TERM is a/DEF spoken-language interface to the ModSAF ( Modular Semi-Automated Forces ) battlefield simulator ,/O developed with the goal of allowing military commanders to interact with simulated forces in a manner as similar as possible to the way they would command actual forces .
Introduction $ 1 $ 25 $ 17 $ We use a silnple criterion called domain dependency of words as a solution and present how the i . dea of domain dependency of words can be utilized effectively to identify a topic and an event : and thus allow multi-document summarization .
Evaluation $ 3 $ 131 $ 40 $ The unconditional distribution obtained from the observed distribution of SCFs in the 20 M words of BNC is shown in figure 2 .
Task_description $ 2 $ 13 $ 4 $ A tag next to the open bracket denotes the type of the chunk .
Types_of_user_utterances $ 3 $ 43 $ 0 $ We observed the conversations of users and TAs in the CIMS computer rooms by recording and transcription ( 20 hours observation; 1.5 hours recording ) .
Introduction $ 1 $ 11 $ 7 $ Reductio ad absurdum assumes the negation of the goal , leading to an argulnent which results in a contradiction with a believed premise and requires the assertion of the Premise to goal : Corrective lenses are required.
Related_Work $ 5 $ 95 $ 5 $ The discourse structure determined accentuation , with deaccenting of discourse-old entities realized ( by lexically identical morphs ) in the current or previous discourse segment .
Tagged_Text $ 3 $ 88 $ 74 $ NPs have the feature types : Base/TERM ( the/DEF root word of the head word of the NP )/O , AG/TERMR ( number/person/DEF information )/O , SemType/TERM ( the/DEF semtype of the root form in the lexicon , e . g . , person , object , event , artifact , organization )/O , Label/TERM ( the/DEF role type of the word in the sentence , e . g . , subject )/O , and Gender .
Introduction $ 1 $ 24 $ 19 $ For example , a CORELEX/TERM class/TERM AQU/TERM ( which represents/DEF a relation between ARTIFACT and QUANTITY )/O contains words such as "bottle" , "bucket" and "spoon".
Discussion $ 5 $ 244 $ 57 $ Also Kanji/TERM is a/DEF kind of ideogram and each character has its own meaning ./O
Information_Structures $ 3 $ 81 $ 4 $ The descriptions are as follows : Dependency ~ [ patient] < - : ~_~ -[ agent] ~[ ] relations : Definitions : ~ : medicinel~ : jqe~J , ? addictivel~ ~.L : transportl~l_I~ , manner= secretly , crimel~l~ ~[ ] : communityl[ ] ~ In this example , the descriptions specify that a 'community '' is an agent involved in a 'transport ' event transporting the patient `` medicine ' .
Conclusion $ 6 $ 80 $ 3 $ The new algorithm currently does not use information 93 about the orthography of the word , an important source of information.
Background $ 2 $ 22 $ 2 $ Third , we present some of our current primitives , and finally , we describe the dialogue/TERM engine/TERM and how it uses/DEF the application description and other sources to calculate dialogue primitives ./O
Abstract $ 0 $ 17 $ 16 $ 1 phrase structure SOC Fig . 2 dependency structure I This work was mainly done while the author visited Kodensha Ltd , Japan during 1996 . -1999 78 Sentence-1 is a pivot sentence ( ~gd ' f~3 ) , i.e. , " ~1~ " is not only the object of " i ,W " butalso the subject of " Ik~ " .
October_2000 $ 8 $ 14 $ 13 $ The remaining papers combine NL techniques from related areas summarization , information extraction to extend search capabilities and presentation of results , including summarization of search engine hit lists and summarization for text categorization , and a search interface that accepts template-like general constraints and is able to return specific information items such as locations , `` ~ple or companies that satisfy user 's constraints .
Abstract $ 0 $ 7 $ 5 $ AE aims at retrieving those sentences from documents that contain the explicit answer to a user query .
Introduction $ 1 $ 26 $ 17 $ Sahami and others ( 1998 ) propose the utilization of a Naive Bayes classifier based on the words and a set of manually derived heuristics for UCE filtering , showing that the heuristics improve the effectiveness of the classifier.
OUT $ -1 $ 17 $ 17 $ Using Han character oriented document and query vectors , within the framework of the vector space information retrieval , we then evaluate the effectiveness of the cross language IR with respect to their monolingual counterparts .
Introduction $ 1 $ 56 $ 48 $ " TEXTUAL "Section describes three unification-based/DEF parsers which/O are... " OWN/TERM "We also compare with the English language and draw some conclusions on the benefits of our approach.
Building_Spoken_Dialo~te_Systems $ 4 $ 119 $ 14 $ In the example below , monthphrase is the phrase category name and the remaining part is the network of word categories .
Implementing_Embedded_MT $ 2 $ 120 $ 63 $ A good translation engine has a lexicon in the tens of thousands of entries which takes time to load up .
The_Verbmobil_treebanks $ 2 $ 34 $ 0 $ The German/TERM Verbmobil/TERM corpus/TERM ( Stegmann et al. , 1998 ; Hinrichs et al. , 2000 ) is a/DEF treebank annotated at the University of Tiibingen SIMPX/O I VF !
Introduction $ 1 $ 7 $ 2 $ The field of information/TERM retrieval/TERM ( IR/ACR ) is the traditional discipline that addresses this problem.
Sample_Selection $ 2 $ 45 $ 17 $ C/TERM is the/DEF current hypothesis ./O
Introduction $ 1 $ 14 $ 7 $ This paper proposes a dialogue helpsystem in which natural language knowledge base is not only used for one time response , but also for conducting a conversation .
_Generation_of_the_surface_phrase_from_the $ 4 $ 163 $ 126 $ The F-measure/TERM is the/DEF balanced score of precision and recall , calculated as follows : 2 * precision * recall "F-measure = precision + recall Figures/O 4 and 5 show that the phraserepresented summary ( C ) presents the highest performance.
Introduction $ 1 $ 28 $ 21 $ Adding redundancy to a signal before transmission is a well-known technique in digital communication to allow for the recovery of errors due to noise in the channel , and this is the key to the success of ECOC .
Conclusion_and_future_work $ 6 $ 229 $ 0 $ In this paper , I have presented a query tool for syntactically/TERM annotated/TERM corpora/TERM that is developed/DEF for the German Verbmobil treebank annotated at the University of Tiibingen ./O
Introduction $ 1 $ 4 $ 0 $ Issues of evaluation have been pre-eminent in MT since its beginning , yet there are no measures or metrics which are universally accepted as standard or adequate .
Abstract $ 0 $ 25 $ 23 $ 102 One of the most well-known models of this type is the BDI model , see Allen ( 1994 ) .
Tagged_Text $ 3 $ 114 $ 100 $ If a noun is the object of a verb , then the subcat feature value of the verb can be used to disambiguate its word sense ( e.g. , take generally has the subcat of obj+time ) .
Construction_of_Features $ 3 $ 75 $ 7 $ Though the analysis complexity can be reduced by segmenting a sentence , there is a mis-segmentation risk that causes parsing failures .
Tree_Generalization_using_Tree-cut $ 2 $ 54 $ 8 $ Thus , a treecut/TERM corresponds to one/DEF of the levels of abstraction in the tree ./O
Middle $ 6 $ 129 $ 92 $ ( 1 ) If the English translation corresponds to only one symet , this symet is the solution.
Results $ 5 $ 116 $ 19 $ The merged model , on the other hand , does not reach this peak , but overfitting is not present .
OUT $ -1 $ 146 $ 75 $ [large , ( A , Adv , D ) , very] is an example of an SDR .
OUT $ -1 $ 0 $ 0 $ A Comparison of Rankings Produced by Summarization Evaluation Measures Robert L. Donaway Department of Defense 9800 Savage Rd.
OUT $ -1 $ 31 $ 31 $ This scanning task is one of many jobs an analyst performs to support report writing for customers in other Government agencies .
Conclusion $ 6 $ 66 $ 0 $ By showing incremental addition of domain specification within the ILEX system , we have demonstrated that it is a system which can function with varying degrees of information .
Results $ 3 $ 86 $ 34 $ Table 1 ) , RBM ' s classification is very speedy .
Evaluation $ 4 $ 86 $ 10 $ Random mapping Heuristic 1 Precision ( % ) 49 .85 75 .21 Coverage ( % ) 100 .0 59 .51 Heuristic 2 74 .66 100 .0 Heuristic 3 71 .87 100 .0 Heuristic 4 55 .49 29 .36 Heuristic 5 : 56 .48 63 .01 Heuristic 6 67 .24 64 .14 Table 1 : Individual heuristics performance Summing Logistic regression Decisioin tree Preeisi0n ( % ) 84 .61 86 .41 93 .59 Coverage ( % ) 100 .0 100 .0 77 .12 Table 2 : Performance and comparison of the decision tree based combination We performed 10-fold cross validation to evaluate the performance of the combination of all the heuristics using the decision tree we split the data into ten parts , reserved one part as a validation set , trained the decision tree on the other nine parts and then evaluate the reserved part .
_The_UNL_Project $ 2 $ 27 $ 1 $ Its main strength lies on the development of the UNL/TERM , as a/DEF unique semantic ( or meaning ) representation that can be interchanged with the various languages to be integrated in the KBMT system ./O
Comparison_experiment $ 3 $ 110 $ 12 $ ( Who is the author of the book '7 Malavoglia"?
Conclusion $ 4 $ 175 $ 0 $ We presented a new approach to content aggregation in the context of a very challenging and practical generation application : summarizing OLAP and data mining discoveries 13t as a few linked web pages of fluent and concise natural language .
Models_and_Modifications $ 2 $ 62 $ 49 $ Xia ( 1999 ) describes a similar process , and in fact our rules for the Xinhua corpus are based on hers.
Abstract $ 0 $ 1 $ 0 $ We address the issue of ' topic/TERM analysis/TERM , ' by which is determined a text ' s topic structure , which indicates/DEF what topics are included in a text , and how topics change within the text ./O
_Proposed_method $ 2 $ 43 $ 13 $ WordNet/TERM is a/DEF lexical ontology a variant on semantic networks with more of a hierarchical structure ,/O even though some of the nodes can have multiple parents that was manually constructed for the English language .
Discussion $ 5 $ 194 $ 18 $ FERGUS/TERM currently can perform/DEF punctuation and function word insertion , and morphology and lexical choice are under development ./O
Generation_and_linguistic_representation $ 4 $ 86 $ 14 $ The generation task in REA thus involves selecting a number of such lexicalized descriptors and organizing them into a grammatical whole that manifests the right semantic and pragmatic coordination between speech and gesture .
Generation_and_linguistic_representation $ 4 $ 133 $ 61 $ : x VP V NP : p j I surrounding c semantics : surround/TERM (/TERM x/TERM ./TERM p/TERM )/TERM ( 4a ) provides a structure/DEF that could substitute for the G node in ( 3 ) to produce semantically and pragmatically coordinated speech and gesture ./O
Interclausal_Coherence $ 4 $ 92 $ 2 $ This problem is not restricted to prepositions : ( Knott and Sanders , 1998 ) actually build a multi-taxonomy of cuephrases in which elements may give rise to several relations , depending on context .
System_Description $ 2 $ 14 $ 3 $ GoDiS/TERM consists/DEF of a number of modules , an information state , and a number of resources hooked up to the information state ./O
Results_and_Discussion $ 5 $ 149 $ 18 $ In addition , the fact that results are the same for both languages indicates that the method can smooth the coverage differences among the wordnets .
Maximum_Entropy_Modeling $ 2 $ 55 $ 13 $ The expected value of feature fi with respect to the empirical distribution i~ ( x , y ) is expressed as x , y and the expected value of fi with respect to the probability distribution p ( ylx ) is p ( . fi ) -~ ~ ( x ) pCylx ) . h ( x , y ) , x~y where l~/TERM (/TERM x/TERM )/TERM is the/DEF empirical distribution of x in the corpus ./O
Implementing_Embedded_MT $ 2 $ 64 $ 0 $ 2.1 Language / Code Set Identification Knowing the language and encoding~ or code set , of a document is a necessary first step in utilizing on-line text.
_Introduction $ 1 $ 16 $ 12 $ In the present study , we treat the task of determining the appropriate distribution of mentions in text as a machine learning classification problem : what is the probability that a mention will have a certain grammatical relation given a deh set of linguistic features?
Results $ 3 $ 87 $ 33 $ after applying corrective measures to base chunker combination.
Rule_Sequence_Learning $ 3 $ 46 $ 5 $ ( 2 ) A sequence of rules of the form : Change the label of a string from m to n if C ( string ) , where C/TERM is a/DEF predicate over strings and/O m ,n ~ L. A string is labelled by first applying the start-state annotator to it , and then applying each rule , in order.
Abstract $ 0 $ 73 $ 42 $ In the deeper linguistic analysis the/TERM two/TERM so/TERM 's/TERM may be related , for they refer to a/DEF situation involving excessive height with implied consequence which may or may not be stated ./O
Highlight $ 3 $ 51 $ 0 $ Highlight/TERM ( Thomas et al . , 2000 ) is a/DEF generalpurpose IE engine for use in commercial applications ./O
Abstract $ 0 $ 1 $ 0 $ This paper describes an automatic method for extracting systematic polysemy from a/DEF hierarchically organized semantic lexicon (/O WordNet/TERM ) .
Introduction $ 1 $ 48 $ 39 $ In this syntactic category , we single out a subclass of sentences according to one possible semantic connection between the two clauses .
The_REXTOR_System $ 5 $ 122 $ 0 $ Using its finite-state language model , the REXTOR/TERM System/TERM generates/DEF a set of ternary expressions that correspond to content of a part/Of-speechtagged input document ./O
OUT $ -1 $ 0 $ 0 $ Query Translation in Chinese-English Cross-Language Information Retrieval Zhang Yibo , Sun Le , Du Lin , Sun Yufang Chinese Information Processing Center , Institute of Software , Chinese Academy of Sciences , P .O .Box 8718 , Beijing , 100080 , P .R .
The_Structure_of_a_Relational $ 3 $ 98 $ 24 $ Each link file also defines a predicate.
Using_KeyWords $ 1 $ 31 $ 0 $ A KeyWord/TERM list/TERM is a/DEF portion of the study corpus word list ./O
Maximum_Entropy_models $ 2 $ 41 $ 37 $ Trying new feature combinations , by adding them manually and testing the new configuration is a time consuming and not very interesting activity .
Reading_Comprehension_Tests $ 2 $ 26 $ 8 $ Quarc/TERM (/TERM QUestion/TERM Answering/TERM for/TERM Reading/TERM Comprehension/TERM )/TERM is a/DEF rule-based system that uses lexical and semantic heuristics to look for evidence that a sentence contains the answer to a question ./O
Cross-corpora_experiments : $ 6 $ 138 $ 3 $ For instance , Table 7 shows a drop in .16 in precision for local content collocations when compared to Table 4 .
_Phrasal_Indexing $ 1 $ 26 $ 1 $ 1.1.
See_ ( Chu-Carroll_and_Carberry_1998 ) _tbr_an $ 4 $ 116 $ 7 $ The only requirement for this method is the specification of a sensible task .
Models_and_Modifications $ 2 $ 79 $ 66 $ *3 of the 400 sentences were not parsed due to timeouts and/or pruning problems .
See_the_report_entitled_ $ 3 $ 19 $ 9 $ We describe preliminary work developing measures on system-internal components that assess : ( i ) the flow of words relevant to the filtering task and domain through the steps of document processing in our embedded MT system , and ( ii ) the level of " noise/TERM " i.e. , processing/DEF errors , passing through the system ./O
Hyperonymy_in_lexical_semantics $ 5 $ 121 $ 24 $ There is a general rule that requires speakers to use this term in order to obtain an unmarked utterance in a given context : - : - . unless . this would result in an ' abnormal communication ' , in which case the speaker should deviate from neutral level , but only to the minimum degree required to ensure normality .
Abstract $ 0 $ 146 $ 4 $ Le VIDAL ® includes a collection of notices , for .around• 5 5.00. dmgs..a~ailable .in France.
Evaluation $ 7 $ 199 $ 15 $ This is the major reason of the failure as shown in Figure 4 .
Experiments $ 5 $ 101 $ 7 $ `` Precision/TERM '' is the/DEF percentage of correct answers among the answers proposed by the system ./O
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 46 $ 11 $ Let us consider further a broad coverage domain which consists of a small number of sanaple news documents about the same topic , 'Kobe Japan quake' .
Introduction $ 1 $ 31 $ 23 $ The philosophy behind the design of HowNet is'its ontological view that all physical and non-physical matters undergo a continual process of motion and change in a specific space and time .
Comparison_experiment $ 3 $ 174 $ 76 $ where p/TERM (/TERM i/TERM )/TERM is the/DEF position of the web document in the ordered list ./O
The_MATE_Approach $ 2 $ 37 $ 1 $ Then it describes how a toolbox ( the MATE Workbench ) has been implemented to support the markup framework by enabling annotation on the basis of any coding scheme expressed according to the framework .
Experimental_Design $ 3 $ 181 $ 15 $ Measures which do not depend on ground truth compute the summarydocument similarity sire ( s , d ) .
Abstract $ 0 $ 6 $ 3 $ Regression analysis of the experimental results reveals that , in order for ECOC to be successful for language learning , the use of the Modified/TERM Value/TERM Difference/TERM Metric/TERM ( MVDM/ACR ) is an/DEF important factor , which is explained in terms of population density of the class hyperspace ./O
Bridging_Natural_Language_and $ 4 $ 119 $ 40 $ For example , indexing adjacent word pairs consists of indexing adjacent words with the adjacent relation .
More_General_Information $ 6 $ 103 $ 3 $ " For this more general case , define B/TERM (/TERM X/TERM )/TERM to be B/DEF ( X1 , X2 , . . .X , ) where each Xi is a possible value of X ./O
Conclusions_and_Future_Work $ 5 $ 144 $ 10 $ To extract the corresponding syntax information of English Chinese bilingual corpus by shallow parsing is a direction for future work , also .
Abstract $ 0 $ 4 $ 1 $ Currently the major part of this architecture consists of a set of datatype definitions for specifying the input and output formats for modules within NLG systems .
Examples_of_YAG_in_use $ 3 $ 84 $ 19 $ SNePS/TERM is a/DEF semantic network processing system (/O Shapiro and Rapaport , 1992 ) .
Conceptualizing_Events $ 2 $ 94 $ 64 $ Investigation/TERM is a/DEF rich source of occurrences that should not happen in civil aircraft WINDOW ,/O TURNING THE HANDLE , PULL , and LET operations .
Advantages_of_automatic $ 3 $ 127 $ 25 $ NLG can be used for checking a CL , which is helpful even if the CL is intended for a human writer because it may avoid the discovery of various cases of incoherence by the writer .
Note_that_the_nouns_at_this_point_are_types_not $ 4 $ 177 $ 62 $ [ Och & Ney , 2000] The latter result has to be considered with caution in the present experimental design context since the evaluation of the alignments was done with a human translation on a closed domain corpus , for only one of the languages under consideration in the current investigation .
ADVF_Far_adverbs_ ( ~ ' l $ 7 $ 130 $ 45 $ The block-based/TERM dependency/TERM parsing/TERM strategy/TERM is a/DEF novel integration of phrase structure partial approach and dependency parsing approach ./O
Methodology $ 2 $ 55 $ 12 $ The words which appear with roughly similar relative frequencies in the two corpora appear lower down the list.
Concepts $ 2 $ 59 $ 34 $ The objective/TERM function/TERM is defined as the/DEF sum of the code length for the model ( " model description length " ) and that for the data ( " data description length " ) ./O
The_computation_of_the_velocity_is_easily ,_done_from $ 4 $ 184 $ 31 $ ( In step 2 the ' new ' node is the ' old " one , not the problem node!
Generation_of_Multiple_Quantifiers $ 5 $ 192 $ 7 $ In `` Each patient is given a high severity rating '' , performing universal quantification on the patients ( ARG3/TERM ) is a/DEF separate decision from the existential quantification of the severity ratings ( ARG2 ) ./O
Evaluation $ 3 $ 195 $ 6 $ The amount of saving in manual scanning for errors is called the skip ratio , which is the number of blocks classified as correct over the total number of blocks .
Abstract $ 0 $ 206 $ 9 $ The nouns that refer to concrete objects and verbal actions are similar to adjectives when they represent a level in context .
LTAGs_and_Extraction $ 3 $ 34 $ 7 $ An auxiliary/TERM tree/TERM represents a/DEF recursive structure and has a unique leaf node ,/O called the foot/TERM node/TERM , which has/DEF the same syntactic category as the root node ./O
Abstract $ 0 $ 15 $ 12 $ Artificial/TERM neural/TERM networks/TERM are a/DEF classification technique that is robust and resistant to noisy input , and learns to classify inputs on the basis of training examples , without specific rules that describe how the classification is to be done ./O
Introduction $ 1 $ 28 $ 20 $ 2 An Overview of HowNet HowNet/TERM is a/DEF bilingual general knowledge-base describing relations between concepts and relations between the attributes of concepts ./O
The_semantic_behavior_of_the $ 5 $ 95 $ 2 $ MIKETTEI NO JOUTAI ( indecision ) ( of ) ( situation ) a situation of indecision In this case , the "MIKETTEI NO ( of indecision ) " also represents the state concretely.
_KNOWLEDGE_EXTRACTION $ 3 $ 120 $ 62 $ it is a known proper name ( bib ) or a location name ( No ) .
Abstract $ 0 $ 36 $ 26 $ A CLCS can also be decomposed on the generation side in different ways depending on the RLCSes of the lexical items in the target language.
Discussion $ 5 $ 93 $ 0 $ We showed that the learnability result of Valiant for learning boolean concepts can be transformed to a learnability result for pattern languages by looking at the transformation of the underlying representational theories ; i.e.
_System_Configuration $ 2 $ 31 $ 7 $ The system accepts users ' queries expressed in Chinese natural language .
Statistics_Based_Hybrid_Approach_to $ 2 $ 24 $ 6 $ Definition 3 : Boundary/TERM tag/TERM denotes the/DEF possible relative position of a word to a base phrase ./O
Results $ 4 $ 73 $ 4 $ The word-list captions , however , were dramatically worse on two-word queries ( 70.5% ) than on one-word queries ( 89.7% ) .
Introduction $ 1 $ 34 $ 23 $ LLR can be used in a form ( -2logA ) which is X 2 distributed .
Introduction $ 1 $ 44 $ 36 $ The fact that HowNet has verified this thesis over 65 ,000 concepts is a good proof of its robustness .
Introduction $ 1 $ 8 $ 5 $ The Deep Read group provided us with an on-line version of the Remedia material along with several marked up versions of * This research was supported in part by NSF grant LIS SBR 9720368.
Related_Work $ 2 $ 30 $ 3 $ Concept/TERM matching/TERM is a/DEF technique that has been used in limited domains ,/O like the legal field were conceptual indexing has been applied by ( Stein , 1997 ) .
Theoretical_Ideas , $ 2 $ 27 $ 2 $ Logic/TERM is indeed an/DEF excellent way to think about representing static relationships like database queries ,/O but it is much less clear that it is a good way to represent commands .
OUT $ -1 $ 197 $ 179 $ First , semantic classes were extremely useful for WHO , WHEN , and WHERE questions because they look for descriptions of people , dates , and locations .
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 38 $ 3 $ Figure 1 : The document titled 'Two Americans l~lown dead in Japan quake' Figure I is the document whose topic is 'Kobe Japan quake' , and the subject of the document ( event 31 words ) is 'Two Americans known dead in Japan quake'.
Topic_Analysis $ 4 $ 85 $ 18 $ 10 ) U5 ( 0 . 06 ) 15 In Hung long , where nauspaporo have alleged Japan has been nailing baler-cost semiconductors , some electronicsmanu~acturnrn share . . . 16 " That is a very short-term vies .
Introduction $ 1 $ 11 $ 8 $ EVIUS/TERM is a/DEF component of a multilingual IE system ,/O MTURBIO ( Turmo et al . , 1999 ) .
_Conclusion $ 4 $ 119 $ 10 $ The preliminary results have demonstrated that the integration of a concept network , a query reformulator , a standard search algorithm , an auto summarizer , and an optional TTS engine indeed suits the current information seeking behavior and make search activities in websites more intuitive , as well as productive .
Experimental_Results $ 7 $ 169 $ 2 $ We thus utilized Reuters news articles referred to as ' Reuters-21578 , ' which has been widely used in text classification v . We used a prepared SAn exception is the method proposed in ( McCallure and Nigam , 1999 ) , which , instead of labeled texts , uses unlabeled texts , pre-determined categories , and keywords defined by humans for each category .
Building_Spoken_Dialo~te_Systems $ 4 $ 118 $ 13 $ Each definition/TERM is a/DEF pair comprising a phrase category name and a network of word categories ./O
Introduction $ 1 $ 6 $ 1 $ Topic/TERM analysis/TERM consists/DEF of two main tasks : topic identification and text segmentation (/O based on topic changes ) .
Overview_of_the_Approach $ 2 $ 34 $ 14 $ Finally , an nn . qpeci~ed object required as an argument to a predicate can appear elsewhere in the sentence , requiring the use of the predicate const ( X ,C ) to bind the variable X to the constant C . Some other database queries ( or training examples ) for the U . S . Geography domain are shown below : What is the capital of Texas?
Introduction : $ 1 $ 10 $ 5 $ Because supervised training typically demands significant human involvement ( e . g . , annotating the parse trees of sentences by hand ) , building a new corpus is a labor-intensive task .
_The_session_was_also_attended_by_observers_from_the_following_international_organizations : $ 7 $ 109 $ 58 $ The productivity is the ratio of the number of candidates to the size of the collection .
OUT $ -1 $ 60 $ 18 $ Class/TERM probability/TERM assignments/TERM are then estimated/DEF using statistics computed on the equivalence classes ./O
Implications_for_NLG $ 5 $ 97 $ 0 $ For many NLG applications , the notion of compatibility defined above is a useful hard constraint ; even if violations of this constraint are sometimes acceptable , they are not essential .
Maximum_entropy-based_parse $ 2 $ 29 $ 0 $ selection The task of parse/TERM selection/TERM involves selecting/DEF the best possible parse for a sentence from a set of possible parses produced by an AVG ./O
Rhetorical_structure_and $ 2 $ 24 $ 0 $ text structure To distinguish clearly between FthetRep and DocRep , we need to define the kinds of information that should be included in the two representations .
Extending_a_Grammar_to_Enable $ 4 $ 121 $ 48 $ `` , in favor of `` his sister 's dog '' , without the application having to request a pronoun explicitly , as in the example shown above , we could add a rule to force the pronominal feature of the inner most possessor to be YES , whenever a ( repeated ) noun phrase is a possessor of a possessor of the primary noun .
Content_Planning $ 1 $ 48 $ 23 $ The model/TERM is the/DEF probability distribution P ( nk ) = P ( nklck ) ,/O where nk/TERM is the/DEF number of attributes and/O Ck/TERM is the/DEF utterance class for system utte~anee k ./O 1.2.2 The bigram model of the attributes This model will predict which attributes to use in a system utterance .
Algorithms_and_Implementation $ 2 $ 28 $ 8 $ The/DEF heuristic approximation of computationally expensive pure MBL variants ,/O ( IGTREE/TERM ) , creates an oblivious decision tree with features as tests , ordered according to information gain of features .
Results $ 6 $ 133 $ 6 $ We deem one evaluation function more effective than another if the smallest set of sentences it selected can train a grammar that performs at least as well as the grammar trained under the other function and if the selected data contains considerably fewer brackets than that of the other function .
Examples_of_YAG_in_use $ 3 $ 69 $ 4 $ In this representation , M2/TERM is the/DEF proposition that the discourse entity B2 is a member of class "dog" ./O
Method $ 2 $ 52 $ 7 $ The filter is the only component of this system which we experiment with here .
Conclusion $ 5 $ 215 $ 3 $ As to retrieval performance , word-based IR systems can be superseded by sense-based ones using effective techniques that are able to identify and compare meanings or senses of words .
Introduction $ 1 $ 21 $ 12 $ The method was evaluated on the LFG grammar for French developed within the PARGRAM project ( Butt et al . , 1999 ) , but it is applicable to any unification grammar with a phrase-structure backbone where the reference treebank contains all possible analyses for each training example , along with an indication of which one is the correct one .
Conclusions $ 6 $ 107 $ 2 $ In this paper we proposed a corpus-based technique for specializing a grammar on a domain for which a treebank exists containing all trees returned for each sentence .
_Tagging_NuLL-marker_CDM_pairs_ ( via $ 8 $ 73 $ 3 $ 4. the/TERM principle/TERM of/TERM superiority/TERM : When/DEF matching a pair of discourse markers for a rhetorical relation , priority is given to the inter-sentence relation whose back discourse marker matched with the first word of a sentence ./O
Abstract $ 0 $ 148 $ 6 $ The main source are the New Drug Authorizations ( Autorisation de Mise sur le March~ ) , regulatory documents written by pharmaceutical laboratories and approved by legal authorities .
Unfolding_and_Specialization $ 2 $ 28 $ 1 $ This is in itself a specialization of the grammar which was used to parse the treebank , since some rules may not show up in any correct parse in the training set; experimental results for this first/Order specialization are reported in ( Cancedda and Samuelsson , 2000 ) .
Danish_Data $ 4 $ 74 $ 0 $ In this section I shortly describe Danish third person personal and possessive pronouns and demonstrative pronouns.
Overview $ 1 $ 4 $ 0 $ Lexical acquisition from large corpora has long been considered as a means for enriching vocabularies ( Boguraev and Pustejovsky , 1996 ) .
_The_co-reference_problem_in_summarization $ 4 $ 43 $ 19 $ ) The Columbia University system ( McKeown et al . , 1999 ) creates a multi-document summary using machine learning and statistical techniques to identify similar sections 41 and language generation to reformulate the summary .
Maximum_Entropy_Modeling $ 2 $ 61 $ 19 $ In building a model , we consider the linear exponential family Q given as 1 Q ( f ) = {p ( ylx ) = ~exp ( E ~ifi ( x ,y ) ) } , ( 2 ) 165 where Ai are real-valued parameters and ZA ( x ) is a normalizing constant : = exp ( y ) ) .
Current_approach $ 3 $ 84 $ 45 $ The product f~w=~wTVk/TERM is the/DEF projection of ~T into the k-dimensional latent semantic space ./O
OUT $ -1 $ 107 $ 56 $ A nor- mal embedding is one satisfying condition 1 , 3 and 4 and the embedded/TERM part/TERM is a/DEF relative clause which provides additional information about the referent ./O
Lexical_Conceptual_Structure $ 2 $ 9 $ 0 $ Lexical/TERM Conceptual/TERM Structure/TERM is a/DEF compositional structure that captures a concept ./O
Future_Research_Issues $ 5 $ 140 $ 18 $ There is a need to study , along with learning and knowledge representation , inference methods that suit this framework ( KR97 ) .
The_Learning_System $ 2 $ 83 $ 57 $ If that is the case , the posterior probability of the values used are reinforced in each of the parameters , and if they achieve a certain threshold , they are retained as the current values , otherwise the previous values are kept.
OUT $ -1 $ 69 $ 64 $ The TREC-8 QA test scores of ( Radev et al . , 2000 ) were also considerably lower than best QA test scores .
Modeling_various_degrees_of $ 5 $ 82 $ 19 $ The ACTIONS/TERM field/TERM is a/DEF stack of ( domain ) actions which the user has been instructed to perform but has not yet performed ./O The LU/TERM field/TERM contains/DEF information about the latest utterance ./O
Abstract $ 0 $ 16 $ 14 $ KeyWords compares a word list extracted from what has been called ' the/TERM study/TERM corpus/TERM ' ( the/DEF corpus which the researcher is interested in describing )/O with a word list made from a reference corpus .
LTAG_formalism $ 2 $ 25 $ 2 $ We choose LTAGs as our target/TERM grammars/TERM ( i.e. , the/DEF grammars to be extracted )/O because LTAGs possess many desirable properties , such as the Extended Domain of Locality , which allows the encapsulation of all arguments of the anchor associated with an etree.
Word_Co/Occurrence_Vector $ 2 $ 49 $ 17 $ The i : j-th element denotes the number of documents in which both words w , : and wj appear , F ( wi , wj ) ( Figure 2 ) .
OUT $ -1 $ 70 $ 50 $ Character encoding schemes of CJK languages have several variations ( e.g. , Chinese : GB and BIG-5 , etc.
_Rare_w~_contains_a_hyphen $ 9 $ 96 $ 41 $ Some are the result of inconsistency in labeling in the training data ( Ratnaparkhi 1996 ) , which usually reflects a lack of linguistic clarity or determination of the correct part of speech in context .
OUT $ -1 $ 102 $ 84 $ Rule #1 is the generic word matching function shared by all question types .
Results $ 8 $ 164 $ 3 $ Concerning tense , our `` gold standard ' ' is the set of human translations , generated tense past present human past 134 17 translation present 17 27 Table 2 : Preliminary Tense Results previously constructed for these sentences .
Example_Dialogue $ 3 $ 35 $ 12 $ None of the above ( U5 ) U : list ( U6 ) S : By " from bob " do you mean : 1 . source folder 2 . sender ( UT ) U : sender ( US ) S " 2christmas " is a way to express : -1 . yesterday ( date relative yesterday ) 2 . tomorrow ( date relative tomorrow ) 3 . today ( date relative today ) 0 .
Introduction : $ 1 $ 21 $ 16 $ The first is a sire• ple heuristic that approximates the grammar ' s uncertainty in terms of sentence lengths .
Introduction $ 1 $ 10 $ 3 $ With ECOC , monadic classes are replaced by codewords , i.e.
The_Learning_System $ 2 $ 40 $ 14 $ The categories and rules in the grammar are defined as types in the hierarchy , represented in terms of TDFSS and the feature structures associated with any given category or rule are defined by the inheritance chain.
October_2000 $ 7 $ 5 $ 3 $ An intimate relationship between the two issues is becoming apparent for example , in the consideration of translation equivalence in parallel corpora , the construction of mullilingual ontologies , and the examination of senses in relation to specific natural language applications such as machine translation , information retrieval , summarization , etc .
Introduction $ 1 $ 35 $ 28 $ Our key interest in this work was to provide a system which allowed users to get answers : not just documents or sub-documents.
Data_Source $ 2 $ 40 $ 0 $ DESAM/TERM ( Pala et al. , 1997 ) , the/DEF annotated and fully disambiguated corpus of Czech newspaper texts ,/O has been used as the source of learning data.
Examples_of_YAG_in_use $ 3 $ 71 $ 6 $ Thus , we can read the whole proposition as "Pluto is a member of class dog .
_Under_what_circumstances_can_an_NR_construc $ 2 $ 117 $ 58 $ We claim that it is the degree of inferrability of the relation between the semantics expressed through the two clauses that makes the difference.
BOV_Stem_NE_Defaults_Qspecific_41 $ 5 $ 99 $ 69 $ Also , the parse trees served as the language for describing very question specific techniques , such as the ones for `` where ' ' questions presented in the previous section .
From_Prepositional_Phrases_to $ 3 $ 80 $ 23 $ The degree expression SPACE/TERM , with its associated negative POL--MARKER , ( Staab and Hahn , 1997 ) is the/DEF trigger for recognizing the evaluative status of the matrix clause ./O
The_TABULATE_ILP_Method $ 3 $ 95 $ 29 $ The metric/TERM M/TERM (/TERM H/TERM )/TERM used as the search heuristic is defined as : M/DEF ( H ) = accuracy ( H ) + C log 2 size ( H ) ( 4 ) where C is a constant used to control the relative weight of accuracy vs. complexity ./O
_Instrumentalists_not_including_string_players $ 8 $ 97 $ 85 $ Experiment 2 : sets of senses that have parallel translations in at least one out of the four target languages .
MALIN $ 4 $ 224 $ 118 $ The answer U7 is a valid response to $6 and produces a new OPM , see figure 14 .
Intelligent_Multimedia_Presentation $ 3 $ 70 $ 2 $ The motivation for this is obvious : when a summarization filter ( which is a program under our control ) is generating a media object , we can often provide sufficient recta-information about that object to generate a short caption and some running text .
Related_Work $ 3 $ 126 $ 28 $ Accuracy Measurements of Parsing with Different Measures of Association are especially interested in experimenting with a Maximum Entropy model .
Results_and_discussion $ 4 $ 221 $ 42 $ KIS KCS % context retrieval increasing with respect to KAS f. ( l , os.
Dialogue_Management $ 2 $ 89 $ 45 $ The second part of the reasoning model consists of reasoning schemas , that supposedly regulate human action-oriented reasoning .
Abstract $ 0 $ 3 $ 1 $ Using clustering techniques and features reflecting the diglossia phenomenon , we have successfully discriminated registers in Modem Greek.
Abstract $ 0 $ 139 $ 15 $ Rather , I intend to put forth the conjecture that syntax acquisition is extremely sensirive to the distribution of ambiguity , and , given this extreme sensitivity , suggest that simulation studies need to be conducted in conjunction with a broader analysis which abstracts away from whatever linguistic particulars are necessary to bring about the sentences required to build the input sample that feeds the simulated learner .
Evaluation_and_results $ 4 $ 67 $ 4 $ Weighted/TERM accuracy/TERM is a/DEF measure that weights higher the hits and misses 100 for the preferred class ./O
Introduction $ 1 $ 37 $ 33 $ However , as higher values of k lead to an enormous number of possible rules , huge data sets would be necessary in order to have a reliable estimate of the probabilities for values above k = 3 .
Topic_Analysis $ 4 $ 91 $ 24 $ The measures proposed include lapse supplementary budget and record public works spending in the firso half of ohe financial year .
Error-driven_Learning $ 4 $ 121 $ 4 $ If F o ( e i ) > 0 , we define the lexical entry ei as positive for lexicon ~ .
_The_co-reference_problem_in_summarization $ 4 $ 46 $ 22 $ There are two types of situations in which multidocument summarization would be useful : ( 1 ) the user is faced with a collection of dis-similar documents and wishes to assess the information landscape contained in the collection , or ( 2 ) there is a collection of topicallyrelated documents , extracted from a larger more diverse collection as the result of a query , or a topically-cohesive cluster .
OUT $ -1 $ 173 $ 168 $ The test/TERM set/TERM consists/DEF of 30 stories from grade 3 and 30 stories from grade 4 ./O
Abstract $ 0 $ 5 $ 1 $ Mercury provides telephone access to an on-line flight database , and allows users to plan and price itineraries between major airports worldwide.
OUT $ -1 $ 94 $ 76 $ Rule #2 rewards sentences that contain a recognized NAME , and rule #3 rewards sentences that contain the word " name " .
Overview_of_the_Approach $ 2 $ 32 $ 12 $ For example , largest/TERM (/TERM X/TERM ,/TERM Goal/TERM )/TERM states that the/DEF object X satisfies Goal and is the largest object that does so , using the appropriate measure of size for objects of its type (/O e.g .
_Introduction $ 1 $ 6 $ 0 $ The Penn Treebank ( Marcus et al.
Language_Modeling_using $ 4 $ 90 $ 13 $ P/TERM (/TERM .wi[w~-lcc/TERM )/TERM denotes the/DEF probability that wi follows w~- : given that a content word follows w~- : , which is a linear interpolation of a standard trigram model and the context co/Occurrence probabilities ./O
The_Complexity_of_Extracting_a $ 2 $ 31 $ 19 $ The level/TERM of/TERM a/TERM fact/TERM , F , in a piece of text is defined by the/DEF following algorithm : F . Suppose {xl ,x~ , . . . ,Xn} are the nodes relevant to F . Let s be the partial network consisting of the set of nodes {xl , x~ , . . . , x~} interconnected by the set of arcs {tl , t2 , . . . , tk} ./O
Global_View_on_the_DE $ 2 $ 67 $ 50 $ is common to several entries and extracting it into abstract entries ( the result is a hierarchical organization of the resource ) .
OUT $ -1 $ 62 $ 44 $ The rules are applied to each sentence in the story , as well as the title of the story , with the exception that the title is not considered for WHY questions .
Hyperonyms_in_NLG_systems $ 4 $ 60 $ 1 $ In these systems , the hyperonym problem as one aspect of the general task of lexical choice arises only in systems that employ a sufficiently rich model of the lexicon and tile concept-lexicon link .
The_Classifiers $ 3 $ 91 $ 54 $ Category choice : Given the F~ and F1 b input vectors A and B , for each F2 node j , the choice function Tj is defined by IA Aw~l IB A w~l = ~a~ + Iw~'l + ( 1 --~ ) ~b + Iw~l' ( S ) where the fuzzy/TERM AND/TERM operation/TERM A is defined by (/DEF p A q ) i --~ min ( pi , qi ) , ( 9 ) and where the norm I-I is defined by IPl -= ~Pi ( 10 ) i for vectors p and q ./O
Interaction_Grammars $ 3 $ 85 $ 32 $ city ( ham ) --> [] .
Support_Vector_Machines $ 2 $ 6 $ 0 $ Support/TERM Vector/TERM Machines/TERM (/TERM SVMs/TERM )/TERM , first introduced by Vapnik ( Cortes and Vapnik , 1995; Vapnik , 1995 ) , are relatively/DEF new learning approaches for solving two-class pattern recognition problems ./O
OUT $ -1 $ 4 $ 4 $ Keywords : Cross Language Information Retrieval , Multilingual Information Processing , Chinese , Japanese and Korean ( CJK ) Languages Introduction After the opening of the Cross/TERM Language/TERM Information/TERM Retrieval/TERM ( CLIR/ACR ) track in the TREC-6 conference ( TREC-1998 ) , several reports have been published on cross language information retrieval in European languages , and sometimes , European languages along with one of the Asian languages ( e .g . , Chinese , Japanese or Korean ) .
Implementing_Embedded_MT $ 2 $ 119 $ 62 $ The second software engineering challenge stemming from this is the amount of time necessary to bring up a translation engine .
The_Generation_System $ 3 $ 51 $ 9 $ The particular format , known as long-hand is equivalent to the form shown in ( 4 ) , but making certain information more explicit and regular ( at the price of increased verbosity ) .
Introduction $ 1 $ 32 $ 25 $ Plain text documents are provided to a lexical analyzer and a noun-recognizer ( XEROX MULTEXT ) , whose output is the document text tagged with parts of speech to be fed to the parser .
Abstract $ 0 $ 17 $ 16 $ The experiments we undertook to assess the performance of these algorithms are the topic of Section 4.
Informational_content_of_sentences $ 2 $ 34 $ 10 $ 2 in the second article , while sentence ' 9 from the former article is later repeated in sentences 3 and 4 of the latter article .
Centroid-based_summarization $ 4 $ 82 $ 10 $ For example , the sentence `` President Clinton met with Vernon Jordon in January '' gets a score/TERM of 243.34 which is the/DEF sum of the individual eentroid values of the words (/O clinton = 36.39 ; vernon = 47.54 ; jordan = 75.81 ; january = 83.60 ) .
Motivation $ 2 $ 44 $ 8 $ The START/TERM System/TERM ( Katz , 1990 ; Katz , 1997 ) analyzes/DEF English text and builds a knowledge base from information found in the text ./O
Conclusion $ 4 $ 55 $ 1 $ The perplexity of the test sample decreases when a combination of models with k = 2 and k = 3 is used to predict string probabilities .
Problem_Space_Modeling $ 2 $ 55 $ 28 $ Each timeout has a reward and a punishment .
Prosodic_Information_and $ 3 $ 30 $ 0 $ 3.2 Part/Of-speech The part/Of-speech/TERM is another basic/DEF information for speech recognition , syntactic/semantic parsing , and dialogue processing as well as linguistic and psycholinguistic analysis of spoken discourse ./O
Comparative_analysis_~of_Japanese_and $ 3 $ 90 $ 2 $ Chinese/TERM is a/DEF non-inflectional language and/O therefore morphological analysis is not essential .
Building_a_reusable_lexical_chooser $ 2 $ 22 $ 5 $ Conceptual/TERM elements/TERM are by definition domain and application dependent ( they are the/DEF primitive concepts used in an application knowledge base )/O .
Implementation $ 4 $ 240 $ 69 $ Regarding the number of terms contained in one sentence as a constant , topic sentences are ext : racted in O ( skh ) time where s/TERM is the/DEF total number of sentences in the document set ./O
Generation_and_linguistic_representation $ 4 $ 96 $ 24 $ The : -organization of ' these entries assures ' that -rasing the same mechanism as with speech -REA ' S gestures draw on the single available conceptual representa174 tion and that both REA ' S gesture and the relationship between gesture and speech-vary as a function of pragmatic context in the same way as natural gestures and speech do .
User_Interface $ 5 $ 104 $ 13 $ As can be seen from Figure 5 , there is a check box along with each retrieved record .
Text_Summarization $ 2 $ 23 $ 2 $ In our case , we are dealing with technical articles which are the result of the complex process of scientific inquiry that starts with the.
Dialog_Management $ 3 $ 62 $ 1 $ 3.1 Content Selection Module The Content/TERM Selection/TERM Module/TERM consists of four components : Level-Adjusting/DEF Agent , UtilityUpdating Agent , Action Planner and Content Selector ./O
Support_Vector_Machines $ 2 $ 19 $ 13 $ SVMs/TERM can be regarded as an/DEF optimization problem ; finding w and b which minimize [ [ w[ [ under the constraints : yi[ ( w • xi ) + b] > 1 ./O
The_Generation_System $ 3 $ 87 $ 45 $ ( 8 ) AMR = <concept> I ( <label> {<role> <AMR>}+ ) Since the roles expected by Nitrogen's English generation grammar do not match well with the thematic roles and features of a CLCS , we have extended/DEF the AMR language with LCS-specific relations ,/O calling the result , an LCS-AMR/TERM .
Approach $ 2 $ 39 $ 12 $ Each leaf node has an associated/TERM goal/TERM , which , when realized , provides/DEF content for that node ./O
Comparing_the_five_approaches $ 4 $ 77 $ 7 $ Restricting to LB results , it can be observed that the accuracy obtained in A-B is 47.1% , while the accuracy in B-B ( which can be considered an upper bound for LB in B corpus ) is 59.0% , that is , that there is a difference of 12 points.
Introduction $ 1 $ 32 $ 25 $ Combined together , a finite-state language model and ternary expression representation provide a convenient and powerful framework for integrating natural language processing with information retrieval .
Introduction $ 1 $ 6 $ 2 $ A venture capitalist who wants to invest in an MT start-up needs to know a different set of attributes about the system than does a developer who needs to see if the most recent software changes improved ( or degraded ) the system .
Examples $ 2 $ 26 $ 4 $ U2 : screen saver .
Comparison_experiment $ 3 $ 167 $ 69 $ " it is not sufficient that the string " Robert Sheckley " or " Sheckley " is in the text , but the document has to say that Robert Sheckley is the author of Options .
Introduction $ 1 $ 47 $ 38 $ TM3 simply hosts the whole collection of aligned bilingual Types of documents in the corpus beyond our interest ( Trados Translator ' s Work . . . . . .
OUT $ -1 $ 87 $ 33 $ ( Sugar maple trees )
Middle $ 6 $ 139 $ 102 $ Some synsets are selected and regarded as the mapping of the Cilin sense tag .
Abstract $ 0 $ 1 $ 0 $ This paper presents the integration of a largescale , reusable lexicon for generation with the FUF/SURGE unification-based syntactic realizer .
Models $ 2 $ 27 $ 7 $ It can be shown ( Della Pietra et al . , 1995 ) that these are the also the values which minimize the Kullback-Liebler divergence D ( p[[q ) between the model and the reference distribution under the constraint that the expectations of the features ( ie , the components of f ) with respect to the model must equal their expectations with respect to the empirical distribution derived from the training corpus .
Tagged_Text $ 3 $ 201 $ 187 $ The network contains an input layer with two groups of features .
Abstract $ 0 $ 4 $ 1 $ An information/TERM structure/TERM consists/DEF of two components : HowNet definitions and dependency relations ./O
Text_Summarization $ 1 $ 20 $ 0 $ The task of summarization/TERM is to/DEF identify informative evidence from a given document , which are most relevant to its content and create a shorter version of smnmary of the document from this information ./O
Evaluation $ 4 $ 104 $ 0 $ To evaluate the approach , we took a sample of 20 million words of the BNC and extracted all sentences containing an occurrence of one of the 60 test verbs on average of 3000 citations of each .
_General_Outline_of_the_Method $ 2 $ 20 $ 4 $ The first stage is the learning process in which several classifiers are built from the training data .
Introduction $ 1 $ 28 $ 16 $ We have chosen Pustejovsky ' s Generative/DEF Lexicon (/O GL/TERM ) framework ( Pustejovsky , 1995 ; Bouillon and Busa , 2000 ) to define what a relevant NV link is , that is , what is a N-V pair in which the N and the V are related by a semantic link which is close , and which can therefore be used to expand indexes .
_Annotation_Guidelines_I : $ 3 $ 110 $ 64 $ Among possible argument roles , the nominal category is the default.
_The_co-reference_problem_in_summarization $ 4 $ 55 $ 31 $ Following is a list of requirements for multi-document summarization : • clustering/TERM : The/DEF ability to cluster similar documents and passages to find related information ./O
Architecture_components $ 2 $ 13 $ 4 $ Tjong Kim Sang ( 2000 ) ) , I use a smaller window , four left and two right , but add the IOB suggestions made by the first level for one token left and right ( but not the focus ) .
The_REXTOR_System $ 5 $ 135 $ 13 $ The template/TERM consists/DEF of a series of legal tokens ,/O which are shown in Table 1 .
Introduction $ 1 $ 11 $ 4 $ Classical/TERM dialogue/TERM systems/TERM like UC ( Wilensky et al . , 1984 ) utilized/DEF a formal language to represent knowledge , which requires the heavy cost of construction and maintenance and makes the scaling up quite difficult ./O
OUT $ -1 $ 0 $ 0 $ Task-based dialog management using an agenda Wei Xu and Alexander I. Rudnicky School of Computer Science Carnegie Mellon University 5000 Forbes Ave Pittsburgh , PA 15213 {xw , air] @cs.
Introduction $ 1 $ 18 $ 9 $ logic , it is a common approach to connect different representational theories , and transform results of one representational theory to results in an other representational theory.
Implementing_PbA $ 3 $ 41 $ 0 $ In PbA , an unknown word is pronounced by matching substrings of the input to substrings of known , lexical words , hypothesizing a partial pronunciation for each matched substring from the phonological knowledge , and assembling the partial pronunciations .
_Sense_Co-occurrence_Frequency $ 3 $ 73 $ 46 $ N ~ g ( SU ) , g ( SU ' ) ( 7 ) Where f/TERM (/TERM SU/TERM ,SU/TERM '/TERM )/TERM is the/DEF co-occurrence frequency corresponding to sememe pair ( SU , SU ' ) in SCFD ./O
Abstract $ 0 $ 26 $ 24 $ In the following example , the DA/TERM consists/DEF of a speaker tag ( a : for agent ) , the speechact give-information , and two main concepts , +price and +room ./O
Dialogue_Structure_and $ 6 $ 119 $ 34 $ Elliptical/TERM coupling/TERM is the/DEF pattern of [A : I] [B : I A : R] , equivalent to the one in which B ' s second response is omitted in coupling ./O
Multiple_heuristics_for_word_sense $ 2 $ 38 $ 13 $ sim ( s , s2 ) computes the conceptual similarity between concepts s~ and sz as in the following formula : sim ( sl , s2 ) = 2 x level ( MSCA ( sl , s : ) ) level ( sO + level ( s2 ) where MSCA/TERM ( sl , s2 ) represents the/DEF most specific common ancestor of concepts s~ and s2 and/O level/TERM ( s ) refers to the/DEF depth of concept s from the root node in the WordNetL 2.2/O Heuristic 2 : Prior Probability This heuristic provides prior probability to each sense of a single translation as score .
Abstract $ 0 $ 15 $ 14 $ Section 2 describes the previous work of summarization.
Supervised_Training $ 2 $ 61 $ 21 $ with tf/TERM ( t , d ) > 1 ~re t is an/DEF estimate of the total number of relevant where : D ( description ) , E ( query expansion ) documents ./O
Introduction $ 1 $ 17 $ 11 $ Transformation-based/TERM learning/TERM (/TERM TBL/TERM )/TERM ( Brill , 1995 ) is a/DEF successful rule-based machine learning algorithm in natural language processing ./O
Introduction $ 1 $ 32 $ 25 $ We use a phrase-based VNSA target language model to retrieve the most likely translation from the lattice .
Information_Structures $ 3 $ 83 $ 6 $ The ' medicine/TERM ' is a/DEF material of ' addictive ' products ./O
OUT $ -1 $ 0 $ 0 $ Comparing corpora with WordSmith Tools : How large must the reference corpus be?
Explaining_Probabilistic_Methods $ 3 $ 70 $ 22 $ Clearly , the LSQ/TERM is a/DEF linear discriminator over the feature space A' ,/O with coefficients f that are computed given ( potentially all ) the values ^D P[x ,t]" The definition generalizes naturally to non-binary classifiers; in this case , the discriminator between predicting l and other values is linear.
Introduction $ 1 $ 18 $ 14 $ ( In many other approaches , speed is a function of the grammar size , because it is searched during realization ( Elhadad , 1992 ; Elhadad , 1993 ; Mann , 1983 ; McKeown , 1982 ; McKeown , 1985 ) .
OUT $ -1 $ 116 $ 79 $ The NL-UNL/DEF encoding tool ,/O or UNL/TERM Encoder/TERM , is generic enough to handle all the 29 languages included in the Project.
Experiments $ 5 $ 87 $ 4 $ The NB/TERM ( naive/DEF Bayes/DEF ) and SNoW classifiers use the same feature set , conjunctions of size 3 of POS tags ( + words ) in a window of size 6 around the target word.
_The_Problem $ 1 $ 38 $ 32 $ the word capitalize in the sense used here 3 denotes a function of two arguments , where J is restricted to ( can only be bound to ) objects of type joint venture and to objects of type amount of money.
Stochastic_Surface_Realization $ 2 $ 75 $ 0 $ 2.2 The input to NLG from the dialogue manager is a frame of attribute-value pairs .
Introduction $ 1 $ 11 $ 3 $ Authoring/TERM is seen as a/DEF top-down interactive process of step-wise refinement of the root nonterminal ( corresponding to the whole document ) where the author iteratively selects a rule for expanding a lBut see ( Wood , 1995 : Prescod , 1998 ) for discussions of the differences ./O
_Introduction $ 1 $ 13 $ 7 $ Section 2 first gives a general outline of the trainable method we have defined to extract Chinese entity names and their relations , then describes person name extraction , entity name classification and relation extraction in detail .
ALLiS $ 3 $ 34 $ 14 $ In order to parse a text , a module automatically converts this formalism into appropriate formalisms which can be used by existing symbolic parsers.
Conclusion $ 5 $ 103 $ 2 $ TRANSTYPE/TERM is a/DEF project funded by the Natural Sciences and Engineering Research Council of Canada ./O
T $ -1 $ 0 $ 0 $ An Extended Architecture for Robust Generation* Tilman Becker , Anne Kilger , Patrice Lopez , Peter Poller DFKI GmbH Stuhlsatzenhausweg 3 D-66123 Saarbriicken , Germany {becker , kilger , lopez , poller}@dfki , de
Example_Dialogue $ 3 $ 34 $ 11 $ None of the above ( U3 ) U : none ( U4 ) S : "retrieve all messages from bob that were sent after christmas" is a way to express : 1 . move mail 2 . list mail 0 .
Concepts $ 2 $ 35 $ 10 $ The coUocational/TERM degree/TERM is defined as the/DEF ratio of the existing collocation instances between the cluster and its distribution envffonment to all possible collocations generated by them ./O
Tagged_Text $ 3 $ 146 $ 132 $ The semantic and syntactic information is coded as shown in Figure 2 ( using XML tags ) .
Introduction $ 1 $ 9 $ 1 $ Resolving/TERM the/TERM ambiguity/TERM of/TERM words/TERM is a/DEF central problem for large scale language understanding applications and their associate tasks (/O Ide and V4ronis , 1998 ) .
Conclusion $ 6 $ 140 $ 0 $ In this paper we described a novel language model of incorporating long-distance lexical dependencies based on context co/Occurrence vectors .
Given_an_input_space_X~*_of $ 3 $ 43 $ 12 $ In general , and this is the case for linguistic concepts , we can only hope that h is a good approximation of Ci..
Introduction $ 1 $ 12 $ 4 $ This situation raises an old question and opens a new area of research : can one separate content from presentation ?
Current_Research_and_Future $ 3 $ 123 $ 6 $ Further , our research shows that completeness is a problem .
_Relations_that_are_relevant_and_correct_in $ 3 $ 185 $ 151 $ We took the simplifying assumption that WordNet be complete , thus aiming at assigning at least one WordNet sense to each term that appeared in both WordNet and ASRS .
Introduction $ 1 $ 12 $ 0 $ Article choice can pose difficult problems in natural language applications.
Corpus_comparison_based_on $ 6 $ 112 $ 3 $ The entropy/TERM for/TERM NE/TERM classes/TERM H/TERM (/TERM C/TERM )/TERM is defined by = E/DEF p ( c ) log 2 p ( c ) H ( C ) cEC where/O : n ( O p ( c ) = " N n/TERM (/TERM c/TERM )/TERM : the/DEF number of words in class c N/TERM : the/DEF total number of words in text We can calculate the entropy for features in the same way ./O
Complexity_Formulas $ 4 $ 71 $ 15 $ = B ( O ) + max ( OCob# ) obj~class where O/TERM is the/DEF specification of an object in class ./O
Introduction $ 1 $ 6 $ 2 $ Disambiguation can be separated between MS/TERM ( morpho-syntactic/DEF , i.e.
The_following_formulas_summarize_the_relations $ 9 $ 157 $ 76 $ Here we see evidence for two patterns that recur in the two measures below.
First_order_weight_determination $ 3 $ 38 $ 5 $ A measure related to this is Information/TERM Gain/TERM , which represents/DEF the difference between the entropy of the choice with and without knowledge of the presence of a feature (/O cf .
Note_that_the_nouns_at_this_point_are_types_not $ 4 $ 209 $ 94 $ She reports that in Nineteen-Eighty-Four , only 86.6 % of the English words have a single lexical item used in the translation .
OUT $ -1 $ 66 $ 20 $ " The latter " words " may include any of the following : wordstrings with OCR-induced spelling changes ( valid or invalid for the specific language ) , wordstrings duplicating misspellings in the source document , and words accurately OCR-ed .
Experimental_Results $ 7 $ 217 $ 50 $ When/DEF the key words for main topics contained at least one of the identification words ,/O we viewed that text as having the corresponding/TERM main/TERM topic/TERM .
Background : _The_STOP_System $ 2 $ 17 $ 0 $ The STOP/TERM system/TERM generates/DEF personalised smokingcessation leaflets , based on the recipient's responses to a questionnaire about smoking beliefs , concerns , and experiences ./O
Implementation $ 3 $ 140 $ 22 $ The hotel/TERM Ariadne/TERM is a/DEF cheap hotel in the city centre ./O
Evaluation_of_NLG_Models $ 3 $ 101 $ 7 $ Unfortunately , this is not the case for our argument generator , where the input consists of a possibly complex and novel argument subject ( e.g. , a new house with a large number of features ) , and a complex model of the user' s preferences .
Robustness $ 4 $ 122 $ 31 $ Other examples for generation constraints that can conflict with the input are the occurrence of some specific cyclic subparts of graphs , selfreferring predicates , and chains of predicates which are not realizable in generation .
Introduction $ 1 $ 64 $ 56 $ The main focus of the paper is the description of two features which are particularly useful for attribution determination : prototypical agents and actions .
Abstract $ 0 $ 2 $ 1 $ Text/TERM meaning/TERM representation/TERM is composed of a/DEF set of ontological concept instances along with ontological links among them ./O
Mapping_a_document_collection_into $ 2 $ 120 $ 73 $ The color intensity of the dot also represents the degree of relevance .
_Measurements $ 3 $ 84 $ 17 $ Equal is open in something of type collection where that collection is a partition of something.
Highlight $ 3 $ 81 $ 30 $ Keyword/TERM based/TERM search/TERM is a/DEF special case where the user specifies one or more keywords which they want to find in a document ./O
October_2000 $ 7 $ 9 $ 8 $ Allied to corpus-similarity is corpus-homogeneity .
Conclusion $ 7 $ 204 $ 1 $ To our knowledge our system is the only one that uses semantic representation as basis for summarizing .
Statistical_Semantic_Parsing $ 4 $ 103 $ 4 $ Suppose our learned parser has n different parsing actions , the ith/TERM action/TERM a/ is a/DEF function a/ ( s ) : ISi -+ OSi where ISi G S is the set of states to which the action is applicable and OSi C_ S is the set of states constructed by the action ./O
Building_a_reusable_lexical_chooser $ 2 $ 17 $ 0 $ for generation While reusable components have been widely used in generation applications , the concept of a " reusable lexical chooser " for generation remains novel .
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 159 $ 124 $ 'Doe' denotes the number of documents .
Conclusions_&_Further_Work $ 6 $ 218 $ 9 $ a information retrieval scenario , a semantic document annotation scenario such as described in ( Erdmann et al . , 2000 ) ) will allow us an application-specific evaluation of the ontology using standard measures such as precision and recall .
Robust_Name_Finding $ 3 $ 79 $ 20 $ Finding names in speech data is a very new topic of research , and most previous work has consisted of the direct application of text-based systems to speech data , with some minor adaptations .
_Summary_from_Common_Sections_and_Unique_Sec $ 6 $ 146 $ 59 $ AP880823-0069 : 17 The ANC/TERM is the/DEF main guerrilla group fighting to overthrow the South African government and/O end apartheid/TERM , the/DEF system of racial segregation in which South Africa 's black majority has no vote in national affairs ./O
Computing_referring_expressions $ 4 $ 152 $ 2 $ The problem is the subject of this section .
Abstract $ 0 $ 6 $ 0 $ Long sentence analysis has been a critical problem because of high complexity .
Approach $ 2 $ 38 $ 11 $ Each node has a label , which offers a brief textual description of the node .
Architecture $ 2 $ 63 $ 19 $ One task is the acquisition of new structures , the second task is the evaluation of given structures .
Conclusion $ 7 $ 189 $ 0 $ Starting from the question what are the elementary units to consider for a text ' s discourse structure , I presented an account for prepositional phrases with adjunct-status as discourse units .
The_semantic_behavior_of_the $ 5 $ 187 $ 94 $ We cannot say "that children are HINOKI-tree" and "the company is the environmental pollution" while we can say "He is mild.
Indexing_and_Retrieval $ 5 $ 233 $ 0 $ The indexing/TERM process/TERM takes/DEF a group of document files and produces a new index ./O
OUT $ -1 $ 108 $ 54 $ The compound noun maple syrup ( i.e.
ADAM : _Architectural_Principles $ 3 $ 88 $ 65 $ ATLAS/TERM offers/DEF a threelayers solution to the problem of integrating different data storage formats by providing a logical level which consists of the language formalism and the API ./O
_Measurements $ 3 $ 85 $ 18 $ The first collection to be seen moving up the headline at a remove of two nodes ( the main verb and the vp ) is the conjunction of companies .
DTD_abstraction $ 2 $ 58 $ 6 $ ( Ahonen , 1995 ) uses a method to build document instances from tagged texts that consists of a deterministic finite automaton for each context model .
Architecture $ 2 $ 62 $ 18 $ Ontology learning operates on the extracted information and is used for three tasks.
Using_linguistic_constraints $ 3 $ 69 $ 9 $ LHS # RHS filters out rules with a single daughter which is the same category as the mother.
_Tagging_NuLL-marker_CDM_pairs_ ( via $ 8 $ 122 $ 52 $ Rule 22 : ( 1 , lift 3 .4 ) B2 = p Fcom > 1 class 2 [0 .667] which can be explained as : if the second word after the RDM is a preposition , and there is more then one commas before the current RDM , then the location of the NULL marker is two commas away from the RDM .
Results $ 8 $ 164 $ 3 $ Concerning tense , our " gold/TERM standard/TERM " is the/DEF set of human translations ./O
Multiple_heuristics_for_word_sense $ 2 $ 39 $ 14 $ Therefore we will give maximum score to the synset of a monosemous translation , that is , the translation which has only one corresponding synset .
Implementing_Embedded_MT $ 2 $ 74 $ 17 $ The implemented algorithm for language/code set identification is a trainable n-graph algorithm and has been discussed in more detail elsewhere ( Reeder & Geisler , 1998 ) .
Introduction $ 1 $ 26 $ 20 $ the English verb pocket or must be saturated by an exterfial argument ) are stated in terms of the pieces of LCS struct-ure in the lexicon.
Only_the_relevant_attributes_of_each_semantic_role $ 5 $ 131 $ 40 $ In syntactic generation the interlingua representation is converted by ' transformational rules ' into an ordered surface-structure tree , with appropriate labeling of the leaves with target language grammatical functions and features .
Embedding_Translation_in_an $ 6 $ 126 $ 24 $ The objective of this experiment is to measure the performance of a translation system in the context of an application .
Introduction $ 1 $ 23 $ 12 $ Traditional IR systems treat the query/TERM as a/DEF pattern of words to be matched by documents ./O
Abstract $ 0 $ 20 $ 17 $ All of these are the research fields of phase .
Selective_Sampling_Evaluation $ 4 $ 88 $ 12 $ The 47 set of probabilities of the possible parse trees for a sentence defines a distribution that indicates the grammar 's uncertainty about the structure of the sentence .
Problem_Space_Modeling $ 2 $ 42 $ 15 $ Reward/TERM and Punishment/TERM are the/DEF utility metrics corresponding to each sub-goal ( Winlder , 95 1972 ) depending upon the hypothesis of uncertainty of understanding and the level of importance ./O
_The_straightforward_unpacking_of_feature $ 3 $ 25 $ 17 $ The Information/TERM Gain/TERM of/TERM feature/TERM f/TERM is measured/DEF by computing the difference in uncertainty ./O
Background $ 2 $ 85 $ 65 $ informValue/TERM (/TERM p=v/TERM )/TERM : user/DEF provides value v for parameter p . p was requested ./O
Analysing_Czech_texts $ 3 $ 71 $ 0 $ Linguistic analysis of an input Czech text consists of a sequence of procedures depicted in Figure 1.
Motivation $ 2 $ 36 $ 0 $ We believe that , for humans , natural language is the best mechanism for information access .
Abstract $ 0 $ 22 $ 12 $ The lexicon entry or Root/TERM LCS/TERM (/TERM RLCS/TERM )/TERM of one sense of the Chinese verb xuel_jian3 is as follows : ( 1 ) ( act_on loc ( * thing 1 ) ( * thing 2 ) ( ( * [on] 23 ) loc ( *head* ) ( thing 24 ) ) ( cut+ingly 26 ) ( down+/m ) ) The top node in the .
_The_center_is_the_entity_which_is_most_likely $ 3 $ 32 $ 4 $ center remains the same but is not realised as Subject in Un+l ; SMOOTH SHIFT .
Dialogue_manager $ 6 $ 159 $ 51 $ To decide this , the dialogue manager regard s the certainty/TERM score/TERM between the utterance and the most similar KU as an/DEF appropriateness measure of the interpretation ./O
The_Argument_Generator $ 1 $ 80 $ 60 $ However , evaluations based on judgements along these dimensions are clearly weaker than evaluations measuring actual attitudinal and Arguing/TERM an/TERM evaluation/TERM involves/DEF an intentional communicative act that attempts to affect the current or future behavior of the addressees by creating , changing or reinforcing the addressees ' attitudes ./O
Related_work $ 6 $ 175 $ 26 $ For example , the GOLEM algorithm ( Muggleton and Feng , 1990 ) used relative/TERM least/TERM general/TERM generalisation/TERM ( rlgg/ACR ) .
Automatic_Extraction_of $ 4 $ 87 $ 36 $ The equation for two-element compound nouns is as follow : P ( x ,y ) I ( x;y ) = log 2 P ( x ) x P ( y ) 61 where x and y are two words in the corpus , and I/TERM (/TERM x;/TERM y/TERM )/TERM is the/DEF mutual information of these two words (/O in this order ) .
Introduction $ 1 $ 2 $ 0 $ We present ALLiS/TERM , a/DEF learning system for identifying syntactic structures which uses theory refinement ./O
Current_approach $ 3 $ 39 $ 0 $ Our algorithm also focuses on inflectional languages .
Ordering_Structures $ 6 $ 76 $ 6 $ At the VP level , it is thus useless of learn contexts in which VBG does not occur in a VP ( cases which mainly correspond to occurrences of VBG in NP ) .
_KNOWLEDGE_EXTRACTION $ 3 $ 155 $ 97 $ However the recall rate is not important , since the whole knowledge extraction process is a recurrent process .
Maximum_Entropy_Modeling $ 2 $ 53 $ 11 $ A feature/TERM of/TERM a/TERM context/TERM is a/DEF binary-valued indicator function ] expressing the information about a specific context ./O
Methodology $ 2 $ 53 $ 10 $ This gives the effect of placing the largest LL value at the top of the list representing the word which has the most significant relative frequency difference between the two corpora .
Applications_of_LexTract $ 4 $ 219 $ 73 $ For Srinivas ' and our grammars , the first line is the results tested on Section 23 , and the second line is the one for Section 22 .
_Annotation_Guidelines_I : $ 3 $ 60 $ 14 $ 2.VP : A VP/TERM is a/DEF phrase headed by a predicate ./O
Abstract $ 0 $ 130 $ 26 $ More features are introduced in our method , such as the linear order of entity names , the word ( s ) between the entity names , the relative/TERM position/TERM of/TERM the/TERM entity/TERM names/TERM ( in/DEF one/DEF sentence/DEF or/DEF in/DEF neighboring/DEF sentences/DEF ) , etc .
Towards_building_a_parallel_corpus $ 2 $ 53 $ 21 $ Each node is characterized by a status ( NUCLEUS or SATELLITE ) and a rhetorical/TERM relation/TERM , which is a/DEF relation that holds between two non/Overlapping text spans ./O
Stochastic_Surface_Realization $ 2 $ 73 $ 15 $ In other words , the most likely utterance is W* = arg max P ( WIu ) , where u/TERM is the utterance class ./O
Models_and_Modifications $ 2 $ 48 $ 35 $ Pi/TERM (/TERM c~/TERM )/TERM is the/DEF probability of beginning a derivation with c~ ;/O Ps/TERM (/TERM o/TERM I/TERM 77/TERM )/TERM is the/DEF probability of substituting o~ at 7 ;/O finally , Pa/TERM (/TERM NONE/TERM I/TERM 7/TERM )/TERM is the/DEF probability of nothing adjoining at ~/ ./O
Conclusion $ 7 $ 270 $ 0 $ This paper presented a scheme for integrating natural language processing and information retrieval by adopting a finite-state model of language and a ternary expression representation of document content .
OUT $ -1 $ 154 $ 83 $ A DMC/TERM of/TERM a/TERM given/TERM word/TERM w/TERM is a/DEF list of its microcontext elements ( MCEs ) ./O
Learning_Algorithms_Tested $ 2 $ 44 $ 24 $ In this setting , a Decision/TERM List/TERM is a/DEF list of features extracted from the training examples and sorted by a log-likelihood measure ./O
Introduction $ 1 $ 7 $ 3 $ The Deep/TERM Read/TERM reading/TERM comprehension/TERM prototype/TERM system/TERM ( Hirschman et al . , 1999 ) achieves/DEF a level of 36% of the answers correct using a bag/Of-words approach together with limited linguistic processing ./O
OUT $ -1 $ 157 $ 86 $ if there is an SDR [w , DT , wd or [wl , DT' , w] , then w~ and the dependency type DT or DT' , respectively , constitute a mierocontext element [DT , wd or [wl , DT'] , respectively , of the word w. The first case implies that w is a head word in the SDR and in the second case the word w is a dependant.
Results $ 8 $ 191 $ 30 $ While these numbers are small , this preliminary data seems to suggest again that atelicity/TERM is a/DEF good cue for cotemporality ,/O while telicity is not a sufficient cue.
Introduction $ 1 $ 18 $ 9 $ The markup will help disclose the underlying logical structure of documents .
The_CGS_system $ 4 $ 68 $ 0 $ The Caption/TERM Generation/TERM System/TERM ( CGS/TERM ) generates/DEF explanatory captions of graphical presentations ( 2D charts and graphs ) ./O
Given_an_input_space_X~*_of $ 3 $ 75 $ 44 $ Probabilistic/TERM learners/TERM usually/DEF associate to uncertain information a measure of the confidence the system has in that information ./O
Related_Work $ 4 $ 127 $ 36 $ From the result state of each alternative , the planner then tries to predict the reaction of A by simulating the execution of the React action by A ( see figure 5 ) , and commits to the plan whose resulting state after the predicted reaction yields the greater utility according to B ' s preferences ( see Figure 6 ) .
Introduction $ 1 $ 5 $ 1 $ Annotated dialogue corpora are of crucial importance for the development of vocal applications .
Architecture_of_WIT-Based_Spoken $ 3 $ 35 $ 1 $ 3 .1 Speech Recognition The speech/TERM recognition/TERM module/TERM is a/DEF phonemeHMM-based speaker-independent continuous speech recognizer that incrementally outputs face Toolldt ./O
_User : _Ok $ 9 $ 215 $ 29 $ Our future work includes conducting evaluation of the hypotheses and the system and investigating machine learning techniques for improving utility adjustments .
Relevant_works $ 9 $ 217 $ 7 $ Second , Czech/TERM language/TERM is a/DEF free word-order language what/O implies that the process of recognition of the verb group structure is much more difficult .
Results $ 3 $ 85 $ 31 $ For all phrase types , the system yields substantially better results than any previously published.
Evaluation $ 4 $ 79 $ 3 $ We also define ' coverage/TERM ' as the/DEF proportion of linked senses of Korean words to all the senses of Korean words in a test set ./O
The_Learning_System $ 2 $ 31 $ 5 $ We concentrate on the description of word/TERM order/TERM parameters/TERM , which reject/DEF the basic order in which constituents occur in different languages ./O
Danish_Data $ 4 $ 123 $ 49 $ Especially noticeable is the Danish use of discourse deictics in cases where elliptical constructions are normal in English .
System_Description $ 2 $ 15 $ 4 $ In addition to the control module , which wires together the other modules , there are six modules in • GoDiS : input/TERM , which receives/DEF input from the user ;/O interpret/TERM , which interprets/DEF utterances as dialogue moves with some content ;/O generate/TERM , which generates/DEF natural language from dialogue moves ;/O output/TERM , which produces/DEF output to the user ;/O update/TERM , which updates/DEF the information state based on interpreted moves ;/O and select/TERM , which selects/DEF the next move ( s ) to perform ./O
Abstract $ 0 $ 65 $ 4 $ In the concrete case , this model is a grammar ; LEXICAL/TERM semantics/TERM determines/DEF the separate constraints that can go into a description and/O COMPOSITIONAL/TERM semantics/TERM determines/DEF how these constraints can share variables and so describe common objects ./O
Problem_Space_Modeling $ 2 $ 50 $ 23 $ Reward and punishment can be predetermined and then re-adjusted later when the user and the group modeling progresses .
Results $ 3 $ 114 $ 75 $ The average recall over all users and all • texts is 66 . 7% for Y and 75% for N . These averages create for the Y and N chart the respective cutoff boundaries for " YES " ( text output is acceptable for filtering ) and " NO " ( it is not ) .
Evaluation $ 3 $ 102 $ 11 $ recall ( 5 ) precision + recall 3 . 2 Results Table 1 gives the raw results for the 14 verbs using each method .
Abstract $ 0 $ 2 $ 1 $ We discuss these concepts and the way they are implemented in the architectural framework of the ADAM/TERM corpus/TERM , which is a/DEF corpus of 450 Italian spontaneous dialogues ./O
BOV_Stem_NE_Defaults_Qspecific_41 $ 5 $ 74 $ 44 $ As already noted , 3 Question Type Default Eliminate Who title dateline What 1st story line ( none ) When dateline ( none ) Where dateline title Why 1st story line title , dateline Figure 3 : Default and eliminable sentences in the " Default " strategy " why " questions are the most difficult for bag-of-words .
_Otherwise ,_add_to_the_current_context_new $ 6 $ 89 $ 6 $ 4 The positive~negative effect of a node X on a node Y is the hypothetical belief in node Y after propagating a high/low belief in node X ( which represents a true/false belief in the corresponding proposition ) .
Maximum_entropy-based_parse $ 2 $ 40 $ 11 $ The statistical features used for parse selection should contain information pertinent to sentence structure , as it is the information encoded in these features which will be brought to bear in prefering one parse over another .
Abstract $ 0 $ 97 $ 4 $ The first type represents inheritance relationships among elements within a single document.
Abstract $ 0 $ 1 $ 0 $ This paper describes a method of comparing corpora which uses frequency profiling .
_Background $ 2 $ 29 $ 13 $ Chinese/TERM is a/DEF syllable-based language , where each syllable carries a lexical tone ./O
Knowledge_base $ 5 $ 97 $ 11 $ </KU> In Japanese , there are many sentential patterns to express if-then relation.
Introduction $ 1 $ 31 $ 27 $ However , in case k = 3 , the expansion/TERM probabilities/TERM depend/DEF on the states that are defined by the node label , the number of descendents the node and the sequence of labels in the descendents (/O if any ) .
Comparing_Three_Treebank $ 4 $ 108 $ 41 $ Truly unmatched templates A truly/TERM unmatched/TERM template/TERM is a/DEF template that does not match any template in the other Treebank even if we assume both Treebanks are perfectly annotated ./O
OUT $ -1 $ 78 $ 60 $ The semantic classes used by Quarc are shown below , along with a description of the words assigned to each class.
_Methods $ 2 $ 56 $ 9 $ The pre-recorded dialogues were copied from CD and digitised for analysis at 22 kHz using Entropic ' s ESPS/ Waves + speech analysis software running on a Sun workstation in the Phonetics Laboratory of the UniversiW of Melbourne .
Multiple_heuristics_for_word_sense $ 2 $ 25 $ 0 $ disambiguation As the mapping method described in this paper has been developed for combining multiple individual solutions , each single heuristic/TERM must be seen as a/DEF container for some part of the linguistic knowledge needed to disarnbiguate the * This/O research was supported by KOSEF special purpose basic research ( 1997 .92000 .8 #970-1020-301-3 ) Corresponding author 142 ambiguous WordNet synsets .
Abstract $ 0 $ 12 $ 11 $ Phrasal terms are utilized either as replacement of single words or as supplemental units for single words , but according to our experience , phrasal terms as replacement of single words do not perform well .
Abstract $ 0 $ 59 $ 2 $ These types include : the explicit topic of the document , the situation , the identification of the problem , the 'identification of the solution , the research goal , the explicit topic of a section , the • authors ' development , the inferences , the description of a topical entity , the definition • of a topical entity , the relevance of a topical enthy , the advantages , etc .
Introduction $ 1 $ 61 $ 57 $ 3.1 The input structure The input of the CLEF generation system is a hierarchical representation ( i.e.
Psycholinguistic_production $ 3 $ 40 $ 5 $ Roelofs [ 1996] , for instance , argues that if a number of nodes representing semantic features are the basis for lexical access , in lemma retrieval it becomes extremely difficult to control the activation spread in such a way that only the most specific lexical unit that combines these features gets selected .
The_Generation_System $ 3 $ 83 $ 41 $ On the other hand , ambiguity can also be introduced at the decomposition stage , if multiple lexical entries can match a single structure The result of the decomposition process is a match-structure indicating the hierarchical relationship between all lexical entries , which , together cover the input CLCS .
Learning_Linear_Classifiers $ 4 $ 115 $ 20 $ SNoW determines the features ' weights using an on-line/TERM algorithm/TERM that attempts/DEF to minimize the number of mistakes on the training data using a multiplicative weight update rule (/O Lit88 ) .
Preferences_among_coherence $ 2 $ 51 $ 0 $ features We claim that it is the relative preferences among features rather than the absolute magnitude of each individual one that play the crucial role in the production of a coherent text.
MALIN $ 4 $ 129 $ 23 $ Figure 3 shows an example OPM which represents the request Which bus lines passes the North gate ? .
Abstract $ 0 $ 153 $ 122 $ ' From the above tagging , we can obtain the following discourse structure with embedding relations : A dversativity ( & F ( 14 ) , Sufficiency ( F rontClause ( 15 ) , BackClause ( 15 ) ) ) where &F/TERM (/TERM n/TERM )/TERM denotes the/DEF Front discourse segment of an inter-sentence rhetorical relation whose sequence number is n ./O We can define &B ( n ) similarly .
OUT $ -1 $ 202 $ 76 $ The Formulaic feature , which is not very strong on its own , is the most diverse , as it contributes to the disambiguation of six categories directly .
Proper_name_recognition $ 4 $ 115 $ 41 $ Likewise the unknown word clinton is ( incorrectly ) interpreted as a common noun in ( 11 ) , as it is the last item of a noun phrase introduced by a determiner , but it becomes a proper name if another noun follows .
Abstract $ 0 $ 11 $ 9 $ Obviously , the latter is a very expensive task since there are so many documents in a collection and there is not yet a reliable machine translation system that can be used to process automatically .
Introduction $ 1 $ 16 $ 11 $ Each participant is to return a ranked list of the five best answer strings for each question , where each answer/TERM string/TERM is a/DEF string of 50 bytes ( or 250 bytes ) that contains an answer to the question ./O
The_TABULATE_ILP_Method $ 3 $ 67 $ 1 $ 3 .1 The Basic TABULATE Algorithm Most ILP methods use a set-covering method to learn one clause ( rule ) at a time and construct clauses using either a strictly top-down/TERM ( general/DEF to specific )/O or bottom-up/TERM ( specific/DEF to general )/O search through the space of possible rules ( Lavrac and Dzeroski , 1994 ) .
OUT $ -1 $ 177 $ 135 $ The resulting curve is a measure of the correlation between the true distribution probability and the probability of the most likely chunk tag , i.e .
Models $ 2 $ 58 $ 38 $ Perplexity/TERM is a/DEF good indicator of Z ( hi ,s ) where A ( i ,Ss ,l ) gives the partition for the current position ,/O B/TERM (/TERM s/TERM ,/TERM t/TERM )/TERM gives/DEF the partition for the current word pair ,/O and following the usual convention , aA ( i ,j~ ,0 ,S ( s ,t ) is zero if these are undefined.
Interlingua_system_ ( ISS ) $ 3 $ 70 $ 1 $ This system , named lnterlingua/TERM Slot/TERM Structure/TERM ( 1SS/TERM ) , generates/DEF an interlingua representation from the SS of the sentence ./O
Conclusion $ 4 $ 131 $ 13 $ Finally , what may sound right for a human speaker may sound awkward for a computer , but we believe that mimicking a human , especially a domain expert , is the best we can do , at least for now .
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 251 $ 216 $ Each of the documents is flagged as to whether it discusses the target event , and these flags ( 'YES' , 'NO' ) are the only information used tbr training the system to correctly classiC " the target event .
Introduction $ 1 $ 38 $ 30 $ Our classification , which is a further development of the scheme in Teufel and Moens ( 1999 ) , can be described procedurally as a decision tree ( Figure 2 ) , where five questions are asked about each sentence , concerning intellectual attribution , author stance and continuation vs . contrast .
Introduction $ 1 $ 12 $ 3 $ A non-restrictive/TERM component/TERM gives/DEF additional information to a head that has already been viewed as unique or as a member of a class that has been independently identified , therefoee is not ' essential for the identification of the head ' (/O Quirk et al . , 1985 ) .
Topic_Analysis $ 4 $ 99 $ 32 $ The Shannon/TERM information/TERM of/TERM word/TERM w/TERM in text t is defined as I/DEF ( w ) = -N ( w ) logP ( w ) ,/O where N/TERM (/TERM w/TERM )/TERM denotes the/DEF frequency of w in t ,/O and P/TERM (/TERM w/TERM )/TERM the/DEF probability of the occurrence of w as estimated from corpus data ./O
Validating_each_tagger_into_its_respective $ 1 $ 24 $ 0 $ domain In order to conduct the comparative study , we used two different morphological analysers; each one has a specific lexicon tailored for its application field.
_Generation_of_the_surface_phrase_from_the $ 4 $ 172 $ 135 $ The time/TERM for/TERM Question-a/TERM is a/DEF sum of the times for Questions al and a2 ./O
Abstract $ 0 $ 2 $ 0 $ This paper describes an implemented system which uses centering theory for planning of coherent texts and choice of referring expressions .
Experiments $ 4 $ 87 $ 8 $ GRAPHON/TERM , finally , is/DEF a grapheme-to-phoneme conversion task for English based on the English Celex lexical database ./O
The_REXTOR_System $ 5 $ 169 $ 47 $ The first extraction rule defines a NounGroup/TERM as a/DEF sequence consisting of : an optional possessive pronoun or determiner , any number of adjectives , one or more nouns ( of any type ) ./O
Corpora $ 3 $ 65 $ 7 $ The MEDLINE/TERM database/TERM is an/DEF online collection of abstracts for published journal articles in biology and medicine and contains more than nine million articles ./O
October_2000 $ 7 $ 11 $ 9 $ The SIGLEX ' 00 Program Committee enabled us to work within a very brief time frame , by quickly turning around reviews for the substantial number of submissions to the conference .
Introduction $ 1 $ 21 $ 7 $ The learning system is equipped with a UG/TERM and associated parameters , encoded/DEF as a Unification-Based Generalised Categorial Grammar ,/O and a learning/TERM algorithm/TERM that fixes/DEF the values of the parameters to a particular language ./O
OUT $ -1 $ 75 $ 55 $ The/TERM Common/TERM CJK/TERM Ideograph/TERM section/TERM of the Unicode encoding scheme includes/DEF all characters encoded in each individual language and encoding scheme ./O
The_Learning_System $ 2 $ 60 $ 34 $ For the learner , the information about subjects ( subjdir = backward ) has already been acquired while learning intransitive verbs , and the learner does not need to learn it again for transitive verbs , which not only inherit this information , but also have the direction for the object defined by vargdir ( vargdir = forward ) , as shown in figure 3 .
Word_Clustering $ 3 $ 48 $ 4 $ For a given data sequence z m = xl . . . zm and for a fixed probability model M , 1 the stochastic complexity of x m relative to M , which we denote as SC/TERM (/TERM x/TERM m/TERM :/TERM M/TERM )/TERM , is defined as the/DEF least code length required to encode x rn with M (/O Rissanen , 1996 ) .
Narrative_Summarization $ 5 $ 378 $ 7 $ 2 ) 2.2.2 text = `` Latest document summary '' audio = text = ere ate ( `` summarize -gen eric -compression .1/peru/p32 '' ) 2.3 Biographies 2.3.1 audio = `` A profile of @ 2 .
Hyperonymy_in_lexical_semantics $ 5 $ 129 $ 32 $ We thereby open the door to both ' vertical ' and ' horizontal ' lexical choice within a hierarchy , which raises a number of questions : * What is the granularity of conceptual , and that of lexical knowledge ?
Enriching_the_Feature_Set_with $ 2 $ 89 $ 19 $ c/TERM (/TERM x~/TERM ,wz/TERM )/TERM represents the/DEF count of the event that x and y occur adjacent and in this order in the training corpus ./O
The_Experiment $ 5 $ 201 $ 27 $ Some details on measures based on subjects' self-reports can be examined in Figure 4 , which shows an excerpt from the final questionnaire that subjects are asked to fill out at the end of the interaction.
Acquisition_Process $ 4 $ 135 $ 36 $ If this stem exists , we need to do find out whether or not the dictionary entry describes the same concept as contained in the ontology .
Previous_Work $ 3 $ 74 $ 20 $ Similarly , PLNLP/TERM ( Heidorn , 1972 ; Jensen et al . , 1993 ) is a/DEF programming language for writing phrase structure rules that include specific conditions under which the rule can be applied ./O
Determination_Schemes_of $ 4 $ 148 $ 33 $ The set of segmentable/TERM positions/TERM T~ is defined somewhat differently as : : D/DEF = {wi , wsj I ( Icc , v , -= lcc~ , ) = 1 or ( Icws~ =-IcC.ws~ ) = 1} ,/O where wsj/TERM denotes a/DEF word set to which the jth word in a sentence belongs ./O
Maximum_entropy-based_parse $ 2 $ 30 $ 1 $ In the present approach , parses are ranked according to their goodness by a statistical model built using the maximum/TERM entropy/TERM technique/TERM , which involves/DEF building a distribution over events which is the most uniform possible , given constraints derived from training data ./O
OUT $ -1 $ 134 $ 97 $ Resuming the example shown in Figure 1 , this is the case of the UW "on" in ( lb ) : the preposition 'on' fills in the position feature of the verb 'sit' and , thus , is represented in UNL correspondingly as the second term of the binary relation 'plc' and the first term of 'obj'.
Multiple_heuristics_for_word_sense $ 2 $ 35 $ 10 $ Hi ( s , ) = max support ( s , , ew~ ) 1 ~ ' ~ , ( n-1 ) +a k , =l where EWi = ( ewl s , ~ synset ( ew ) } In this formula , Hi/TERM (/TERM s/TERM )/TERM is a/DEF heuristic score of synset s ,/O s/TERM is a/DEF candidate synset ,/O ew/TERM is a/DEF translation into English ,/O n/TERM is the/DEF number of translations and/O synset/TERM (/TERM ew/TERM )/TERM is the/DEF set of synsets of the translation ew ./O
Rule_set_learning $ 3 $ 53 $ 15 $ Related to this , ( Freitag , 1998 ) uses words to learn only slot rules ( learned from text-relation examples ) , selecting as negative those non-positive word pairs that define a string as neither longer than the maximum length in positive examples , nor shorter than the minimum.
Introduction $ 1 $ 39 $ 34 $ Section 5 describes the user interface of the system.
Introduction $ 1 $ 30 $ 22 $ Figure 2 : Annotation Scheme for Argumentative Zones Our hypothesis is that a segmentation based on regularities of scientific argumentation and on attribution of intellectual ownership is one of the most stable and generalizable dimensions which contribute to the structure of scientific texts .
Abstract $ 0 $ 7 $ 6 $ Since the central barrier to developing such a system today is the incompleteness of the knowledge base , we outline a strategy starting with the implementation of a series of form-based resolution algorithms that are applied directly to the referring expressions of the input text .
Grammar_Induction $ 3 $ 62 $ 1 $ When the training corpus consists of a large reservoir of fully annotated parse trees , it is possible to directly extract a grammar based on these parse trees .
Introduction $ 1 $ 25 $ 13 $ Article choice is particularly important for this application : many AAC users drop articles and resort to a sort of telegraphese , but this causes degradation in comprehension of synthetic speech and contributes to its perception as unnatural and robot-like .
OUT $ -1 $ 195 $ 190 $ The second tree is a default tree that just classifies any sentence as not an answer .
Abstract $ 0 $ 33 $ 8 $ In section 3 Eckert and Strube ' s algorithm is introduced and in 4 the Danish personal and demonstrative prononn~ are described with focus on discourse deictics in dialogues .
ADVF_Far_adverbs_ ( ~'l $ 7 $ 92 $ 7 $ In } ( ~ ) D : a set of 3-tuple in the form of {governor , dependant , dependency-relation} , which represents dependency relations between blocks.
Solving_the_generation_problem $ 5 $ 150 $ 10 $ Along with these goals , the dialogue manager supplies its communicative/TERM context/TERM , which represents/DEF the centrality of the house in attentional prominence , cognitive status and information structure ./O
Word_Clustering $ 3 $ 59 $ 15 $ Let w m ' ' be the sequence of all wi ' s ( wi E w rn ) such that its corresponding si is 1 , where ms denotes the number of l ' s in s ~ .
_Otherwise ,_add_to_the_current_context_new $ 6 $ 125 $ 42 $ The argument by cases consists of a pair of Argument Graphs , one graph for each case.
Informational_content_of_sentences $ 2 $ 24 $ 0 $ Cluster-based/TERM sentence/TERM utility/TERM ( CBSU/TERM , or utility/TERM ) refers to the/DEF degree of relevance ( from 0 to 10 ) of a `` particular sentence to the general topic of the entire cluster (/O for a discussion of what is a topic , see [ Allan et al .
Conclusion $ 7 $ 367 $ 0 $ In this paper , we proposed a method for extracting key paragraph for summarization based on distinction between a topic and an event .
PAR_as_anIL $ 4 $ 22 $ 11 $ ( 1 ) The bottle . floated out La boteUa sali6 flotando ( the bottle exited floating ) ( 2 ) I blew out the candle Apagud la vela sopldndola ( I extinguish the candle blowing ) 4 . 1 In order to capture generalizations about motion actions , we have a generalized PAR schema for motion , and our hierarchy includes different types of motion actions such as inherently directed motion and manner of motion actions that inherit from the more general schema , as shown in Figure 4 .
Approach $ 2 $ 17 $ 11 $ In memory-based/TERM learning/TERM the/DEF training data is stored and a new item is classified by the most frequent classification among training items which are closest to this new item ./O
CoreLex-II $ 3 $ 42 $ 6 $ Comparison of sense distributions is now performed over synsets on all levels , not just over a small set on the top levels.
Introduction $ 1 $ 16 $ 7 $ DTDs/TERM determine/DEF the logical structure of documents and how to tag them accordingly ./O
Future_Work $ 7 $ 178 $ 0 $ The biggest remaining step is a more careful evaluation of different sub-systems and preference strategies to more efficiently process very ambiguous and complex inputs , without substantially sacrificing translation quality .
Selection_of_candidate_strings $ 2 $ 38 $ 14 $ The existence of a single-character string is the necessary but not sufficient condition for a new word .
Introduction $ 1 $ 65 $ 57 $ It is a part of the class 'computer' .
_Proposed_method $ 2 $ 41 $ 11 $ The required linguistic/TERM knowledge/TERM resource/TERM is a/DEF lexical ontology that has the words in the target language and a listing of their associated senses ./O
About_Theory_Refinement $ 2 $ 12 $ 4 $ ( Mooney , 1993 ) defines it as : Theory/DEF refinement systems developed in Machine Learning automatically modify a Knowledge Base to render it consistent with a set of classified training examples ./O
OUT $ -1 $ 37 $ 37 $ The number of documents in a batch may vary from a few to hundreds.
Discussion $ 6 $ 220 $ 42 $ A ternary expression representation of natural language mimics its syntactic organization , and hence sentences that differ in surface form but are close in meaning will not map into the same structure .
Information_Structures $ 3 $ 109 $ 32 $ R1 between C1 and C2 should be interpreted as " C1 is the RI of C2 " .
Memory-Based_Language $ 1 $ 14 $ 8 $ IGTREE/TERM is a/DEF variant in which an oblivious decision tree is created with features as tests , and in which tests are ordered according to information gain of the associated features ./O
Highlight $ 3 $ 74 $ 23 $ Deriving semantic relationships from this is not particularly reliable , so it is preferable to use search terms which rely more on positional clues.
Introduction $ 1 $ 10 $ 4 $ A dialogue/TERM move/TERM engine/TERM ( DME/TERM ) updates/DEF the information state on the basis of observed dialogue moves and selects appropriate moves to be performed ./O
OUT $ -1 $ 0 $ 0 $ Multi-Document Summarization By Sentence Extraction Jade Goldstein* Vibhu Mittal t Jaime Carbonell* Mark Kantrowitzt jade@cs . cmu . edu mittal@jprc . com jgc@cs . cmu . edu mkant@jprc . com *Language Technologies Institute Carnegie Mellon University Pittsburgh , PA 15213 U . S . A . tJust Research 4616 Henry Street Pittsburgh , PA 15213 U . S . A .
Abstract $ 0 $ 8 $ 5 $ Features of word senses and the significance of word contexts are analysed and possibility of searching based on word senses instead of mere words is examined .
System_Overview $ 2 $ 28 $ 2 $ 2 The Tree/TERM Chooser/TERM uses/DEF a stochastic tree model to choose syntactic properties ( expressed as trees in a Tree Adjoining Grammar ) for the nodes in the input structure ./O
Optimizations $ 5 $ 89 $ 4 $ We can similarly define GoodPotential 1 to0 ( S ) , and then define GoodPotential/TERM (/TERM S/TERM )/TERM = max/DEF ( GoodPotential 0 to_l ( S ) ,/O GoodPotential 1 to O ( S ) ) As we construct the RRE-tree , we keep track of the largest Goodness ( S ) we have encountered.
Background $ 2 $ 52 $ 0 $ 2.3 Dialogue Primitives Following the procedure outlined in Section 2.4 , the dialogue/TERM manager/TERM calculates/DEF a bag of primitives for each turn and speaker ./O
Normalization_and_tagging $ 3 $ 57 $ 0 $ The label normalization groups three components , which clean up and tokenize the input .
Conclusions_and_future_work $ 6 $ 220 $ 3 $ The statistical results partially justify our claim that it is the preferences among generation features that decide the coherence of a text .
OUT $ -1 $ 268 $ 78 $ An example is the annotation framework recently proposed by Bird and Liberrnan ( 1999 ) which is based on annotation graphs.
Abstract $ 0 $ 63 $ 30 $ Most importantly in SFG the only irreducible definition of meaning , or structure , is a set of contrasts between events , or observations .
_Final_Observations $ 5 $ 132 $ 4 $ More interesting is the fact that some node transitions will certainly be different from others in their practical implementation and this should probably be factored into the cost calculation .
System_Description $ 2 $ 46 $ 35 $ The ( supposedly ) shared part of the IS consists of three subparts.
_Current_Status_of_the_Sinica $ 5 $ 133 $ 0 $ Treebank and On-line Interface Following the above criteria and principles , we have already finished Sinica Treebank 1.0.
Related_Work $ 2 $ 47 $ 20 $ The basic semantic relation used in their systems is the synonymy relation.
_The_+END+_ ( null ) _postmodifier_con $ 8 $ 77 $ 24 $ Lexical Model The desired output structure of our combined parser/word-sense disambiguator is a standard , Treebank-style parse tree , where the words not only have parts ef speech , but also WordNet synsets.
See_the_report_entitled_ $ 3 $ 24 $ 14 $ related w~ tagged TL do domain wore Figure 1 Analysis Template Measures 2 An Embedded MT System Design 4 Our three systems process documents using a sequence of three software modules.
From_References_to_Coherence $ 5 $ 146 $ 6 $ Since ~the motherboard" in ( l-a ) is a definite noun phrase and syntactic as well as conceptual information match with the plausible antecedent "the P6LXZ-A' , a referential link can be established , see the ISCOREFERENTIAL relation in Figure 6 .
Related_Research_and_Motivation $ 1 $ 57 $ 37 $ One of the authors of this paper is not a native speaker of Chinese or Japanese but has the intermediate level proficiency in both languages now .
Implementing_Embedded_MT $ 2 $ 104 $ 0 $ 2.4 Lexicon Update The highest portion of the cost of providing a machine translation capability reflects the amount of lexicography that must be done as much as 70 % of the cost of a machine translation engine .
Corpus_comparison_based_on $ 6 $ 109 $ 0 $ information theoretical measures In this section we attempt to present measures that overcome some of the limitations of the classtoken method.
Comparison_experiment $ 3 $ 151 $ 53 $ For example , if the question is " Who is the inventor of the electric light?
Introduction $ 1 $ 14 $ 10 $ There is a great diversity among the web pages in terms of document length , style , and content .
Reinterpretation_of_CGS_in_RAGS $ 5 $ 109 $ 9 $ Text Planner The input to the Longbow text planner discussed in section 4 above is a representation of a picture in SAGE/TERM format/TERM ( which has been annotated/DEF to indicate the types of complexity of each grapheme )/O together with a goal , which can typically be interpreted as `` describe '' .
OUT $ -1 $ 69 $ 16 $ Mandarin Chinese presents a challenge for word-level indexing by LVCSR , because of the ambiguity in tokenizing a sentence into words ( as mentioned earlier ) .
Abstract $ 0 $ 164 $ 17 $ Relating to the highest unit providing antecedents works only when there is a mini segment boundary every time an anaphoric expression is used .
OUT $ -1 $ 77 $ 26 $ Figure 1 : Aggregation examples of ( Mellish et al . , 1998a ) which uses a joint relation to connect every two text spans that do not have a semantic relation other than objectattribute elaboration and conjunct/disjunct in between .
Significance_of_word_contexts $ 2 $ 70 $ 22 $ The investigation of word contexts is the most important , essential , unique and indispensable means of understanding the sense of words and texts .
Abstract $ 0 $ 112 $ 10 $ We are investigating whether this is a result of the BNC discussing weather more often , or a result of which particular grammatical structures are used to describe the weather floods in British and American English.
Attribute_Grammars $ 3 $ 53 $ 0 $ An attribute/TERM grammar/TERM consists/DEF of a context-free grammar , a finite set of attributes , and a set of semantic rules ./O
Text_Summarization $ 1 $ 21 $ 1 $ The informative evidence associated with techniques used in summarization may also provide clues for text categorization to determine the appropriate category of the document .
_Relations_that_are_relevant_and_correct_in $ 3 $ 37 $ 3 $ We define a synonymy/TERM relation/TERM as a/DEF binary relation between two synonym terms (/O with respect to • a particular sense ) .
Experimental_Setup $ 5 $ 156 $ 10 $ We asked 2 native Japanese who have an intermediate level understanding of Chinese language and who are the fluent users of the Internet search engines , to formulate 5 queries each in natural Japanese .
Evaluation $ 5 $ 134 $ 1 $ There is a line for each document size considered.
Abstract $ 0 $ 46 $ 42 $ Another ( integrative ) approach improves the language model accuracy using more sophisticated recognizers , instead of a complementary language model .
Abstract $ 0 $ 2 $ 1 $ In particular , we get improved results by incorporating these features : ( i ) more extensive treatment of capitalization for unknown words ; ( ii ) features for the disambiguation of the tense forms of verbs ; ( iii ) features for disambiguating particles from prepositions and adverbs .
Related_Work $ 7 $ 170 $ 1 $ ( 2000 ) describes a text mining tool that performs document clustering and text summarization .
_Amanda_Huggenkiss ,_she_proposes_to_meet_you $ 2 $ 12 $ 5 $ Our practical goal is to enhance the effectiveness of a wearable device that provides spoken advice to a user operating in a real-world physical environment .
OUT $ -1 $ 167 $ 125 $ The resulting curve is a measure of the correlation between the true probability distribution and the one given by the classifier .
_The_identity_of_the_speaker ,_denoted_as_the $ 6 $ 163 $ 53 $ Finally , an important issue is the comparison of the results obtained in our experiments to these generatedby alternative techniques proposed by other researchers.
Experimental_results $ 3 $ 153 $ 99 $ When running SMART on this re-segmented corpus , we obtain an average precision of 43.42% , which shows a slight improvement of 1.2% .
Introduction $ 1 $ 14 $ 7 $ We describe the unified learning framework and show that , in addition to explaining the success and robustness of the statistics based methods , it also applies to other machine learning methods , such as rule based and memory based methods .
Stochastic_Topic_Model $ 2 $ 43 $ 12 $ Hence , STM/TERM is a/DEF natural representation of statistical word occurrence based on topics ./O
Building_a_reusable_lexical_chooser $ 2 $ 18 $ 1 $ There are two main reasons why such a lexical chooser has not been developed in the past : 1 .
Conclusion $ 9 $ 184 $ 2 $ We believe there is a stronger role that NL generation can play in the narrative aspects of our briefings , which currently rely for the most part on canned text .
Dialog_Management $ 3 $ 120 $ 59 $ User expertise level and corresponding dialog strategies 98 3.3 Algorithm The proposed algorithm for action planning , content selection and content realization is given in Figure 3.
Abstract $ 0 $ 16 $ 15 $ Summarization-based algorithms for text categorization are outlined in Section 3 .
Brief_introduction_to_neural_networks $ 2 $ 117 $ 14 $ Since the knowledge the network acquires is a result of the mappings , how the input and output is represented is of great importance .
The_TransType_model $ 2 $ 69 $ 54 $ The total/TERM probability/TERM of the ith target-text token ti is just the/DEF average of the probabilities with which it is generated by each source text token sj ;/O this is a weighted average that takes the distance from the generating token into account : is1 p ( tils ) = ~p ( tilsj ) a ( jli , Is[ ) j=O ( 3 ) where p/TERM (/TERM ti/TERM Is/TERM j/TERM )/TERM is a/DEF word-for-word translation probability ,/O Isl/TERM is the/DEF length ( counted in tokens ) ofthe source segment s under translation ,/O and a/TERM (/TERM jli/TERM ,/TERM Is\/TERM ]/TERM )/TERM is the/DEF a priori alignment probability that the target-text token at position i will be generated by the source text token at position j ;/O this is equal to a constant value of 1~ ( Is I + 1 ) for model 1 .
Introduction $ 1 $ 32 $ 16 $ The column labeled idf/TERM is the/DEF mean idf for the terms in each bin ./O
Previous_Schemes_for_Grunt $ 2 $ 30 $ 2 $ The central inspiration here is the fact that grunts/TERM are unlike words , in that they contain/DEF sounds which are never seen in the lexical items of the language ./O
POS_Assignment $ 3 $ 85 $ 25 $ For example , N ( vl2 ( c ) ) Pv12 ( c ) = N ( c ) where N/TERM (/TERM v12/TERM (/TERM c/TERM )/TERM )/TERM is the/DEF number of occurrences of a character in the first position of a two-character verb while/O N/TERM (/TERM c/TERM )/TERM is the/DEF total number of occurrences of this character in the dictionary headwords ./O
Introduction $ 1 $ 17 $ 13 $ An event/TERM cluster/TERM , produced by a TDT system , consists/DEF of chronologically ordered news articles from multiple sources , which describe an event as it develops over time ./O
OUT $ -1 $ 93 $ 92 $ al , a2 , refo ) [ compellingness ( o , al , a2 , refo ) [ >px+ko'x , where o , al , a2 and refo are defined as in the previous Def; opop/TERM is an/DEF objective population (/O e .g . , siblings ( o ) ) , and I opopl >2 pe opop; xeX = [compellingness ( p , al , a_~ , refo ) l gx/TERM is the/DEF mean of X ,/O ~x/TERM is the/DEF standard deviation and/O k/TERM is a/DEF user/DEFined constant We/O have defined similar measures for arguing the value of a single entity and we named them s-compellingness and s-notably-compelling? .
_Instrumentalists_not_including_string_players $ 8 $ 143 $ 131 $ • 3 inappropriate clusters that relate homonyms , such as band 2 ,7 ( strip or stripe of a contrasting color or material/unofHcial association of people ) .
PAR_as_anIL $ 4 $ 23 $ 12 $ Directed motion actions , such as enter and exit , don ' t bring with them the manner by which the action is carried out but they have a inherent termination condition .
Introduction $ 1 $ 9 $ 2 $ Indeed , logical approaches may have a relevant impact at the level of semantic interpretation , where a logical representation of the meaning of a sentence is important and useful ( Mooney , 1999 ) .
Introduction $ 1 $ 19 $ 15 $ Section 2 describes our choice of the feature selection and extraction methods .
OUT $ -1 $ 198 $ 72 $ Wiebe ( 1994 ) also reports segment-based agenthood as one of the most successful features .
Abstract $ 0 $ 21 $ 18 $ The second part briefly describes neural networks.
Interclausal_Coherence $ 4 $ 124 $ 34 $ In the simplest case , the relation/TERM is one/DEF of Elaboration ./O
Architecture_of_WIT-Based_Spoken $ 3 $ 78 $ 44 $ Precisely speaking , the participant having the initiative is the one the system assumes has it in the dialogue .
OUT $ -1 $ 147 $ 96 $ ( Mellish et al . , 1998a ) summarises the genetic/TERM algorithm/TERM roughly as follows : quences/DEF by loosely following sequences of facts where consecutive facts mention the same entity ./O
Feature_merging_and_overfitting $ 3 $ 71 $ 14 $ The merged elements in this case are the lexical items offered and allow .
The_Filtering_Problem $ 1 $ 4 $ 0 $ How do people quickly determine whether a particular foreign language text document is relevant to their interest when they do not understand that foreign language?
The_semantic_behavior_of_the $ 5 $ 104 $ 11 $ " KUMORI/TERM ( cloudiness ) " is a/DEF natural phenomenon which can be pointed to concretely ./O
Abstract $ 0 $ 2 $ 1 $ The system consists of four components , a concept network , a query reformulation model , a standard search engine , and an automatic summarizer.
Abstract $ 0 $ 79 $ 48 $ From the perspective of discourse analysis , the study of discourse markers basically involves four distinct but fundamental issues : 1 ) the occurrence and the frequency of occurrence of discourse markers ( Moser and Moore 1995 ) , 2 ) determining whether a candidate linguistic item is a discourse marker ( identification / disambiguation ) ( Hirschberg and Litman 1993 ; Siegel and McKeown 1994 ) , 3 ) determination or selection of the discourse function of an identified discourse marker ( Moser and Moore 1995 ) , and 4 ) the coverage capabilities ( in terms of levels of embedding ) among rhetorical relations , as well as among individual discourse markers .
_Sense_Co/Occurrence_Frequency $ 3 $ 65 $ 38 $ score ( S , C ) ( 1 ) = score ( SS , C ' ) score ( SS , GlobalSS ) Where S/TERM is a/DEF sense item of polysemouse word W ,/O C/TERM is the/DEF context containing W , SS/TERM is the corresponding sememe set of S ,/O C/TERM '/TERM is the/DEF set of sememe expansion of words in C and/O GlobalSS/TERM is the/DEF sememe set that containing all of the sememe defined in Hownet ./O
Introduction $ 1 $ 30 $ 24 $ The bedroom window was broken and broken glass was found inside the window .
Tagging_Verb_Groups $ 6 $ 155 $ 8 $ The value of attribute tag is computed automatically from the verb rule that describes the compound verb group.
OUT $ -1 $ 125 $ 54 $ The constructions of coordination and apposition are represented by a special/TERM node/TERM ( usually the node of the coordinating conjunction or other expression ) that is the/DEF governor of the coordinated subtrees and their common complementation in the ATS ./O
Family_weights $ 2 $ 24 $ 3 $ Examples of such rudimentary weighting schemes are the use of a weight of k!
The_semantic_behavior_of_the $ 5 $ 94 $ 1 $ For example , KIKEN_NA JOUTAI ( dangerous ) ( situation ) dangerous situation In this case "dangerous" represents the state concretely .
Construction_of_Features $ 3 $ 91 $ 23 $ An example of a training data and a resulting lexical context is shown in Figure 3 .
OUT $ -1 $ 58 $ 53 $ Also , an answer/TERM as defined in the TREC-8 QA task is a/DEF 50-byte or 250byte answer string ,/O whereas an answer/TERM is a/DEF complete sentence in the reading comprehension task ./O
Generating_from_Bare_Data $ 2 $ 37 $ 25 $ * Fact Annotations : ILEX was designed to work with various extra information known about facts , such as the assumed level of interest to the current reader model , the importance of the fact to the system 's educational agenda , and the assumed assimilation of the information ( how well does the system believe the reader to already understand it ) .
Generation_of_Multiple_Quantifiers $ 5 $ 208 $ 23 $ But in " Every patient with a balloon pump had hypertension " , the existentially quantified expression " with a balloon pump " is a restrictive modifier of its head .
Statistical_Semantic_Parsing $ 4 $ 142 $ 43 $ P(NEG)/TERM is the/DEF probability that a negative example is mislabelled and its value can be estimated given # ( in equation ( 6 ) ) and the total nnrnber of positive and negative examples ./O
Introduction $ 1 $ 19 $ 9 $ Semantically annotated documents are accessed using the vocabulary provided by a domain-specific ontology .
Abstract $ 0 $ 64 $ 8 $ Many NLG modules have to be sensitive to a number of levels at once ( consider , for . . . . . . . . . . instance , -aggregatiomxeferring ,expmssion . ,generation and lexicalisation , all of which need to take into account rhetorical , semantic and syntactic constraints ) .
The_results_reported_in_this_paper_are_part_of_a_more $ 1 $ 42 $ 31 $ Both of them try to expand a " basic-keyword/TERM " , that is a/DEF keyword direcdy derived from a natural language question ./O
The_Verbmobil_treebanks $ 2 $ 56 $ 22 $ ' This example illustrates the usefulness of syntactic annotations for linguistic research • and it shows the need of query languages and query tools that allow access to these annotations .
Issues_and_proposals $ 3 $ 122 $ 79 $ RST in its original formulation does not cover enveloping or parallel structures or conventional forms .
Bridging_Natural_Language_and $ 4 $ 81 $ 2 $ Despite its limitations , a finite-state grammar seems to provide the best natural language model for information retrieval purposes .
Setting $ 3 $ 80 $ 1 $ This corpus was collected by Ng and colleagues ( Ng and Lee , 1996 ) and it is available from the Linguistic Data Consortium ( LDC ) 5 .
Experimental_Results $ 7 $ 171 $ 4 $ split of the data 'Apte/TERM split/TERM , ' which consists/DEF of 9603 texts for training and 3299 texts for test ./O
Introduction $ 1 $ 16 $ 14 $ From the requester's point of view , it results in 84 the production of complex linguistic forms aimed at reducing the potential offence intrinsic to a demand to act ( conversationally or behaviorally ) ; from the requestee's point of view , while acceptance normally addresses the requester's potential offence by a displaying of good-tempered feelings , any refusal at the conversational or behavioral level constitutes in turn a potential offence to the requestee's face , and sets up the social need for the refusing agent to act in order to nullify this potential offence ( Goffman , 1981 ) .
_Annotation_Guidelines_I : $ 3 $ 59 $ 13 $ S/TERM is the/DEF start symbol )/O .
Abstract $ 0 $ 129 $ 98 $ We denote this as a binary relation Causality ( FrontClause ( 2 ) , BaekClause ( 2 ) ) where FrontClause/TERM ( n ) denotes the/DEF discourse segment that is encapsulated by the Front discourse marker of the corresponding rhetorical relation whose sequence number is n ./O 15 BackClause ( n ) can be defined similarly.
Learning_Frameworks $ 2 $ 23 $ 3 $ Pr ( s ) = Pr ( wl , W2 , . . . Wn ) ---= H~=lPr ( wilwl , . . . wi-1 ) = H~=lPr ( wilhi ) where hi/TERM is the/DEF relevant history when predicting wi ,/O and s/TERM is any/DEF sequence of tokens , words , part-of-speech ( pos ) tags or other terms ./O
Clustering_into $ 5 $ 91 $ 22 $ Figure 6 depicts a 3D representation of results obtained from profiling VBtags with six other major syntactic categories ; figure 7 shows the main syntactic behavioural features found for the co-occurrence of some of the major syntactic classes ranging over the chosen window of ten words .
Tagged_Text $ 3 $ 120 $ 106 $ If a number is a four-digit number starting with 16 to 19 or is followed by A.D or B.C.
The_interplay_of_focus_and_word $ 2 $ 50 $ 26 $ Extrapositions/TERM are the/DEF linguistic means in German to separate sense units ./O
Unless_necessary ,_we_don ' t_list_relationship_in_the $ 3 $ 83 $ 25 $ Suppose Pi~-Po , Ei/TERM (/TERM ~-Po/TERM )/TERM is the/DEF set of the edges between points in P1 ,/O Ri/TERM (/TERM ~/TERM (/TERM PlX/TERM El/TERM )/TERM )/TERM is the/DEF set of relations between points in PI and edges in Et s ,/O then : s Here , Edges are also points .
Learning_Phrase-based_Variable $ 4 $ 84 $ 15 $ The c/TERM transition/TERM from state 4 to state 6 is a/DEF back/Off transition to a lower order n-gram probability ./O
Conclusions_and_Future_Work $ 5 $ 262 $ 6 $ Moreover , the content-based measures which rely on a ground truth are only slightly more correlated to each other than theyare to the measures which perform summary-document comparisons.
Validating_each_tagger_into_its_respective $ 1 $ 36 $ 12 $ In parallel , we chose three types of medical texts to make up the medical/TERM corpus/TERM : it/DEF represents 16024 tokens , with 3 equal thirds : discharge summaries , surgical reports , and laboratory or test results ( in this case , tables were removed ) ./O
Burstiness $ 3 $ 118 $ 11 $ Kwok ( 1996 ) suggested average/TERM term/TERM frequency/TERM , avtf/DEF = TF ( t ) /df ( t ) ,/O be used as a tie-breaker for cases like this , where TF ( t ) = ~a if ( t , d ) is the standard notion of frequency in the corpus-based NLP .
Word_Clustering $ 3 $ 50 $ 6 $ 36 MDL/ACR ( Minimum/TERM Description/TERM Length/TERM ) principle/TERM is a/DEF model selection criterion which asserts that , for a given data sequence , the lower a model ' s SC value , the greater its likelihood of being a model which would have actually generated the data ./O
Introduction $ 1 $ 6 $ 0 $ Decomposing syntactic analysis into several phases so as to decrease its difficulty is a new stream in NIP research.
Introduction $ 1 $ 8 $ 1 $ There are many definitions for the compound noun which cause ambiguities as to whether a given continuous noun sequence is a compound noun or not .
Abstract $ 0 $ 22 $ 21 $ However , traditional Dependency gammar realizes the dependency relations between any of two specific words , then numerous word based dependency knowledge should be constructed , this is a time-consuming task .
Evaluation $ 3 $ 193 $ 4 $ The recall/TERM is the/DEF number of identified errors over the total number of errors ./O
_Otherwise ,_add_to_the_current_context_new $ 6 $ 95 $ 12 $ Inference to the best explanation The assertion of the goal G supports a proposition Q which is firmly believed ( i.e. , P ( Q ) = High , where Q/TERM is a/DEF premise or inferred from premises )/O , but which would be unexplained ( improbable ) without supposing the truth of the goal .
Conclusion $ 6 $ 134 $ 9 $ Extra- position provides a useful and sometimes important means of rearranging complex material in an abstract discourse representation in order to satisfy the constraints posed by linearisation into text .
_General_Outline_of_the_Method $ 2 $ 67 $ 51 $ For example , for the feature lthNext-Word ( the first word after the word sequence ) , its value is the top 500 words ( ordered by frequency ) that can appear after a person name.
Discussion $ 6 $ 268 $ 90 $ However , many prior techniques/DEF used in natural language information retrieval (/O e . g . , head/modifier/TERM pairs/TERM ) can be expressed within the ItEXTOR framework , and furthermore the system provides a playground for experimenting with new techniques .
Conclusion $ 4 $ 116 $ 4 $ The last step is to recover the implicit information structures from the surface information structures based on two additional knowledge sources : ( i ) relations between the event types as defined in HowNet ; and ( ii ) rules governing the interplay of dynamic roles between event types .
Introduction $ 1 $ 43 $ 38 $ Most people don't know he is a real person who is grown now .
_KNOWLEDGE_EXTRACTION $ 3 $ 171 $ 113 $ When the proper name X is a new word , it will be segmented into shorter segments ( xl+x2+ . . . +xn ) .
Dependency_Analysis_using $ 3 $ 68 $ 5 $ Dbest = argmax P ( D[ B ) D If we assume that the dependency probabilities are mutually independent , P ( DIB ) could be rewritten as : rn-1 P ( DIB ) = ~I P ( Dep ( i ) =j Ifit ) i=1 fit = { fl , . . . , fn } e R n . P/TERM (/TERM Dep/TERM (/TERM i/TERM )/TERM =/TERM J/TERM If0/TERM )/TERM represents the/DEF probability that bi depends on ( modifies ) b t ./O fit/TERM is an/DEF n dimensional feature vector that represents various kinds of linguistic features related with the chunks bi and b t ./O We obtain Dbest taking into all the combination of these probabilities .
_Features $ 1 $ 73 $ 2 $ We use the recall and precision measurements for evaluation .
_The_co-reference_problem_in_summarization $ 4 $ 81 $ 57 $ Following is a list of various methods of creating multi-document summaries by extraction : Find the important relevant parts that the cluster of documents have in common ( their intersection ) and use that as a summary .
Abstract $ 0 $ 5 $ 2 $ The RIPPER rule induction algorithm is adopted for the selection of the underlying rules.
Introduction $ 1 $ 27 $ 21 $ 'instance ~-to a set of alternatives C : the adverb describes a property that makes ~ unique in C . Thus in ( 3a ) Mary is unique among some set C of individuals in passing .
Learning_Algorithms_Tested $ 2 $ 46 $ 26 $ When testing , the decision list is checked in order and the feature with the highest weight that matches the test example is used to select the winning word sense .
Current_approach $ 3 $ 103 $ 64 $ " Yet , it is true that " -es " is a valid suffix for the words " flashes , " " catches , " " kisses , " and many other words where the " -es " is preceded by a voiceless sibilant .
OUT $ -1 $ 230 $ 40 $ 25 The attribute vtype is mandatory , vtype/TERM is a/DEF reference to a description of a guideline violation in a file which contains the different kinds of violations of the individual guidelines ./O
OUT $ -1 $ 76 $ 75 $ If the subject is a single entity e , the value of the subject for an objective o is vo ( e ) , and it is positive when it is greater than 0 . 5 , the midpoint of [0 ,1] ( negative otherwise ) .
Our_approach_to_Multilingual $ 2 $ 41 $ 14 $ In this formaldirectly expressing the choices which uniquely charism , the carrier of meaning is a choice tree ( called aeterize a given document in an homoge~cous class `` abstract tree ' ' in GF ) , a strongly typed object in of documents belonging to the same domain .
Abstract $ 0 $ 70 $ 39 $ Moreover , primary discourse markers can also be classified as simple adverbials , as is the case in English : ( I ) Even though a child , John is so tall that he has problem getting half-fare .
Examples $ 5 $ 144 $ 2 $ Text preceded by USER represents spoken utterances from the user .
The_semantic_behavior_of_the $ 5 $ 100 $ 7 $ That is , " MIKETTEI NO ( of indecision ) , " represents the situation of a problem .
OUT $ -1 $ 63 $ 37 $ It is based on string edit distance between the output of the generation system and the reference corpus string .
Qualitative_Evaluation_of_the $ 4 $ 136 $ 35 $ \Ve proposed a mmlber/Of'models : in which various conLfrom the two tree models binations of intrinsic metrics were used to predict user judgments of understanding and quality .
Why_Reading_Comprehension $ 2 $ 55 $ 1 $ Each story comes with a set of questions about information that is stated or implied in the text .
EXOT $ 9 $ 108 $ 36 $ SEMCAT/TERM weights/TERM are calculated/DEF based on the following equations ./O
Analysis_module $ 2 $ 46 $ 6 $ A grammar defined by means of the grammatical formalism SUG/TERM ( Slot/DEF Unification Grammar )/O is used as input of SUPAR.
Brief_System_Description $ 2 $ 13 $ 4 $ The Ontology/TERM is a/DEF directed acyelic graph automatically derived from the Grammar in which the nodes correspond to grammar nonterminals ( NTs ) and the arcs record immediate dominance relation ,/O i.e. , the presence of , say , NTi in a right-hand/TERM side/TERM ( RHS/ACR ) alternative of NTj will result in an arc from NTi to NTj .
Discussion $ 6 $ 114 $ 0 $ A knowledge-based/TERM machine/TERM translation/TERM can be viewed as extracting/DEF and representing the meaning of a text and generating a text in target language based on the meaning presented ./O
Abstract $ 0 $ 61 $ 5 $ For instance , many systems have a referring expression generation module whose task is to complete a semantic representation which lacks those structures which will be realised as NPs .
OUT $ -1 $ 111 $ 106 $ Intuitively , a feature value of 0 is the best , indicating that for that question-sentence pair q and s , they have the most number of matching words in the story , when comparing q with all sentences sz in the same story .
ADAM : _Architectural_Principles $ 3 $ 71 $ 48 $ Second , because of its platform-independence it enhances the potential for wide circulation of the annotated material , together with a considerable flexibility of use .
Abstract $ 0 $ 6 $ 1 $ We present such a component : a fast and robust morphological generator for English based on finite-state techniques that generates a word form given a specification of the lemma , part-of-speech , and the type of inflection required .
ALLiS $ 2 $ 22 $ 17 $ LT TTT : the last tool tried , LT/TERM TTT/TERM ( Grover et al. , 1999 ) , is a/DEF text tokenisation system and toolset ./O
HMM_for_Cross-lingual_IR $ 3 $ 29 $ 0 $ For CLIR we extend the query generation process so that a document Dy written in language y can generate a query Qx in language x .
Types_of_requests_and $ 2 $ 17 $ 1 $ Bunt ( 1989 ) makes a distinction between factual information acts and dialogue control acts.
Implementing_Embedded_MT $ 2 $ 112 $ 55 $ Another part is the analysis of not-translated words .
_Conclusion $ 4 $ 112 $ 3 $ It does not assume that the user receives any training in search and retrieval or has any prior experience in using Internet .
OUT $ -1 $ 136 $ 94 $ The second experiment compares the performances achieved by TBLDT and C4.5 training on samples selected by active learning .
The_MATE_Approach $ 2 $ 52 $ 16 $ This list of information items which we call a coding module , is the core concept of the MATE markup framework and extends and formalises the concept of a coding scheme .
Human_Annotation_of $ 2 $ 65 $ 0 $ Argumentative Zones We have previously evaluated the scheme empirically by extensive experiments with three subjects , over a range of 48 articles ( Teufel et al . , 1999 ) .
Introduction $ 1 $ 62 $ 53 $ Finally , VALDIA is described in more detail and then the paper is closed by a discussion of relevant results and papers .
Abstract $ 0 $ 37 $ 35 $ Fullfledged QA on the basis of natural language texts is far beyond the present state of the art .
Introduction $ 1 $ 21 $ 14 $ " Section 2 discusses the properties of numerical evaluation measures , points out several drawbacks associated with intrinsic measures and introduces new measures developed by the authors .
Social_Goals_and_Conversational $ 3 $ 86 $ 42 $ In this case , two plans can apply : the simple plan for refusing , or the elaborated plan for refusing which includes a justification for the refusal .
_Rare_w~_contains_a_hyphen $ 9 $ 147 $ 92 $ Below is the performance on the test set of the resulting model when features for disambiguating verb forms are added to the model of Section 2.
_INTRODUCTION $ 1 $ 39 $ 33 $ Unfortunately there is no wellprepared knowledge sources containing the above information .
Portable_Information_Extraction $ 4 $ 77 $ 0 $ For our Phase I feasibility demonstration , we chose a minimal scenario template for meeting and negotiation events consisting of one or more participant slots plus optional date and location slots .
Abstract $ 0 $ 27 $ 25 $ As in Paice ( 1990 ) , summarization techniques in text analysis are severely impaired by the absence of a generally accepted discourse 11 model and the use of superstructural schemes is promising for abstracting text .
OUT $ -1 $ 160 $ 58 $ We can conclude that the filtering method H is the best , considering the effectiveness and the efficiency at the same time .
Topic_Analysis $ 4 $ 132 $ 65 $ In the text in Figure 3 , the topic represented by seed-words ' trade-export-tariff-import ' is the main topic , and ' Japan-Japanese , ' ' Hong Kong , ' etc . , are subtopics .
OUT $ -1 $ 123 $ 118 $ To compute these feature values for a sentence , we used the Remedia/TERM corpus/TERM provided by MITRE which has been hand-tagged/DEF with named entities ./O
Tagged_Text $ 3 $ 237 $ 223 $ The next most dominant feature is the SemType value object , which appears in the NP for 29.41% of the answer sentences and PP NeedSemType for 15.68% of the answer sentences .
Conclusions $ 4 $ 106 $ 2 $ The tools suggest a group of key items by decreasing order of significance which distinguish one corpus from another .
The_following_formulas_summarize_the_relations $ 9 $ 171 $ 90 $ The cross-system comparisons using the measures presented , with one exception , yielded the following expected rankings : ( i ) the GT-MT pass exhibits better performance than the ScanOCR-MT pass and ( ii ) the Haitian Creole system is at the low end , Arabic is in the middle , and Spanish is at the high end .
Setting $ 3 $ 111 $ 32 $ The symbol " +/TERM " stands for set/DEF union ,/O therefore A+B-B means that the training set is A union B and the test set is B .
Abstract $ 0 $ 3 $ 0 $ We present two measures for comparing corpora based on information theory statistics such as gain ratio as well as simple term-class ~equency counts .
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 254 $ 219 $ ttpj = ttpn { / ( t , pj ) ift , pj is atoplc of Stp 0 otherwise f ( w ) denotes term frequency of word w. term vectors 35 Let $1 , - , S , , , be all the other training documents ( where m/TERM is the/DEF number of training documents which does not belong to the target event )/O and Sx/TERM be a/DEF test document which should be classified as to whether or not it discusses the target event ./O
Complexity $ 3 $ 110 $ 27 $ Figure 3 shows how different parameter setting affects the cardinality of utterances for different values of M. The ( logarithmic/TERM ) yaxis/TERM represents the/DEF cardinality of utterances ,/O and the ( linear/TERM ) x-axis/TERM the/DEF maximal number of semantic items in one utterance ./O
Abstract $ 0 $ 100 $ 39 $ As a matter of fact , the three never wrote a musical individually or as a single ties whose union or sum is the overall plural argument .
Informational_content_of_sentences $ 2 $ 24 $ 0 $ Cluster-based/TERM sentence/TERM utility/TERM ( CBSU/TERM , or utility ) refers to the/DEF degree of relevance ( from 0 to 10 ) of a " particular sentence to the general topic of the entire cluster (/O for a dis cussion of what is a topic , see [Allan et al.
Architecture_of_WIT-Based_Spoken $ 3 $ 82 $ 48 $ In the phases in which the system has the initiative , only the initial function and the language model are assigned .
LTAGs_and_Extraction $ 3 $ 52 $ 25 $ X m is further expanded into a spine-etree whose head X ° is the anchor of the whole mod-etree .
Learning_Algorithms_Tested $ 2 $ 36 $ 16 $ In the SNo W architecture there is a winnow/TERM node/TERM for each class , which learns/DEF to separate that class from all the rest ./O
Abstract $ 0 $ 41 $ 37 $ For example , the bigram probabilities of the language model may be estimated and updated with the corrected data .
The_MATE_Markup_Framework $ 3 $ 89 $ 1 $ When a coding module has been applied to a corpus , the result is a coding file .
The_MATE_Markup_Framework $ 3 $ 167 $ 79 $ An example is the following of coding elements which may refer to utterances in a transcription but which depend on the technical programming choice of the underlying , non-user related representation : ModuleRefs CDATA 'href : transcription # u ' See Figure 2 for a concrete example from the MATE Workbench .
Experiment $ 3 $ 104 $ 22 $ In order to provide a better estimate of how close two discourse trees were , we computed PositionDependent and -Independent recall and precision figures for the sentential/TERM level/TERM ( where/DEF units are given by edus and spans are given by sets of edus or single sentences )/O ; paragraph/TERM level/TERM ( where/DEF units are given by sentences and spans are given by sets of sentences or single paragraphs )/O : and text/TERM level/TERM ( where/DEF units are given by paragraphs and spans are given by sets of paragraphs )/O .
Conclusions_and_Future_Work $ 5 $ 261 $ 5 $ In addition , these measures provide a finer grained score with which to compare summaries.
_Rare_w~_contains_a_hyphen $ 9 $ 160 $ 105 $ The second feature template has the form : The last verb is v and the current word is w and w has been tagged as a particle and the current tag is t . The last verb is the pseudo-symbol NA if there is no verb in the previous three positions .
Tree_Structures $ 2 $ 26 $ 10 $ 87 S Hennessy [NNP] a [D~jj] act ~ I vP to [TO~ow [VB] Figure 1 : Syntactic structure for the sentence "Hennessy will be a hard act to follow".
Clustering_Systematic_Polysemy $ 3 $ 89 $ 9 $ In the calculation of the data description length in equation ( 6 ) , each word in a cluster , observed or unobserved , is assigned an estimated/TERM probability/TERM , which is a/DEF uniform fraction of the probability of the cluster ./O
Evaluation_Measures $ 2 $ 71 $ 46 $ Perhaps the postponement is a sign that the studio is looking askance at this expensive product directed and co-produced ".by its female lead.
Given_an_input_space_X~*_of $ 3 $ 78 $ 47 $ For example , the Mutual/TERM Information/TERM measures/DEF the strength of a correlation between co-occurring arguments ,/O and the Plausibility/TERM ( Cucchiarelli , Luzi and Velardi ( 1998 ) ) assigns/DEF a weight to a feature vector , depending upon the degree of ambiguity of its arguments and the frequency of its observations in a corpus ./O
MIMIC's_Concept-to-Speech $ 4 $ 82 $ 45 $ " denotes a downstepped accent ( see ( Pierrehumbert , 1980 ) ) .
Results $ 3 $ 81 $ 29 $ ' */TERM ' denotes significantly/DEF better accuracy of RBM or RIPPER over IBi-IG with p 0.05 ./O
Abstract $ 0 $ 49 $ 22 $ In this paper we have presented a more efficient distributed algorithm which construct a breadth-first search tree in an asynchronous communication network P/T -/Title Lst/Presents a model and gives an First we present a model and give overview of lst/Overview of related research , related research .
Selection_of_candidate_strings $ 2 $ 35 $ 11 $ Here is an example : After dictionary look up , we get which is a sequence of 10 single characters.
Conceptualizing_Events $ 2 $ 108 $ 78 $ This could be done by a proposition like turn ( ok314 , goal ( tw-echo ) ) , which is a potential increment for a preverbal message .
Results $ 3 $ 63 $ 11 $ The data/TERM consists/DEF of fourtuples of words , extracted from the Wall Street Journal Treebank ./O
The_TransType_model $ 2 $ 33 $ 18 $ The core/TERM of/TERM TRANSTYPE/TERM is a/DEF completion engine which comprises two main parts : an evaluator which assigns probabilistic scores to completion ./O
Abstract $ 0 $ 3 $ 2 $ Our method uses a modification of a tree generalization technique used in ( Li and Abe , 1998 ) , and generates a tree-cut , which is a list of clusters that partition a tree.
_The_session_was_also_attended_by_observers_from_the_following_international_organizations : $ 7 $ 215 $ 164 $ This study is another application that demonstrates the usability of the WWW as a resource for NLP ( see , for instance , ( Grefenstette , 1999 ) for an application of using WWW frequencies in selecting translations ) .
Introduction $ 1 $ 20 $ 15 $ Each story has an average of 20 sentences , and the question/TERM answering/TERM task/TERM as formulated for a computer program is to/DEF select a sentence in the story that answers to a question ./O
The_Classifiers $ 3 $ 67 $ 0 $ Adaptive/TERM Resonance/TERM Associative/TERM Map/TERM ( ARAM/TERM ) is a/DEF class of predictive serforganizing neural networks that performs incremental supervised learning of recognition categories ( pattern classes ) and multidimensional maps of patterns ./O
Abstract $ 0 $ 4 $ 0 $ We present discourse annotation work aimed at constructing a parallel corpus of Rhetorical Structure trees for a collection of Japanese texts and their corresponding English translations .
Abstract $ 0 $ 3 $ 2 $ We have tested Quarc on reading comprehension tests typically given to children in grades 3-6 .
Conclusion $ 9 $ 204 $ 22 $ Here is the latest document summary .
Results $ 3 $ 56 $ 4 $ Base-NP/TERM chunking/TERM ( NPSM/TERM ) : the/DEF segmentation of sentences into non-recursive NPs ./O
Determination_Schemes_of $ 4 $ 124 $ 9 $ with safe segmentation accuracy = ~ of actually segmented Sent .
Architecture_of_WIT-Based_Spoken $ 3 $ 43 $ 9 $ The language/TERM model/TERM for/TERM speech/TERM recognition/TERM is a/DEF network ( regular ) grammar , and it allows each speech interval to be an arbitrary number of phrases ./O
Dichotomizer_Ensembles $ 2 $ 53 $ 10 $ Assuming a normal distribution of errors and bit values in every 2 bits-block , there is a 25% chance that both bits in a 2-bit block are wrong .
_Limitations $ 5 $ 33 $ 7 $ • Time in the 911 domain there are at least two temporal contexts that can be "used" by the conversants : there is the actual time ( i.e. , when they are talking ) , but there also is the time relative to a point of focus in a plan , or even simply talking about the past or the future.
The_REXTOR_System $ 5 $ 170 $ 48 $ Also , the sequence of adjectives is saved as the 0th bound variable , and the sequence of nouns is saved as the 1st bound variable .
Abstract $ 0 $ 2 $ 1 $ This paper realizes a practical system for Chinese parsing by using a hybrid model of phrase structure partial parsing and dependency parsing .
ADAM : _Architectural_Principles $ 3 $ 80 $ 57 $ We believe that this approach represents a further value of the ADAM Corpus .
Abstract $ 0 $ 187 $ 45 $ Intuitively , it appears that there is a strong link between the pharmaceutical form of a given drug and the way it should be administered : tablets are swallowed , eye drops are put in the eyes , powder is diluted in water etc.
Abstract $ 0 $ 2 $ 1 $ The abstracts are generated by a process of conceptual identification , topic extraction and re-generation .
Abstract $ 0 $ 2 $ 0 $ Morphology/TERM induction/TERM is a/DEF subproblem of important tasks like automatic learning of machine-readable dictionaries and grammar induction ./O
Word_Clustering $ 3 $ 52 $ 8 $ 2 For a fixed seed word s , we take a word w as a frequently co-occurring/TERM word/TERM if/DEF the presence of s is a statistically significant indicator of the presence of w ./O Let a data sequence : ( sl ,wl ) , ( s2 ,w2 ) , . . , ( Sin ,Win ) be given where ( si , wi ) denotes the state of co-occurrence of words s and w in the i-th text in the corpus data .
Principles_and_Parameters $ 1 $ 7 $ 0 $ Chomsky ( 1981 ) ( and elsewhere ) has proposed that all/DEF natural languages share the same innate universal principles (/O Universal/TERM Grammar/TERM -UG/ACR ) and differ only with respect to the settings of a finite number of parameters .
Summary_and_Future_Work $ 4 $ 213 $ 9 $ Although the precision ( so far ) is not high ( 60% 80% ) , it is not the most important result because ( 1 ) this only represents a minor waste of checking effort , compared with scanning the entire text , and ( 2 ) the identified errors will be checked further or corrected either manually or automatically.
Previous_work $ 2 $ 38 $ 14 $ He also builds a probabilistic model which indicates that the probability of two words being morphological variants is based upon the probability of their respective changes in orthography and morphosyntactics .
Introduction $ 1 $ 15 $ 4 $ A textual/TERM IR/TERM system/TERM stores/DEF a collection of documents and special data structures for effective searching ./O
Generation_of_Multiple_Quantifiers $ 5 $ 224 $ 39 $ If the name of the surgeon is not available but the identifiers for the surgeon entities across the propositions are the same , the system will generate " The same surgeon operated on each patient .
Complexity_Formulas $ 4 $ 73 $ 17 $ The most that can be is the maximum object complexity in the class.
Experimental_work $ 4 $ 71 $ 15 $ The base-line model is a trigram model ( Trigram ) PS algorithm and features of n-grams and distance 2 n-grams.
Empirical_Evaluation $ 4 $ 124 $ 13 $ A set of domain knowledge consists of 56 rules with about one to 10 rules for each category was generated .
Results $ 6 $ 190 $ 12 $ Since referent S was identifiable by /i definite description for the listener and is the topic , it remains identifiable by definite means , resulting in a definite NP.
MALIN $ 4 $ 106 $ 0 $ In what follows we describe and exemplify a dialogue system with separate modules for dialogue management and domain knowledge management.
Abstract $ 0 $ 113 $ 18 $ In the Wall Street Journal texts , for example , the top two grammatical relations are Or ( object of transitive verb ) and PPN ( prepositional phrase complement of a NP ) .
Prosody_Prediction $ 4 $ 70 $ 8 $ The previous mapping between the two structures defines a tree transformation.
Introduction $ 1 $ 18 $ 11 $ The ILEX project was supported by EPSRC grant GR/K53321.
_Pronominalise_the_Cb_only_after_a_Continue $ 4 $ 182 $ 143 $ The implementation of Centering/TERM reported here is a/DEF special case of text planning by constraint satisfaction , where the user has control over the different constraints ,/O and this approach means that different strategies for e . g .
_The_+END+_ ( null ) _postmodifier_con $ 8 $ 156 $ 103 $ For example , in the sentence in Figure 2 , the subject Jane is predicted conditioning on the head of the VP , which is the modal wdl , as opposed to the more semanticallycontent-rich kill.
ALLiS $ 3 $ 20 $ 0 $ ALLiS/TERM (/TERM Architecture/TERM for/TERM Learning/TERM Linguistic/TERM Structures/TERM )/TERM ( D~jean , 2000a ) is a/DEF symbolic machine learning system which generates categorisation rules from a tagged and bracketed corpus ./O
_IBM_expected_[SBAa_each_employee_to $ 5 $ 43 $ 31 $ Most recently , in ( Stetina et al . , 1998 ) , the authors made use of head-driven bilexical dependencies with syntactic relations to attack the problem of generalized word-sense disambiguation , precisely one of the two problems we are dealing with here .
Overview $ 1 $ 13 $ 9 $ • The types of NEs collected here are much more accurate than the four basic types defined in MUC .
Background $ 2 $ 71 $ 51 $ inform/TERM (/TERM aTask=n/TERM )/TERM : system/DEF presents the n'th answer to the query t ./O
Applications $ 3 $ 62 $ 2 $ This is a subcorpus of circa/TERM 4.5/TERM million/TERM words/TERM , in/DEF which speakers and respondents are identified by such factors as gender , age , social group and geographical region ./O
Reference_Resolution $ 4 $ 99 $ 0 $ What follows here is a discussion of what is needed to resolve the 273 references made in the example Spanish text .
Validating_each_tagger_into_its_respective $ 1 $ 25 $ 1 $ The first system is specialised for tagging medical texts ( Ruch and al . , 2000 ) , while the second is a general parser ( based on FIPS , cf .
Significance_of_word_contexts $ 2 $ 66 $ 18 $ The contextual/TERM representation/TERM of/TERM a/TERM word/TERM has been defined as a/DEF characterisation of the linguistic context in which a word appears ./O
Implementation $ 3 $ 155 $ 37 $ The nodes in the output vector represents different syntactic categories , so we also get a surface syntactic structure directly output from the net , which could be used for stress information etc .
OUT $ -1 $ 176 $ 50 $ As the first baseline , we use a standard text categorization method for classification ( where each sentence is considered as a document* ) Baseline 1 has an accuracy of 69 % , which is low considering that the most frequent category ( OWN ) also coyerrs 69 % of all sentences .
Maximum_Entropy_Modeling $ 2 $ 54 $ 12 $ Given a training sample of size N , ( Xl ,Yl ) , . . , ( XN ,YN ) , an empirical/TERM probability/TERM distribution/TERM can be defined as y/DEF ) = y ) N where # ( x , y ) is the number of occurrences of ( x , y ) ./O
Related_Research_formalism_ ( BNs_were_chosen_because_of_their_abil $ 2 $ 42 $ 1 $ An argument is represented as an cluding a discussion of counterfactual reasoning and Argument/TERM Graph/TERM , which is a/DEF network of nodes that modality ,/O may be found in ( Rescher , 1964 ) .
Introduction $ 1 $ 9 $ 0 $ Spam/TERM , or more properly Unsolicited/TERM Commercial/TERM E/TERM-mail ( UCE/ACR ) , is an/DEF increasing threat to the viability of Internet E-mail and a danger to Internet commerce ./O
Modeling_various_degrees_of $ 5 $ 71 $ 8 $ is a list of dialogue actions that the agent wishes to carry out .
Introduction $ 1 $ 11 $ 7 $ A baseline algorithm for Word Domain Disambignation is presented and then compared with a mutual/TERM help/TERM disambignation/TERM strategy/TERM , which makes/DEF use of the shared senses of parallel bilingual texts ./O
Robust_Name_Finding $ 3 $ 96 $ 37 $ A closely related statistical approach to named entity tagging specifically targeted at speech data was developed at Sheffield by Gotoh and Renals ( 2000 ) .
Definitions $ 2 $ 33 $ 8 $ We will notate the jth String of a corpus C as C[j].
Abstract $ 0 $ 154 $ 59 $ The addition of the heterogeneous Wall Street Journal articles , however , dilutes the focus of the model constructed for Encarta .
Models_and_Modifications $ 2 $ 56 $ 43 $ Since multiple modifier trees can adjoin at the same location , Psa ( 7 ) is also conditioned on a flag f which indicates whether ' 7 is the first modifier tree ( i.e. , the one closest to the head ) to adjoin at that location .
Abstract $ 0 $ 17 $ 10 $ who developed topic?
OUT $ -1 $ 0 $ 0 $ A trainable method for extracting Chinese entity names and their relations Yimin Zhang & Zhou Joe F Intel China Research Center Kerry Center 6F1 No .
Theoretical_Ideas , $ 2 $ 95 $ 70 $ For instance , if the command is " go to the crew hatch and open it " and the crew hatch is already open , the interface has the option of informing the user that there is a problem without-first carrying out the " go to " action .
OUT $ -1 $ 181 $ 176 $ For each question type , we uniformly use the same , identical set of features .
Learning_validation_and_results $ 4 $ 147 $ 9 $ We have first defined a measure of the theoretical generality of the clauses 16 .
Tree_Generalization_using_Tree-cut $ 2 $ 77 $ 31 $ Then , the best model is the one which requires the minimum total description length.
Introduction $ 1 $ 8 $ 2 $ Machine translation between any two languages often requires the generation of information that is implicit in the source language .
MALIN $ 4 $ 130 $ 24 $ For complex requests the Dialogue Manager needs an information structure that holds the parameters needed before successful access of the background system can be performed .
Interaction_Grammars $ 3 $ 122 $ 69 $ grammar ( program ) , but which introduce terminals specific , to ~the_ language -at .
Abstract $ 0 $ 162 $ 6 $ We believe that this is a good example of a case where developing a system with the RAGS data structures in mind simplifies the task .
Approach_for_Chunk_Identification $ 3 $ 27 $ 0 $ The chunks in the CoNLL-2000 shared task are represented with IOB based model , in which every word is to be tagged with a chunk label extended with I/TERM ( inside/DEF a chunk )/O , O/TERM ( outside/DEF a chunk )/O and B/TERM ( inside/DEF a chunk , but the preceding word is in another chunk )/O .
_Introduction $ 1 $ 21 $ 14 $ Hownet/TERM is a/DEF knowledge base which was released recently on Intemet ./O
The_lexicon_size_of_a_typical_large-vocabulary $ 9 $ 109 $ 30 $ One way of dealing with multiple translations is to weight the alternative translations using either a statistical translation model trained on parallel or comparable corpora to estimate translation probability conditioned on the source language term .
Abstract $ 0 $ 3 $ 2 $ We compare performance using different measures of association , and find that Yule's coefficient of colligation Y gives somewhat better results over other measures.
_Introduction $ 1 $ 14 $ 8 $ First , it is one of the first structurally annotated corpora in Mandarin Chinese .
_Proposed_method $ 2 $ 47 $ 17 $ Accordingly , with an appropriate optimization function over the distance measures between all the senses of the two words , sense #2 for bank and sense # 1 for shore are assigned as the correct tags for the words , respectively .
_The_identity_of_the_speaker ,_denoted_as_the $ 6 $ 161 $ 51 $ Another possible extension involves the inclusion of the speech topic .
Experiments $ 5 $ 99 $ 5 $ `` F-score/TERM '' is a/DEF measurement combining `` Recall '' and `` Predsion '' and/O defined in Equation 3 .
Abstract $ 0 $ 8 $ 4 $ The Inductive/TERM Logic/TERM Programming/TERM learning/TERM method/TERM that we have developed enables us to automatically/DEF extract from a corpus N-V pairs whose elements axe linked by one of the semantic relations defined in the qualia structure in GL , and to distinguish them , in terms of surrounding categorial context from N-V pairs also present in sentences of the corpus but not relevant ./O
_Preliminary_Evaluation $ 3 $ 106 $ 51 $ The algorithm disambiguate_class/TERM , which is implemented by Resnik and described in detail in [ Resnik , 1999] , calculates/DEF the similarity between all the words' senses of words in a set ./O
_The_session_was_also_attended_by_observers_from_the_following_international_organizations : $ 7 $ 192 $ 141 $ This example is the most explicit form an initializer can take as it contains a lexical element which corresponds to each of the four functions outlined above .
Introduction $ 1 $ 2 $ 0 $ In the shared task for CoNLL-2000 , words and tags form the basic multi-valued features for predicting a rich phrase segmentation code .
Given_a_context-based_representadon $ 2 $ 30 $ 1 $ words or word tags ) ,
The_Learning_System $ 2 $ 58 $ 32 $ As a result , intransitive/DEF verbs are/O defined as S\NP/TERM , figure 1 , for the grammar to account for these sentences .
Introduction $ 1 $ 24 $ 15 $ The output of the discourse component [Seneff ( 1996 ) ] is the framein-context , which is transformed into a flattened Eform/TERM ( electronic/DEF form )/O by the generation server .
Introduction $ 1 $ 16 $ 12 $ The main contributions of this paper are : the development of a centroid-based multi-document summarizer , the use of cluster-based/TERM sentence/TERM utility/TERM ( CBSU/ACR ) and cross-sentence/TERM informational/TERM subsumption/TERM ( CSIS/ACR ) for evaluation of single and multi-document summaries , two user studies that support our findings , and an evaluation of MEAD .
Only_the_relevant_attributes_of_each_semantic_role $ 5 $ 208 $ 117 $ The EnglishGerman translation , like English-Spanish , supposes a translation from a language with neutral gender into a language that assigns gender grammatically .
Setting $ 3 $ 93 $ 14 $ The topical context is formed by Cl/TERM ,/TERM .../TERM ,/TERM Cm/TERM , which stand/DEF for the unordered set of open class words appearing in the sentence 7/O .
OUT $ -1 $ 87 $ 69 $ A NAME/TERM is defined as a/DEF PROPER_NOUN that contains at least one HUMAN word ./O
Implementation $ 4 $ 260 $ 89 $ The briefing can then be presented by the author if desired , or else directly by the computer ( particularly useful if the briefing is being sent to someone else ) .
Abstract $ 0 $ 5 $ 3 $ A preliminary evaluation of the proposed method yielded results of up to 79 % accuracy rate for the English data on 81 . 8 % of the SemCor manually tagged data .
Conclusion $ 5 $ 248 $ 0 $ We have presented a system for grammar extraction that produces an LTAG from a Treebank .
CoreLex-II $ 3 $ 41 $ 5 $ In response to this , a new approach was formulated and implemented that addresses both these points .
OUT $ -1 $ 75 $ 29 $ The basic count measures A. through M. are preliminary and will require refinement as more data sets are tested.
The_MATE_Approach $ 2 $ 43 $ 7 $ The resulting report ( Klein et al. , 1998 ) describes more than 60 coding schemes , giving details per scheme on its coding book , the number of annotators who have worked with it , the number of annotated dialogues/segments/ utterances , evaluation results , the underlying task , a list of annotated phenomena , and the markup language used.
OUT $ -1 $ 122 $ 71 $ ILEX/TERM is an/DEF adaptive hypertext generation system , providing natural language descriptions for museum objects ./O
Problem_Space_Modeling $ 2 $ 47 $ 20 $ One good indication of such importance , for example , in the Acyclic Problem Graph , is the branch factor of each node .
Models $ 2 $ 30 $ 10 $ To create a MEMD/TERM analog/TERM to/TERM IBM/TERM model/TERM 1/TERM (/O MEMD1/TERM ) , I used boolean features corresponding to bilingual word pairs : 1 , sEsandt----w fst ( W ,S ) = 0 , else where ( s , t ) is a ( source ,target ) word pair.
Background $ 2 $ 31 $ 11 $ RECprimitives are translated into ( recognition ) contexts and grammars for speech recognition and they may activate sub-components of a synsem grammar.
The_remaining_pronouns_are_not_in_third_person_or $ 9 $ 284 $ 11 $ These problems are the generation of intersentential anaphora , the detection of coreference chains and the generation of Spanish zero-pronouns into English .
Levels_of_Annotation $ 4 $ 135 $ 41 $ Informally speaking , a dialogue/TERM act/TERM tag/TERM is a/DEF label belonging to a tag set which refers to a given iUocutionary dimension that may be performed by uttering a sentence ./O
Prosody_Prediction $ 4 $ 63 $ 0 $ The simple method that we have firstly used is the nearest/TERM neighbour/TERM algorithm/TERM : given/DEF a new sentence , the closest match among the corpus of sentences of known prosody is retrieved and used to infer the prosody of the new sentence ./O
The_MATE_Approach $ 2 $ 54 $ 18 $ Roughly 20 speaking , a coding module includes or describes everything that is needed in order to perform a certain kind of markup of spoken language corpora .
_Instrumentalists_not_including_string_players $ 8 $ 114 $ 102 $ A group of musicians playing popular music for dancing 14 ( b ) ( c ) ( d ) Sense 1 is a specialization of Sense 2 , and this pattern is repeated in French and German .
Introduction $ 1 $ 68 $ 0 $ 2.4 Runtime Analysis In this analysis , we will not consider the computational complexity of part of speech tagging , as that is not the focus of this research .
The_Feasibility_of_the_STL $ 3 $ 110 $ 68 $ For example , if Di imposes a uniform distribution , then DI ( x ) = 1/emax where every sentence expresses at least 1 parameter and emax/TERM is the/DEF maximum number of parameters expressed by any sentence ./O
Related_Work $ 6 $ 178 $ 10 $ 53 Cue-phrases are not necessarily alone responsible for the discourse structure of texts.
_Conclusion $ 4 $ 128 $ 5 $ Although the five categories are defined by us , they can describe basic situations of Chinese.
Abstract $ 0 $ 3 $ 1 $ We argue that text and sentence planning need to be driven in part by the goal of maintaining referential continuity and thereby facilitating pronoun resolution : obtaining a favourable ordering of clauses , and of arguments within clauses , is likely to increase opportunities for non-ambiguous pronoun use.
OUT $ -1 $ 154 $ 103 $ Given two sequences , crossover inserts a random segment from one sequence in a random position in the other to produce two new sequences.
Statistical_Semantic_Parsing $ 4 $ 113 $ 14 $ S+/TERM and S/TERM are the/DEF sets of good and bad states respectively ./O
Conclusions $ 8 $ 253 $ 4 $ It has the following novel features : 1 ) it represents topics by means of word dusters and employs a finite mixture model ( STM ) to represent a word distribution within a text ; 2 ) it constructs topics on the basis of corpus data before conducting topic analysis ; 3 ) it segments a text by detecting significant differences between STMs ; and 4 ) it identifies topics by estimating parameters 1°Here , k was set to 5 because the average length of a text was about 10 sentences .
Learning_Phrase-based_Variable $ 4 $ 74 $ 5 $ state recognizes a symbol wi E lZU { e } , where e/TERM is the/DEF empty string ./O
The_link_between_~i~l~ ( /guniang/ ,_girl ) _and $ 7 $ 116 $ 10 $ 35 ) /shi/ /sheng/ teacher student teacher and student Intuitively , there is a relationship , i.e. , 9-f ( /bing/ , and ) , between the two concepts denoted by the two words.
Introduction $ 1 $ 85 $ 76 $ In the case of a tie , the lower sense nmnber from WordNet is used , since this denotes a more general concept.
OUT $ -1 $ 191 $ 120 $ The first is query expansion replacing words in the query with a set of words of the same meaning .
Instructions_and_Interactivity $ 3 $ 38 $ 0 $ It is obvious that instructional situations profit from an interactive setting .
_The_Problem $ 1 $ 55 $ 49 $ We assume for present purposes that shoulder denotes a model-level category we can gloss as contributes-to-capitalization .
Abstract $ 0 $ 11 $ 8 $ Temporal expressions in Chinese form a complex system.
Related_work $ 6 $ 178 $ 29 $ Having grammar rules encoded as unit clauses alleviates this problem as does our decision to use lgg rather than rlgg .
Introduction $ 1 $ 16 $ 9 $ This is a linear representation over a new feature space a transformation of the original instance space to a higher dimensional and more expressive space.
Introduction $ 1 $ 46 $ 37 $ TM2/TERM contains/DEF elements which are translation segments ranging from whole sections of a document or multisentence paragraphs to smaller units , such as short phrases or proper names ./O
Sample_Dialogues $ 3 $ 112 $ 5 $ ( ASR output : I would like to the zoo historic sites in stanhope historic ) $2 : Did you say you are interested in going to a zoo?
EXOT $ 9 $ 130 $ 58 $ ( H3 ) There is a positive correlation between the sum of long run SEMCAT weights and the semantic coherence of a paragraph , the total paragraph SEMCAT weight.
Principles_and_Parameters $ 1 $ 8 $ 1 $ The syntactic component of a grammar in the principles and parameters ( henceforth P&P ) framework , is simply a collection of parameter values /One value per parameter .
EXOT $ 9 $ 166 $ 94 $ In order to facilitate the use of long runs as predictors , we modified the traditional measures of Boyd et al.
Abstract $ 0 $ 8 $ 7 $ The main methodological considerations associated with our current work are : a ) how natural dialogues can be reliably annotated to allow independent comparisons and correlations of prosodic and structural features , b ) the identification and classification of units of dialogue that reflect the ' joint action ' feature of interactive discourse ( ie .
Query_compositions $ 2 $ 76 $ 1 $ In particular we experimented the `` Boolean/TERM phrase/TERM '' modality/TERM , which allows/DEF the user to submit queries with keywords composed by means of logical operators ./O
_Generation_of_the_surface_phrase_from_the $ 4 $ 124 $ 87 $ LI : A summary describes about a trip by the Malay Railway , but the fare is not referred in it .
Complexity_Measures $ 2 $ 31 $ 10 $ 2.2 Query Complexity The standard query length for Web applications is between two and three words , and our experience with PictureQuest confirms that observation.
Generation_of_Crisp_Descriptions $ 3 $ 71 $ 17 $ 'Success'/TERM means that the/DEF properties in L are sufficient to characterize S ./O
_Introduction $ 1 $ 16 $ 10 $ annotations which capture information in the raw data at several different conceptual levels or mark up phenomena which refer to more than one level.
YAG's_Template_Specification $ 2 $ 45 $ 2 $ Template/TERM slots/TERM are parameters/DEF or variables that applications or users can fill with values ./O
Learning_Verb_Rules $ 3 $ 91 $ 42 $ The meanings of non/TERMinals used in the rule are following : 221 be/TERM (/TERM )/TERM represents auxiliary/DEF verb b~t ,/O cond/TERM (/TERM )/TERM represents various/DEF forms of conditionals by , aby , kdyby ,/O reflex_pron/TERM (/TERM )/TERM stands for reflexive/DEF pronoun se ( si ) ,/O gap/TERM (/TERM )/TERM is a/DEF special predicate for manipulation with gaps ,/O and k5/TERM (/TERM )/TERM stands for arbitrary/DEF non-auxiliary verb ./O
The_link_between_~i~l~ ( /guniang/ ,_girl ) _and $ 7 $ 107 $ 1 $ 106 between them sometimes.
How_to_generate_technical $ 2 $ 75 $ 24 $ Similarly , the syntactic constructions and the discourse structures of this component should correspond to the set of allowed constructions / structures in the CL .
Building_Spoken_Dialo~te_Systems $ 4 $ 120 $ 15 $ opt/TERM means/DEF an option and or means a disjunction ./O
Issues_and_proposals $ 3 $ 137 $ 94 $ This means that the standard segmentation of a dialog into utterances may have to be modified for the purposes of an RST analysis , although a segmentation into utterances and one into minimal units will be very similar .
_Background $ 2 $ 39 $ 23 $ 5 In its written form , Chinese/TERM is a/DEF sequence of characters ./O
Towards_building_a_parallel_corpus $ 2 $ 40 $ 8 $ Hence , tile/TERM mappings/TERM in ( 7 ) provide/DEF an explicit representation of the way information is re/Ordered and re-packaged when/O translated from Japanese into English .
Introduction $ 1 $ 29 $ 24 $ He selects the Mann-Whitney/TERM test/TERM that : uses/DEF ranks of frequency data rather than the frequency values themselves to compute the statistic ./O
_Generation_of_the_surface_phrase_from_the $ 4 $ 139 $ 102 $ Therefore , we introduce the relevance score , which represents the correspondence between the subject judgement and the correct document relevance .
Experiments $ 6 $ 114 $ 19 $ Both for execution time and space considerations for the learner and for fear of overtraining , we put a bound on the length of the RRE that could be learned , s We define an atomic/TERM RRE/TERM as any/DEF RRE derived without any concatenation operations ./O
Embedding_Translation_in_an $ 6 $ 143 $ 41 $ Also , at higher false rejection rates , the task performance is better for trigram-based translation model than the phrase trigram-based translation model since the precision of lexical choice is better than that of the phrase trigram-based model as shown in Table 3 .
Introduction $ 1 $ 14 $ 7 $ The distribution environment of a word is the set of words of other parts of speech that can be collocated with it .
_Functional_tag_of_the_constituent $ 5 $ 57 $ 1 $ The functional tag of the constituent embedding the problem in Figure 1 is DIR .
Compound_Verb_Groups $ 1 $ 16 $ 6 $ </vg> I <vg> am sorry </vg> that I <vg> did not know </vg> about the conference , I <vg> would have participated </vg> in it .
Conclusions_and_Discussions $ 3 $ 61 $ 7 $ Of course , new techniques to improved the accuracy of statistical model are the constant aim of our research .
Focusing_on_Definitory_Contexts $ 2 $ 15 $ 0 $ Two issues are addressed in this paper : be accessed directly and entirely through large-scale filters such as shallow parsers , access to Web pages is restricted to the narrow and specialized medium of a search engine .
_Background $ 1 $ 34 $ 19 $ CGUs/TERM , which represent/DEF grounding at the ' illocutionary level ' (/O Clark 1996 ) , have been proposed as a/DEF meso-level dialogue structure roughly the same level that dialogue games ( Carletta et al , 1997 ) or adjacency pairs (/O eg .
Chunking_with_the_Phrase_Rule $ 2 $ 30 $ 7 $ The parser follows a sequence of rules in order to build phrases out of parse islands .
OUT $ -1 $ 0 $ 0 $ Word Alignment of English-Chinese Bilingual Corpus Based on Chunks Sun Le , Jin Youbing , Du Lin , Sun Yufang Chinese Information Processing Center Institute of Software Chinese Academy of Sciences Beijing 100080 P. R. China lesun , ybjin , yfsun , ldu@sonata.iscas.ac.cn
Prerequisites $ 2 $ 19 $ 0 $ VERBMOBIL/TERM is a/DEF speech-to-speech translation project , which at present is approaching its end and in which over 100 researchers 1 at academic and industrial sites are developing a translation system for multilingual negotiation dialogues ( held face to face or via telephone ) using English , German , and Japanese ./O
The_Complexity_of_Extracting_a $ 2 $ 36 $ 24 $ The semantic/TERM vicinity/TERM of/TERM a/TERM node/TERM in a network consists of the/DEF nodes and the arcs reachable from that node by traversing a small number of arcs ./O
Principles_and_Parameters $ 1 $ 13 $ 6 $ grammars/TERM is the/DEF set of all possible combinations of parameter values (/O and lexicon ) .
_Relations_that_are_relevant_and_correct_in $ 3 $ 38 $ 4 $ Therefore , a WordNet synset containing n terms defines ~11 k synonym relations .
Robustness $ 4 $ 138 $ 47 $ There is a limit to the power of heuristics that we have determined using a large corpus of test data.
Interlingua_system_ ( ISS ) $ 3 $ 69 $ 0 $ As said before , the Interlingua/TERM system/TERM takes/DEF the SS of the sentence after applying the anaphora resolution module as input ./O
The_formula_is_valid_when_J_>_R_ ( that_is ,_the_judges $ 7 $ 117 $ 31 $ In Table 10 we show the normalized performance ( D ) of MEAD , for the six clusters at nine compression rates .
_Preliminary_Evaluation $ 3 $ 90 $ 35 $ Therefore , we produced the alignments for the 6 parallel corpora a parallel corpus comprises the English corpus and its translation into one of the three languages using one of the MT packages with English as the target language .
Generating_Summaries $ 4 $ 94 $ 2 $ It solves the task of mapping the DIREX structures selected in the dialogue nmmory into sequences of full fledged semant . ic sentence descriptions ( VITs ) , thereby performing the following steps : * Document Planning : Extracting , preparing and dividing the content of the dialogue memory into a predefined format .
Word_Clustering $ 3 $ 65 $ 21 $ For example , Hofmann's is of order O ( ]DIIWI2 ) , while ours is only of O ( ID I + ]WI2 ) , where IDI/TERM denotes the/DEF number of texts and/O IW]/TERM the/DEF number of words ./O
The_NJFun_System $ 2 $ 14 $ 0 $ NJFun/TERM is a/DEF real-time spoken dialogue system that provides users with information about things to do in New Jersey ./O
POS_Assignment $ 3 $ 61 $ 1 $ This is required for sentence analysis where every word in the sentence must have at least one POS .
Abstract $ 0 $ 54 $ 0 $ the linear ordering of the constituents of the Rhetorical Representation with a POSITION feature , as well as two other features , TEXT-LEVEL/TERM , which takes/DEF values such as paragraph or sentence ;/O and LAYOUT/TERM , which takes/DEF values such as wrapped-text and vertical list ./O
The_Diversity_of_Semantic $ 2 $ 27 $ 3 $ Here , we show several examples that demonstrate the diversity of the sel `` NO ' ' is a Japanese postpositiona| which can represent a wide range of semantic relations .
Content_Presentation $ 4 $ 110 $ 17 $ The informative/TERM abstract/TERM is the/DEF information obtained/DEF by this process as it is shown in Figure 1 ./O
Abstract $ 0 $ 2 $ 1 $ The query tool is developed to search the Verbmobil treebanks annotated at the University of Tfibingen .
Introduction $ 1 $ 30 $ 19 $ Ambiguity/TERM and synonymity/TERM of words is a/DEF property of natural language causing a very serious problem in IR ./O
Framing_the_generation_problem $ 3 $ 66 $ 16 $ We can assume that houses and their rooms are hearer-new until REA describes them; and that just those entities mentioned in the prior sentence are in-focus .
Rules_as_features $ 1 $ 8 $ 0 $ A common machine-learning solution to classification problems is rule induction ( Clark and Niblett , 1989 ; Quinlan , 1993 ; Cohen , 1995 ) .
Introduction $ 1 $ 21 $ 17 $ The segmentation/TERM component/TERM provides/DEF a word lattice of the sentence that contains all the possible words ,/O and the final disambiguation is achieved in the parsing process .
The_Structure_of_a_Relational $ 3 $ 86 $ 12 $ We require that each entity record provides a type for the entity in a field labelled Class .
Hyperonyms_in_NLG_systems $ 4 $ 75 $ 16 $ Thus , on the taxonomic path Tweety ( instance/Of ) Robin Bird Vertebrate Animal Object , the concept Bird is a basic-level one , which leads to a preference for using the corresponding lexical item when referring to some kind of bird ( i.e. , some concept or instance subsumed by it ) .
Identifying_'Words' $ 3 $ 51 $ 12 $ Using statistical measures of significance , it was found that most groups fell well within 5only two individual languages were near exceeding these limits of the proposed Human language word-length profile ( E1liott et al. , 2000 ) .
Quality_of_the_first_order_weights $ 4 $ 78 $ 20 $ For comparison with some other well-known machine learning algorithms , I complement the WPDV experiments with accuracy measurements for three other systems : 1 ) A system using a Naive Bayes probability estimation ; 2 ) TiMBL , using memory based learning and probability estimation based on the nearest neighbours ( Daelemans et al. , 2000 ) , 7 for which I use the parameters which yielded the best results according to Daelemans et al .
_The_co-reference_problem_in_summarization $ 4 $ 52 $ 28 $ When a group of three people created a multi-document summarization of 10 articles about the Microsoft Trial from a given day , one summary focused on the details presented in court , one on an overall gist of the day ' s events , and the third on a high level view of the goals and outcome of the trial .
The_Structure_of_a_Relational $ 3 $ 77 $ 3 $ Each record ( row ) in the entity file defines a unique entity .
_Tagging_NuLL-marker_CDM_pairs_ ( via $ 8 $ 118 $ 48 $ " denotes that no corresponding word is at the position ( beginning or end of sentence ) ; a , d , q , and u are part/Of-speech symbols in our segmentation dictionary , representing adjective , adverb , classifier , and auxiliary , respectively.
Experimental_work $ 4 $ 69 $ 13 $ The second model used a 1EuTrans ESPRIT-LTR Project 20268 2IMH/TERM has been reported recently as the/DEF most useful MCMC algorithm used in the WSME training process ./O
Acquisition_Process $ 4 $ 102 $ 3 $ Presently it builds a lexical semantic network for 16 .000 German words , where three different types of word classes are distinguished : nouns , verbs and adjectives .
Proposed_Architecture $ 1 $ 50 $ 28 $ Implemented in a combination of C++ , Java and Lisp , the new version represents a service-oriented architecture .
Theoretical_Ideas , $ 2 $ 59 $ 34 $ The PSA will primarily be controlled by voice commands through a hand-held or head-mounted microphone , with speech and language processing being handled by an offboard processor .
Only_the_relevant_attributes_of_each_semantic_role $ 5 $ 100 $ 9 $ Finally , the semantic role MODIFIER has the following attributes : Cat that contains the syntactic category of the constituent ; Identifier with the value of the discourse marker ; Prep with the preposition of the constituent and ENTITY , which is the object of the PP and contains the same attributes as the THEME .
Introduction $ 1 $ 9 $ 2 $ Error-correcting/TERM output/TERM codes/TERM ( ECOC/TERM ) have been introduced/DEF to machine learning as a principled and successful approach to distributed class encoding (/O Dietterich and Bakiri , 1995 ; Ricci and Aha , 1997 ; Berger , 1999 ) .
Interlingual_representation_issues $ 5 $ 159 $ 1 $ A 57 case in point is the representation of numbers.
Generating_Summaries $ 4 $ 135 $ 43 $ The goal of the operators uses an interface based on a triple with the following usage : o < description > This is the input position of the operator .
Only_the_relevant_attributes_of_each_semantic_role $ 5 $ 194 $ 103 $ For the correct generation into Spanish the following morphological rule is constructed : pronoun + third..person + plural + antecedent ( ~olice ' ) ~ ~sta ( pronoun , third person , feminine and singular ) The left-hand side of the morphological rule contains the interlingua representation of the pronoun and the right-hand side contains the pronoun in the target language .
Abstract $ 0 $ 45 $ 0 $ tions are the first level at which semantic predicates are associated with arguments .
Implementation $ 5 $ 141 $ 7 $ The result is a lattice of possible realizations , representing both the preserved ambiguity from previous processing phases and multiple ways of linearizing the sentence .
ADAM : _Architectural_Principles $ 3 $ 85 $ 62 $ However , at the best of our knowledge ADAM/TERM is the/DEF first corpus being architecturally designed by explicitly adopting the concept of annotation modularity and metascheme at different levels ./O
Middle $ 6 $ 213 $ 176 $ The overall performance in the sense tagger is 76.04% .
Related_work $ 6 $ 191 $ 42 $ ful parse , the set of candidate rules used in that parse constitutes a model.
OUT $ -1 $ 112 $ 10 $ Figure 5 shows a process of the compound noun indexing with an example .
Summary $ 6 $ 226 $ 13 $ Simplifying the domain specification task is a necessity as text generation systems move outside of research labs and into the real world , where the domain developer may not be a computational linguist , but a museum curator , personnel officer or wine salesman .
Abstract $ 0 $ 5 $ 0 $ Weighted/TERM Probability/TERM Distribution/TERM Voting/TERM ( WPDV/TERM ) is a/DEF newly designed machine learning algorithm , for which research is currently aimed at the determination of good weighting schemes ./O
Whole_Sentence_Maximum $ 2 $ 31 $ 11 $ The 5i values are obtained as the solution of the m equations : 1 Z = 0 w wEN ( 3 ) where/ = 1 , ... , m , f # ( w ) = ~=lfi ( w ) and f~ is a training corpus .
Example_Dialogue $ 3 $ 54 $ 31 $ cheerful ( U10 ) ) may be much more profound than a shorthand for December 25 -but , alas , conveying that is well beyond the simple grammar presented here.
OUT $ -1 $ 141 $ 39 $ We perform a retrieval experiment to evaluate the automatically extracted rules .
The_Generation_of_the_Initial $ 4 $ 39 $ 1 $ The result of its operation is a set of rules which assign a default category to each tag.
Abstract $ 0 $ 12 $ 10 $ Introduction WordSmith/TERM Tools/TERM ( Scott , 1998 ) offers/DEF a program for comparing corpora , known as KeyWords ./O
_Introduction $ 1 $ 18 $ 9 $ What makes the interlingua/TERM UNL/TERM special is its intended use : as an/DEF electronic language for networks ,/O it has to allow for high quality 2 conversation systems involving many languages .
Tree_Structures $ 2 $ 22 $ 6 $ The syntactic structure corresponding to the sentence " Hennessy will be a hard act to follow " is presented in Figure 1 as an example ( the syllable level has been omitted for clarity ) .
Abstract $ 0 $ 12 $ 8 $ Language/TERM models/TERM are important/DEF post-processing modules to improve recognition accuracy of a wide variety of input ,/O namely speech recognition ( Balh et al . , 1983 ) , handwritten recognition ( Elliman and Lancaster , 1990 ) and printed character recognition ( Sun , 1991 ) , for many human languages .
Middle $ 6 $ 72 $ 35 $ ( 2 ) High ambiguous words tend to be high frequent.
Discussions $ 6 $ 105 $ 36 $ They are : a ) dis'=-deglndis dis b ) dis' deg • dis c ) dis' /deg d ) dis' = -dis In deg Where dis'/TERM denotes the/DEF revional distance and/O dis/TERM denotes the/DEF original distance ./O
Knowledge_Representations $ 2 $ 73 $ 42 $ For example , if the user's first query were " I want to go to Denver next Friday morning , returning the following Wednesday , " the system would record that this is a round trip flight and would save the return date ( unresolved , in case there was a recognition error on the forward leg date ) in the user model .
Introduction $ 1 $ 65 $ 56 $ Barzilay and Elhadad use the notion of strong/TERM chains/TERM ( i.e. , chains/DEF whose scores are in excess of two standard deviations above the mean of all scores )/O to determine which chains to include in a summary.
OUT $ -1 $ 96 $ 54 $ The splitting criterion used in the experiments is the information gain measure .
The_steps_in_the_strategy_are_marked_with_the $ 7 $ 135 $ 13 $ ( Elhadad 1992 ) investigated a general computational framework that covers all aspects of generating evaluative arguments of single entities , from content selection and structuring to fine-grained realization decisions .
Complexity_Formulas $ 4 $ 57 $ 1 $ This is common in information theory ( Ash , 1965 ) ; that is , when the user makes a statement , it must be encoded , and the number of bits needed to encode the statement is a measure of its information content .
_Category_of_the_constituent_embedding_the $ 4 $ 55 $ 1 $ See Figure 1 : The category of the constituent embedding the NP the problem is PP .
The_Corpus $ 6 $ 152 $ 3 $ In the 80-sentence corpus under consideration , the sentence structure is complex and stylized; with an average of 20 words per sentence.
Tagged_Text $ 3 $ 70 $ 0 $ 2 .2 .1 The Lexicon There were two methods we used to construct the lexicon : open/TERM lexicon/TERM , which includes/DEF all words from the development set along with all determiners , pronouns , prepositions , particles , and conjunctions (/O these words are essential to achieving good sentence segmentation ) , and closed/TERM lexicon/TERM , which includes/DEF all of the development and testing words 2 ./O
Experiments $ 4 $ 85 $ 9 $ The features themselves were culled using this schema on 2290 sentences from the training data.
Introduction $ 1 $ 39 $ 35 $ It offers a tool for helping people edit templates and see what text would be realized from a template , given a set of values for its slots .
Implementing_Embedded_MT $ 2 $ 81 $ 24 $ MT systems were examined in light of the outputs of translation and the types of errors that can be generated by the translation engine .
OUT $ -1 $ 79 $ 74 $ The one notable exception is the work of ( Wang et al. , 2000 ) , which attempted a machine learning approach to question answering for the same reading comprehension task.
Experimental_setting $ 2 $ 63 $ 1 $ Section 3 ) , Yarowsky ( 1993 ) used a measure of entropy as well as the results obtained when tagging heldout data with the collocations organized as decision lists ( el .
Summary $ 7 $ 87 $ 0 $ RSTTool/TERM is a/DEF robust tool which facilitates manual analysis of a text 's rhetorical structure ./O
The_MATE_Markup_Framework $ 3 $ 177 $ 89 $ Example : The declaration Occursln : href ( lxanscription , u ) allows an attribute used as , e .g . , Occursln="base~_123" , where base is a coding file using the transcription module and u_123 is the value of the id attribute of a t~ element in that file .
Abstract $ 0 $ 4 $ 2 $ In this method , Hownet was used as our information source , and a co-occurrence frequency database of sememes was constructed and then used for WSD .
Modeling_various_degrees_of $ 5 $ 79 $ 16 $ One subfield is a set of propositions which the agent assumes for the sake of the conversation .
Methodology $ 2 $ 57 $ 14 $ As Kilgarriff & Rose ( 1998 ) note , even Pearson~ X 2 is suitable without the hypothesis-testing link : Given the non-random nature of words in a text , we are always likely to find frequencies of words which differ across any two texts , and the higher the frequencies , the more information the statistical test has to work with.
Summarization_Filters $ 4 $ 93 $ 0 $ As mentioned above , create goals are satisfied by summarization filters , which create new media objects summarizing information sources .
Abstract $ 0 $ 3 $ 1 $ The system INTHELEX , used to carry out this task , requires a logic representation of sentences to run the learning algorithm.
Performance $ 4 $ 173 $ 10 $ Time/TERM is the/DEF total time for the query in seconds ./O
Results_and_Discussion $ 5 $ 142 $ 11 $ WDD in parallel texts .
Clustering_Systematic_Polysemy $ 3 $ 81 $ 1 $ Our assumption is that , if a semantic concept is systematically related to another concept , words that have one sense under one concept ( sub ) tree are likely to have another sense under the other concept ( sub ) tree .
Architecture_and_Principles $ 3 $ 38 $ 15 $ The third structure is a list marked by bullets .
Statistics_Based_Hybrid_Approach_to $ 2 $ 27 $ 9 $ The task is to find RC/TERM , a/DEF most possible sequence of duples formed by base phrase tags and boundary tags ,/O among the POS sequence T . RC = ( <ro , co > . . . . . . . . <rn , Cn> ) , in whil~h ri ( l <i< =n ) indicates the boundary tags , ci represents the base phrase tags .
Discourse_coherence_and $ 1 $ 9 $ 1 $ In the theory of discourse structure developed by Grosz and Sidner ( 1986 ) , each discourse segment exhibits two types of coherence : local coherence among utterances inside the segment , and global coherence between this segment and other discourse segments .
YAG's_Template_Specification $ 2 $ 43 $ 0 $ Language A template/TERM is a/DEF pre/DEFined form with parameters that are specified by either the user or the application at run-time ./O
The_lexicon_and_its_benefits_to $ 3 $ 40 $ 11 $ An alternation is a variation in the realization of verb arguments .
_Preliminary_Evaluation $ 3 $ 59 $ 4 $ The SemCor data is tagged in lamning text words of varying parts of speech are tagged in context using WordNet 1.6 .
Acquisition_Process $ 4 $ 101 $ 2 $ GermaNet/TERM is the/DEF German counterpart to the well known WordNet ./O
PAC_Boolean_concept_learning $ 3 $ 51 $ 5 $ An example is a vector denoting the truth ( presence ,l ) or falsehood ( absence ,0 ) of propositional variables.
Architecture_of_WIT-Based_Spoken $ 3 $ 68 $ 34 $ The domain-dependent/TERM knowledge/TERM used in this module consists of a/DEF unification-based lexicon and phrase structure rules ./O
Psycholinguistic_production $ 3 $ 49 $ 14 $ The model consists of three layers of nodes : A layer of concept nodes with labelled concept links , a layer of lemma nodes , and a layer of word form nodes that include morpho94 logical information.
Abstract $ 0 $ 3 $ 0 $ A computational/TERM framework/TERM is presented which is used/DEF to model the process by which human language learners acquire the syntactic component of their native language ./O
OUT $ -1 $ 68 $ 15 $ Continuous/TERM Speech/TERM Recognition/TERM ( CSR/TERM ) 9 for audio indexing , followed by text retrieval techniques.
Query_Expansion $ 4 $ 143 $ 21 $ We normally expect to see slopes of .7 or more when t.f > 3 , but in this case ( b ( 3 , D , 0 ) = 0.11 ) , there is a considerable shrinking because we very much expected to see the term in the expansion and we didn't.
Dialog_Management $ 3 $ 102 $ 41 $ Thusfor a given path , higher its path utility , greater is the difficulty to understand the concepts it contains and thus higher is the level of expertise required .
Dialog_Management $ 3 $ 75 $ 14 $ If the ACCUiVLVALUE of a user crosses a threshold , the accumulated user expertise level changes long term as it is assumed that there is a change in the user ' s overall understanding of the solution .
Comparison_experiment $ 3 $ 167 $ 69 $ `` it is not sufficient that the string `` Robert Sheckley '' or `` Sheckley '' is in the text , but the document has to say that Robert Sheckley is the author of Options .
Middle $ 6 $ 65 $ 28 $ Low/TERM frequency/TERM denotes the/DEF number of occurrences less than 100 ,/O middle/TERM frequency/TERM denotes the/DEF number of occurrences between 100 and 1000 ,/O and high/TERM frequency/TERM denotes the/DEF number of occurrences more than 1000 ./O
Introduction $ 1 $ 13 $ 6 $ An ontology/TERM is a/DEF set of knowledge concepts about the world ./O
Sentence_planning_in_FOGS $ 4 $ 90 $ 3 $ Our discourse/TERM model/TERM is a/DEF knowledge store consisting of two major registers ./O
OUT $ -1 $ 156 $ 138 $ But when the words " story " or " this " appear , the question seems to be referring to the story in its entirety and the dateline is the best answer .
_General_Outline_of_the_Method $ 2 $ 100 $ 84 $ The most relevant earlier work is the experiment described in [8] using the machine learning algorithm C4.5 .
Maximum_Entropy_Modeling $ 2 $ 42 $ 0 $ Sentence patterns or pattern ruels specify the sub-structures of the sentences .
OUT $ -1 $ 48 $ 6 $ The final classification is the one attained when all rules have been applied.
Middle $ 6 $ 92 $ 55 $ A sense/TERM tag/TERM Ctag/TERM is in/DEF terms of a vector ( wl , w2 , ... , wn ) ,/O where n/TERM is the/DEF vocabulary size and/O wi/TERM is a/DEF weight of word cw ./O
Abstract $ 0 $ 7 $ 3 $ Bayesian , decision tree and neural network classifiers ) to discover language model errors .
OUT $ -1 $ 157 $ 157 $ Defined as " an/DEF optimized body of coordinated on-line methods and resources that enable and maintain a person's or an organization's performance , "/O EPSS/TERM interventions range from simple help systems to intelligent wizard-types of support.
Only_the_relevant_attributes_of_each_semantic_role $ 5 $ 145 $ 54 $ Moreover , there is a pronoun , lo ( him ) that functions as complement of the verb vi ( saw ) .
The_results_reported_in_this_paper_are_part_of_a_more $ 1 $ 40 $ 29 $ Section 4 describes and discusses the results obtained , while in the conclusions we propose some directions for future work .
SYSTEM : _U_S-air_flight_63_57_departs_la $ 9 $ 125 $ 118 $ Also , the tools presented here tend to reduce the growth in code size with complexity ( as measured by the number of possible constraints ) .
Abstract $ 0 $ 4 $ 2 $ Models using a lexicon alone are generally held to be incapable of explaining these data.
Related_Work $ 2 $ 80 $ 53 $ The key to our system is a WSD method for open text .
Robustness $ 4 $ 107 $ 16 $ Wrong Index : The robustness preprocessor tests whether the index points to the root of the graph or one of the subgraphs .
Adequacy $ 4 $ 115 $ 15 $ Regarding unambignity , the scheme is an improvement but has one failing : repetition of a letter represents either extended duration or the presence of multiple syllables .
Word_Domain_Disambiguation $ 3 $ 44 $ 0 $ In this section we present two baseline algorithms for word/DEF domain disambiguation and/O we propose some variants of them to deal with WDD/TERM in the context of parallel texts .
Abstract $ 0 $ 80 $ 19 $ The problem is the meaning.
_Decision_trees $ 4 $ 76 $ 11 $ Figure 2 gives a simpler decision tree that predicts the grammatical relation of a mention for Enearta at variety of kernel functions .
Word_Sense_Dis~mbiguation $ 4 $ 227 $ 135 $ The novelty of this method consists of the fact that the disambiguation process is done in an iterative manner .
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 126 $ 91 $ In formula ( 5 ) , di is the i-th document and consists of the number of n paragraphs ( see Figure 4 ) .
_System_Configuration $ 2 $ 94 $ 70 $ The document entitled J.~ZJ~ll/kJl~ ! ~3-~qF0~ ~1~I~~ ( Reorganization of Legend Corporation to welcome the joining of WTO ) receives the relevance rate of 100 % in both searches .
Segmentation $ 5 $ 189 $ 39 $ ( 15 ) this , however , is a political science course .
Comparison_experiment $ 3 $ 115 $ 17 $ ) Chi ~ il regista di "I Mostri" ( Who is the director of "I Mostri' ) Quale attore ha recitato con Benigni nel film "I1 piccolo Diavolo"?
The_Generation_System $ 3 $ 68 $ 26 $ 3 . 1 . 3 Alignment/Decomposition The heart of the lexical access algorithm is the decomposition process .
Using_CST_for_information_fusion $ 5 $ 142 $ 5 $ : \ Figure 6 : Processing stages G ~ / / / \ \ DOC2 _J Figure 7 : Summarization using graph cover operators The third stage is the automatic creation and typing of links among textual spans across documents .
_Functional_tag_of_the_constituent $ 5 $ 56 $ 0 $ embedding/TERM the/TERM NP/TERM : If/DEF the category of the constituent embedding the NP is associated with one or more functional tags , they are used as features ./O
Abstract $ 0 $ 90 $ 23 $ The sorted templates constitute the text plan .
Empirical_Evaluation $ 4 $ 149 $ 38 $ kNN/TERM is a/DEF lazy learning method in the sense that it does not carry out any off-line learning to generate a particular category knowledge representation ./O
Implementing_Embedded_MT $ 2 $ 94 $ 37 $ Much 20 current CyberTrans work consists of developing and transitioning tools which can accurately detect and remediate errors , converting documents into a standard ( normalised ) form .
_Introduction $ 1 $ 7 $ 0 $ Word/TERM sense/TERM disarnbiguafion/TERM (/TERM WSD/TERM )/TERM is one/DEF of • the most difficult problems in NLP ./O
The_lexicon_size_of_a_typical_large-vocabulary $ 9 $ 103 $ 24 $ Word/TERM segmentation/TERM is a/DEF natural by-product of large vocabulary Mandarin speech recognition ,/O and white space provides word boundaries for the English queries .
Introduction $ 1 $ 9 $ 1 $ While speed is one factor associated with the construction of such a device , another factor is the language type and format .
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 160 $ 125 $ 'Rec'/TERM (/TERM Recall/TERM )/TERM is the/DEF immber of correct events divided by the total mnnber of events which are selected by a human ,/O and 'Prec'/TERM (/TERM Precision/TERM )/TERM stands for the/DEF number of correctevents divided by the number of events which are selected by our method ./O
Contributions $ 6 $ 128 $ 2 $ An important issue for future research is the relation of question and task accommodation to plan recognition approaches to dialogue ( Sidner , 1985 ) .
Abstract $ 0 $ 2 $ 0 $ This paper describes a first attempt at a statistical model for simultaneous syntactic parsing and generalized word-sense disambignation .
Comparing_Three_Treebank $ 4 $ 77 $ 10 $ NP ) V@ NP!
Abstract $ 0 $ 10 $ 9 $ Dialog strategies and management should be adjusted to the evolving state of the user .
Dialogue_Management $ 2 $ 44 $ 0 $ 2.1 Reasoning Model A dialogue participant chooses his/her responses to the parter's communicative acts as a result of certain reasoning process.
LTAGs_and_Extraction $ 3 $ 48 $ 21 $ X/TERM °/TERM is the/DEF head of X m and the anchor of the etree ./O
Introduction $ 1 $ 24 $ 15 $ Section 2 describes parse selection and discusses the "compositional" statistical features employed in a maximum entropy approach to the task.
Prosody_Prediction $ 4 $ 68 $ 6 $ To decide on the prosody for these unlinked parts is a problem .
The_Importance_of $ 1 $ 22 $ 18 $ It would also facilitate the systematic corpus-based study of the meanings and functions of these sounds 4 .
Acquiring_Lexical_Translations $ 3 $ 64 $ 13 $ A/TERM string/TERM in/TERM a/TERM bilanguage/TERM corpus/TERM consists/DEF of sequences of tokens where each token ( wi-xi ) is represented with two components : a source word ( ]possibly an empty word ) as the first component and the target word ( possibly an empty word ) that is the translation of the source word as the second component ./O
Abstract $ 0 $ 4 $ 2 $ GoDiS/TERM is a/DERF prototype dialogue system for information-seeking dialogue , capable of accommodating questions and tasks to enable the user to present information in any desired order , without explicitly naming the dialogue task ./O
Architecture_and_Principles $ 3 $ 36 $ 13 $ The second collection is a list organized into two sublists .
_Our_Classification_Algorithm_for $ 2 $ 103 $ 39 $ So , at the beginning of the algorithm , we judge if the verb is a "linking verlS' ( ~ ( be~' , ~ ( equal~' ,etc ) or a "possessive verlS' ( '~-q~J' ( have ) ) .
Discussion $ 5 $ 204 $ 28 $ Such evaluations are costly , and they can not be the basis of work in stochastic generation , for which evaluation is a frequent step in research and development .
Applications $ 3 $ 91 $ 31 $ These are small pieces of cardboard with printed flight details that are the most fundamental artefact used by the air traffic controllers to manage their air space.
Abstract $ 0 $ 216 $ 74 $ Screen copies of the IG interface during an authoring process of a VIDAL notice are given on figures 1 and at a given stage .
Chunk_Types $ 3 $ 30 $ 13 $ Our NP chunks are very similar to the ones of Ramshaw and Marcus ( 1995 ) .
Comments $ 6 $ 75 $ 15 $ Overall accuracy : 94.88% phrasal recognition ) is a fairly easy task .
Abstract $ 0 $ 1 $ 0 $ Discourse markers are complex discontinuous linguistic expressions which are used to explicitly signal the discourse structure of a text .
Comparison_experiment $ 3 $ 166 $ 68 $ That is , if the question is "Who is the author of Options?
Informational_content_of_sentences $ 2 $ 43 $ 19 $ Maximal/TERM marginal/TERM relevance/TERM ( or MMR/ACR ) is a/DEF technique similar to CSIS and was introduced in ( Carbonell and Goldstein , 1998 ) ./O
About_Theory_Refinement $ 2 $ 16 $ 8 $ ples : ( a ) Identify the revision points ( b ) Correct them The first step consists of acquiring an initial grammar ( or more generally a knowledge base ) .
Structural_compatibility $ 3 $ 59 $ 16 $ Since the set {B ,C} is missing from this list , there is a grouping in figure 1 that is not realized in figure 2b , so these representations are not isomorphic .
OUT $ -1 $ 0 $ 0 $ Using Co-occurrence Statistics as an Information Source for Partial Parsing of Chinese Elliott Franeo DRABEK The State Key Laboratory for Intelligent Technology and Systems Department of Computer Science Tsinghua University , Beijing 100084 elliott_drabek@ACM . org Qiang ZHOU The State Key Laboratory for Intelligent Technology and Systems Department of Computer Science Tsinghua University , Beijing 100084 zhouq@slOOOe . cs . tsinghua . edu . cn
_Tagging_NuLL-marker_CDM_pairs_ ( via $ 8 $ 115 $ 45 $ The order of these attributes is : CDM , F1 , F2 , B1 , B2 , Fcom , Boom Acorn for Null marker location , and CDM , F1 , F2 , B1 , B2 , Fcom , Bcom , IsRDM for CDM classification , where IsRDM/TERM is a/DEF Boolean value ./O
Implementation $ 4 $ 368 $ 197 $ For example , Figure 2 illustrates a complex filter created by using a GUI to compose together a named entity extractor , a date extractor , a component which discovers significant associations between the two and writes the result to a table , and a visualizer/TERM which plots/DEF the results as a graph ./O
Interclausal_Coherence $ 4 $ 103 $ 13 $ The cue "however" alone does not give enough information to decide whether Sentence ( 3-c ) should connect to ( 3-b ) or to ( 3-a ) : further information is needed , like that there is a referential relation between the old MessagePad 120 and the MessagePad family.
_Introduction $ 1 $ 4 $ 0 $ Natural/TERM language/TERM generation/TERM involves/DEF a number of processes ranging from planning the content to be expressed through making encoding decisions involving syntax , the lexicon and morphology ./O
Introduction $ 1 $ 20 $ 13 $ Considering this , we assumed that it is a very promising domain for an experimental dialogue system .
OUT $ -1 $ 93 $ 51 $ This alleviates the problem of overpartitioning/TERM of/TERM data/TERM , which is a/DEF widely-recognized concern during decision tree growth ./O
Introduction $ 1 $ 45 $ 36 $ A hypotactic/TERM construction/sentence/TERM : a/DEF sentence that has a main clause and a dependent clause , connected by a cue phrase ./O
Using_CST_for_information_fusion $ 5 $ 146 $ 9 $ The third technique , information/TERM extraction/TERM [ Radev & McKeown 98 ] identifies/DEF salient semantic roles in text ( e . g . , the place , perpetrator , and effect of a terrorist event ) and converts them to semantic templates ./O
Using_Equal_Probability $ 5 $ 88 $ 5 $ Under this assumption , the number of bits of information conveyed by referring to one entity out of v possible entities is log2v .
Examining_SCF_Correlation $ 2 $ 40 $ 9 $ The latter verbs were chosen so that one of the verbs is a synonym , and the other a hypernym , of a test verb .
Analyst_Scenario $ 2 $ 29 $ 10 $ Korean Source Report Et-~ "~I-DlXi'~ ~oo ~ Cll~o" oj_a , xd~ .
OUT $ -1 $ 152 $ 101 $ So it is an excellent framework for experimenting with the interaction between aggregation and text planning .
The_hyperonym_problem $ 2 $ 18 $ 0 $ Following tile psycholinguistics literature , the hyperonym problem is regarded as all aspect of lemrna retrieval .
Global_View_on_the_DE $ 2 $ 22 $ 5 $ The central module of the DE iS a compiler that maps a structure specified at one of tile five first of the above strata on a structure at the adjacent stratum .
Approach $ 2 $ 31 $ 25 $ Like with the voting algorithms , we have tested these meta-classifiers with the output of the first classification stage.
Discussion $ 7 $ 170 $ 6 $ The verb rules defined here are less general then the basic verb groups ( Osolsob~ , 1999 ) .
Overview_of_the_system $ 2 $ 13 $ 5 $ In this case , the symbols are the POS tags ( Ci ) that belong to the corresponding chunk ( Si ) .
Introduction $ 1 $ 23 $ 15 $ The generator is freely available to the NLG research comnmnity ( see Section 5 below ) .
Abstract $ 0 $ 1 $ 0 $ This paper describes a Japanese dialogue corpus annotated with multi-level information built by the Japanese Discourse Research Initiative , Japanese Society for Artificial Intelligence .
Abstract $ 0 $ 3 $ 2 $ The ternary expressions that we use are not only linguistically-motivated , but also amenable to rapid large-scale indexing.
_The_Problem $ 1 $ 54 $ 48 $ Consider the partially saturated relation below that is the denotation of the relative clause of la at the point when the downstairs S has been parsed ( " which the three companies will equally shoulder " ) .
OUT $ -1 $ 0 $ 0 $ A Measure of Semantic Complexity for Natural Language Systems Shannon Pollard*and Alan W. Biermann Department of Computer Science , Duke University Box 90129 , D224 , LSRC , Durham , NC 27708-0129 office : ( 919 ) 660-6583 fax : ( 919 ) 660-6519 e-mail : shannon@cs.duke.edu
Conclusions $ 5 $ 136 $ 1 $ Semantics alone worked at least as well as Goldsmith's frequency-based approach.
Introduction $ 1 $ 23 $ 7 $ idf would assign values well outside this range .
OUT $ -1 $ 12 $ 12 $ In addition to these three themes , this year ' s workshop includes a special-theme session on • Principles for Evaluation of Dialogue Systems .
Evaluation $ 4 $ 117 $ 13 $ The improved KL indicates that the method improves the overall accuracy of SCF distributions .
Introduction $ 1 $ 9 $ 4 $ A dialogue/TERM manager/TERM facilitates/DEF the negotiation of parameter values between a user and an SDS ./O
OUT $ -1 $ 26 $ 23 $ to to from from Figure 1 : A single scenario for the colour domain In order to learn a rule set for a concept , EVIUS/TERM uses/DEF the relational learning method explained in section 3 , and defines the learning space by means of a dynamic predicate model ./O
Reordering_the_output $ 5 $ 100 $ 8 $ The most/TERM likely/TERM string/TERM in the word lattice is then decoded/DEF as follows : ^ W~ = argmax ( ~T o ~WT ) = arg max P ( ~VT I ) ~T ) (/O 6 ) Where o/TERM is the/DEF composition operation defined for weighted finite-state machines (/O Pereira and Riley , 1997 ) .
MALIN $ 4 $ 125 $ 19 $ For simple information requests we have identified two important concepts , termed Objects and Properties ( JSnsson , 1997 ) where Objects/TERM models the/DEF set of objects in the database and/O Properties/TERM denotes a/DEF complex predicate ascribed to this set ./O
Experimental_setting $ 2 $ 65 $ 3 $ As Yarowsky shows , both measures correlate closely , so we 208 only used the experimental results of decision Word PoS #Senses #Ex .
Conclusions $ 6 $ 139 $ 2 $ These sets typically tend to benefit from the Modified/TERM Value/TERM Difference/TERM Metric/TERM , which creates/DEF a condensed hyperspace of features ./O
Cross-corpora_experiments : $ 6 $ 144 $ 9 $ There is a worrying proportion of contradicting collocations .
Introduction $ 1 $ 75 $ 66 $ For this analysis , we define constants from WordNet 1.6 as denoted in Table 2.
Generating_from_Bare_Data $ 2 $ 12 $ 0 $ We start initially with a relational/TERM database/TERM , as defined/DEF by a set of tab-delimited database files , plus some minimal semantics ./O
Abstract $ 0 $ 14 $ 12 $ Consistency/TERM was measured/DEF by two means ./O
YAG ' s_Template_Specification $ 2 $ 43 $ 0 $ Language A template/TERM is a/DEF pre-defined form with parameters that are specified by either the user or the application at run-time ./O
Introduction_1 $ 1 $ 14 $ 9 $ Text either from the recognizer or directly input by IThis paper also appears in the proceedings of the Sixth International Conference on Applied Natural Language Processing~ Seattle , WA , April 2000. the user is then converted into some kind of logical formula , which abstractly represents the user 's intended command ; this formula is.then fed into a command interpreter , which execdtes the command .
Abstract $ 0 $ 21 $ 20 $ If each corpus generates a separate set of probabilities , which probabilities are the correct ones to use as a model of human language processing?
EXOT $ 9 $ 91 $ 19 $ Since our basic query unit is a paragraph , document/TERM frequencY/TERM ( df/ACR ) and inverse/TERM document/TERM frequency/TERM ( idf/ACR ) have to be redefined .
Method $ 2 $ 66 $ 21 $ pro ( 1 _p ) n-m ( 2 ) The probability of the event happening m or more times is : = ( 3 ) k=rn Finally , P/TERM (/TERM m+/TERM , n/TERM , p/TERM e/TERM )/TERM is the/DEF probability that m or more occurrences of cues for scfi will occur with a verb which is not a member ofscfi , given n occurrences of that verb ./O
The_Classifiers $ 3 $ 107 $ 70 $ Given a rule of the following format , IF Xl , X2 , • • , Xm THEN Yl , Y2 , .-. , Yn where xt , ... , xm are antecedents and Yt , ... , Yn are consequences , the algorithm derives a pair of vectors a and b such that • for each index i = 1 , ... , M , 1 ifwi = xj for some j 6 { 1 , ... , m } ai = 0 otherwise ( 19 ) where wi is the i th entry in the keyword feature table ; and for each index i = 1 , ... , N , 1 ifwi = yj for some j E { 1 , ... , n } bi = 0 otherwise ( 20 ) where wi is the class label of the category i .
Examples_of_YAG_in_use $ 3 $ 70 $ 5 $ M5/TERM is the/DEF proposition that the name of the discourse entity B2 is " Pluto " ./O
OUT $ -1 $ 232 $ 42 $ Both these attributes refer to an orthographic transcription , wref delimits the word ( s ) which caused or might cause a communication problem , and uref refers to one or more entire utterances which caused or might cause a problem .
Background : _The_STOP_System $ 2 $ 27 $ 10 $ STOP/TERM is a/DEF different type of application in that ( 1 ) there are many possible leaflets which can be generated ( and the system cannot tell which is best ) , and ( 2 ) no human currently writes personalised smoking-cessation leaflets ( because manually writing such leaflets is too expensive ) ./O
The_REXTOR_System $ 5 $ 167 $ 45 $ ( PRPZ/TERM is the/DEF part/Of-speech tag for possessive pronouns ,/O DT/TERM for determiners/DEF ,/O JJX/TERM for adjectives/DEF ,/O J JR/TERM for comparative/DEF adjectives ,/O JJS/TERM for superlative/DEF adjectives ,/O NNX/TERM for singular/DEF or mass nouns ,/O NNS/TERM for plural/DEF nouns ,/O NNPX/TERM for singular/DEF proper nouns ,/O NNPS/TERM for plural/DEF proper nouns ,/O IN/TERM for prepositions/DEF ./O
The_Generation_System $ 3 $ 125 $ 83 $ For the LCS-AMR in Figure 3 , the thematic/TERM hierarchy/TERM is what/DEF determined that the lunited statesl is the subject and Iquotal is the object of the verb Ireducel ./O
OUT $ -1 $ 78 $ 32 $ Only 26 OCR-ed " words " are found in the NIT lexicon , i.e. , recognized as valid Spanish words .
Word_Co/Occurrence_Vector $ 2 $ 48 $ 16 $ In this matrix : the rows and colunms correspond to words and the ith diagonal/TERM element/TERM denotes the/DEF number of documents in which the word wl appears , F ( wi ) ./O
The_results_reported_in_this_paper_are_part_of_a_more $ 1 $ 39 $ 28 $ Section 3 reports the experimental setting for the comparative evaluation of the three search modalities .
The_Filtering_Problem $ 1 $ 5 $ 1 $ FALCon , our embedded MT system , has been designed to assist an English-speaking person in filtering , i .e . , deciding which foreign language documents are worth having an expert translator process further .
OUT $ -1 $ 202 $ 131 $ The first is the database of dependency microcontexts extracted from a large text corpus .
Reconstructing_Centering_for_NLG $ 2 $ 21 $ 1 $ The center/TERM in/TERM an/TERM utterance/TERM Un/TERM is the/DEF most grammatically salient entity realised in U~_i which is also realised in Un ./O
Dialogue_Structure_and $ 6 $ 95 $ 10 $ The DS/TERM tag/TERM consists/DEF of a topic break index ( TBI ) , a topic name and a segment relation ./O
The_Structure_of_a_Relational $ 3 $ 101 $ 27 $ ® Fact : each entry in a record defines what we call a fact about that entity , a A/TERM fact/TERM consists/DEF of three parts : its predicate name , and two arguments , being the entity of the record , and the filler of the slot ./O
Note_that_the_nouns_at_this_point_are_types_not $ 4 $ 146 $ 31 $ FG/TERM is the/DEF French translation of the Brown corpus rendered by the MT system GL ;/O GG/TERM is the/DEF German translation by GL ;/O SG/TERM is the/DEF Spanish translation by GL ;/O SS/TERM is the/DEF Spanish translation by the MT system SYS ;/O and MSp/TERM is the/DEF merged Spanish translations from both NIT systems ./O
Abstract $ 0 $ 24 $ 23 $ We then ask two questions : Do these verbs have the same subcategorizafion probabilities across corpora , and , when there are differences , what is the cause.
Evaluation $ 3 $ 200 $ 11 $ Also , the skip ratio is 65% , which is much higher than the skip ratio of 0 . 1% if we did not use the classifier .
POS_Assignment $ 3 $ 95 $ 35 $ If a character string reaches the threshold of more than one P ( Cat ) , it will be assigned more than one syntactic category .
Highlight $ 3 $ 140 $ 89 $ This contains a set of all the sorts and words found in the file .
Introduction $ 1 $ 51 $ 42 $ We take cue phrases "as for" or "what is more" to signal elaboration relations 3 .
Extending_a_Grammar_to_Enable $ 4 $ 121 $ 48 $ " , in favor of " his sister ' s dog " , without the application having to request a pronoun explicitly , as in the example shown above , we could add a rule to force the pronominal feature of the inner most possessor to be YES , whenever a ( repeated ) noun phrase is a possessor of a possessor of the primary noun .
Experiments $ 5 $ 153 $ 1 $ We used the `` short query ' ' condition of the NACSIS/TERM NTCIR-1/TERM Test/TERM Collection/TERM ( Kando et al . , 1999 ) which consists/DEF of about 300,000 documents in Japanese , plus about 30 queries with labeled relevance judgement for training and 53 queries with relevance judgements for testing ./O
Our_approach_to_Multilingual $ 2 $ 27 $ 0 $ Document Authoring Our Multilingual Document Authoring system has the following main features : First , the authoring process is monolingual , but the results are multilingual.
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 118 $ 83 $ In a similar way , DispPt denotes dispersion of term t in the level of Paragraph .
Background $ 2 $ 59 $ 39 $ requestValue ( p ) : system requests a value for the paramter p. p E params ( AD ) U {aTask}.
The_Interactional_Framework $ 2 $ 25 $ 5 $ 4 The planner we use is a modification of the DRIPS decision-theoretic hierarchical planner ( Haddawy and Hanks , 1998 ) .
Impact_of_Missing_Translations $ 8 $ 127 $ 3 $ To test the conjecture , for each English query term , a native speaker in Chinese or Spanish manually checked whether the bilingual lexicon contains a correct translation for the term in the context of the query .
Introduction $ 1 $ 16 $ 10 $ 116 As ( lc ) records , these are the intersecting dotted segments of ( 1 a ) , and can be designated as such .
Comparison_experiment $ 3 $ 125 $ 27 $ ( Which is the longest river of the worM?
Reference_Resolution $ 4 $ 104 $ 5 $ If there is a match , assume the current PN is being used to corefer to the referent of the matching PN .
Representing_cross-document $ 3 $ 91 $ 9 $ Definition An extractive/TERM summary/TERM S/TERM of a cube C is a/DEF set of document units , S c C ,/O see Figure 3 ( d ) .
Subcategorization_Frequency $ 3 $ 56 $ 0 $ 3.1 Methodology : For the second experiment , we coded the examples of the 64 verbs from each of the three corpora for transitivity .
Note_that_the_nouns_at_this_point_are_types_not $ 4 $ 197 $ 82 $ Resnik proposed an unsupervised method for sense disambiguation using selectional preference information , thereby using grammatical relations between words in a corpus in order to arrive at the correct sense for a word .
Introduction $ 1 $ 94 $ 90 $ In fact , the main idea of the semi-recursive algorithm is the separated l St-order relations LB approaches are not adapted to text generation , where lexical choices must be done in a global , holistic perspective ( Danlos , 1998 ) and ( Busemann , 1993 ) .
Interaction_Grammars $ 3 $ 127 $ 74 $ ger : : country --> "Germany".
_Head_countability_preferences_of_the_head $ 7 $ 60 $ 0 $ of the NP : In case the head of an NP is a noun we also use its countability as a feature .
Abstract $ 0 $ 6 $ 5 $ Introduction Verb subcategorizafion probabilities play an important role in both computational linguistic applications ( e .g .
Perfect_Sampling $ 3 $ 47 $ 3 $ In PS , we obtain a sample from the limit distribution of an ergodic Markov Chain X = { Xn ; n _ > 0 } , taking values in the state space S ( in the WSME case , the state/TERM space/TERM is the/DEF set of possible sentences )/O .
Support_Vector_Machines $ 2 $ 8 $ 2 $ In the field of natural language processing , SVMs are applied to text categorization , and are reported to have achieved high accuracy without falling into over-fitting even with a large number of words taken as the features ( Joachims , 1998 ; Taira and Haruno , 1999 ) First of all , let us define the training data which belongs to either positive or negative class as follows : ( Xl , YX ) , ... , ( Xl , Yl ) Xi 6 R n , Yi 6 { +1 , -1 } xi is a feature vector of the i-th sample represented by an n dimensional vector , yi is the class ( positive ( +l ) or negative ( -1 ) class ) label of the i-th data .
Conclusion_&_Future_Work $ 7 $ 124 $ 2 $ In Our future work includes : 1 ) Because the sparsity of collocations is a main factor of affecting the word clustering accuracy , we can use the clustering results to discover new data and enrich the thesaurus .
Aspect_in_Lexical_Conceptual $ 3 $ 101 $ 1 $ The LCS/TERM framework/TERM consists/DEF of primitives ( GO , BE , STAY , etc . ) ./O
Models $ 2 $ 57 $ 37 $ p ( TIS ) -1~IT] , where p/TERM is the/DEF model being evaluated ,/O and (/TERM S/TERM ,/TERM T/TERM )/TERM is the/DEF test corpus ,/O considered to be a set of statistically independent sentence p ( w[hi ,s ) = q ( wlhi ) exp ( ~ses asw + aA ( i ,j~ ,O ,B ( s ,t ) ) pair s ( s ,t ) .
Conclusions_and_Future_Work $ 5 $ 260 $ 4 $ Content-based/TERM measures/TERM assign/DEF different rankings when ground truths do disagree in focus ./O
Explaining_Probabilistic_Methods $ 3 $ 62 $ 14 $ ) A statistical/TERM queries/TERM algorithm/TERM is a/DEF learning algorithm that constructs its hypothesis only using information received from an SQ oracle ./O
Reference_Resolution $ 4 $ 131 $ 32 $ # .2.1 Explicit pronouns The basic form-based strategy for resolving pronominal reference is to begin by inspecting in reverse order of mention those referring expressions whose forms are compatible with the morphological constraints imposed by the pronominal .
_Generation_of_the_surface_phrase_from_the $ 4 $ 76 $ 39 $ Task-based/TERM evaluation/TERM in general consists/DEF of the following three steps : ( l ) Data preparation : Assume an information need , create a query for the information need , and prepare simulated search results with different types of summaries ./O
_Introduction $ 1 $ 22 $ 15 $ Potentially , we can have parallel corpora in a myriad of languages , yet the downside is the scarcity of linguistic knowledge resources and processing tools for less widely represented/studied languages .
Abstract $ 0 $ 2 $ 1 $ We present a tool for the acquisition and the typing of NEs from the Web that associates a harvester and three parallel shallow parsers dedicated to specific structures ( lists , enumerations , and anchors ) .
BOV_Stem_NE_Defaults_Qspecific_41 $ 5 $ 64 $ 34 $ Of course , our application is sentence retrieval , not document retrieval , so we define term/TERM frequency/TERM as the/DEF number of times the word appears in the candidate sentence ,/O and document/TERM frequency/TERM as the/DEF number of sentences in which this word appears ./O
Discussion $ 4 $ 111 $ 5 $ Second , the method is a potentially helpful extension to memory-based learning of language processing tasks .
OUT $ -1 $ 247 $ 57 $ The MATE Workbench allows its users to specify a coding module via a coding module editor.
Introduction $ 1 $ 16 $ 0 $ A major advantage of inductive logic programming is the ability to incorporate domain knowledge ( background knowledge ) into the inductive process.
Language_Modeling_using $ 4 $ 107 $ 30 $ The lower part of Table 1 shows the comparison results of the standard bigram model and the context language model .
_Introduction $ 1 $ 9 $ 3 $ Penn Treebank has enabled and motivated corpus and computational linguistic research based on information extractable from structurally annotated corpora .
Transformation_rule_lists $ 2 $ 25 $ 0 $ The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set .
Methodology $ 2 $ 45 $ 6 $ The reason for choosing it is that newspaper text is the most typical kind of reference corpus used by applied linguists , mainly because it is easy to get .
Summarization_Filters $ 4 $ 127 $ 34 $ For the above example , the pattern would be "Here is @6.label" , where 6 is the number of a non-narrative node , with label being its label.
Abstract $ 0 $ 48 $ 46 $ The Chinese query is translated into English via looking up the English senses of Chinese query term and words in its associated word list in a Chinese-English dictionary.
OUT $ -1 $ 136 $ 99 $ As a result , languages whose syntactic structures deeply differ from the English ones may 30 present an additional level of complexity that makes mapping to/from UNL impossible or unrealistic .
OUT $ -1 $ 165 $ 160 $ We pick as the answer to the question the sentence whose feature vector was classified positive with the highest confidence , or in the absence of such , the sentence classified negative with the lowest confidence .
OUT $ -1 $ 111 $ 93 $ For example , the question "What is the name of the creek?
Hyperonyms_in_NLG_systems $ 4 $ 83 $ 24 $ He gives the example [Reiter 1991 , p. 248] of a speaker pointing the hearer to a cow and a horse with the utterance Look at the animals / mammals / vertebrates , t None of the terms is basic-level or signigificantly shorter than the others , yet there is a clear order of-'normality ' in the sequence of the three candidates .
OUT $ -1 $ 0 $ 0 $ Dynamic User Level and Utility Measurement for Adaptive Dialog in a Help-Desk System Preetam Maloor Department of Computer Science , Texas A & M University , College Station , TX 77843 , USA preetam@csdl.tamu.edu Joyee Chai Conversational Machines IBM T. J. Watson Research Center , Hawthorne , NY 10532 , USA jchai@us.ibm.com
Accommodation_in_GoDiS $ 3 $ 76 $ 25 $ Furthermore , as this is the first indication of what the customer wants , the travel agent cannot have a plan with detailed questions .
Introduction $ 1 $ 9 $ 5 $ kNN and SVM have been reported as the top performing methods for English text categorization ( Yang and Liu , 1999 ) .
_Annotation_Guidelines_I : $ 3 $ 103 $ 57 $ ( 4 ) Default Inheritance Hierarchy for Categories a ) Lexical Categories : V > N > P > Ng b ) Phrasal Categories : S > VP > NP > PP > GP When phrasal conjuncts are involved , S/TERM is the privileged category since it is the start symbol of the grammar .
Introduction $ 1 $ 38 $ 34 $ Fiof prospective arguments on two models : ( 1 ) a normally , we illustrate the operation of our mechanism/TERM mative/TERM model/TERM , which represents/DEF NAG ' s beliefs ,/O and with an example , discuss results from our preliminary ( 2 ) a user/TERM model/TERM , which represents/DEF a user ' s presumed beliefs ./O
_System_Configuration $ 2 $ 68 $ 44 $ When we make our integrated search and summarization system available to him , the only word in his mind is the name of the company ~ ,~ ( Legend ) .
Pre-processing_design $ 2 $ 55 $ 20 $ The next section describes the pipeline up to tagging .
Related_works $ 4 $ 177 $ 13 $ 9,124 compounds are extracted from the corpus consists of 74,404 words , with the precision of 47.43% .
Rule_set_learning $ 3 $ 64 $ 26 $ , word._X B~ , lemma-X sl , ... , lemma_X B~ , sem-X B1 , ... , sem_X B~ , context ) where B1/TERM ,/TERM .../TERM ,/TERM Bn/TERM are the/DEF unrepeated terminal nodes from A1 , ... , An ,/O context/TERM is the/DEF set of all predicates subsumed by the syntactico-semantic structure between the nearest positive example on the left and the nearest one on the right ,/O and sem_XB/TERM is the/DEF list of isa_X and has_hypernym_X predicates for Bi ./O
Proper_name_recognition $ 4 $ 113 $ 39 $ ( 9 ) biZ~ ~ ( i0 ) the bill is . . . ( 11 ) the bill clinton is . . . ( 12 ) the bill clinton administration is The lexically ambiguous bill , interpreted as a proper name in isolation , becomes a common noun if preceded by a determiner .
_Semantic_classes_of_the_head_of_the_NP : _If $ 8 $ 66 $ 0 $ the head of the NP is a noun we also take into account its semantic classification in a large semantic hierarchy .
Corpus_comparison_based_on $ 6 $ 154 $ 45 $ Coverage/TERM means that how/DEF many pairs which appeared in a test set also appear in a trainlug set ./O
Reference_Resolution $ 4 $ 126 $ 27 $ The same procedure can be used to establishing that the reference of Doctor Andreu is the same as that of Docteur Andreu : establish the semantic class of Doctor Andreu , inspect each existing referent of that class to 5 see whether or not a plausible connection can be established.
The_Verbmobil_treebanks $ 2 $ 51 $ 17 $ 1 , one needs to search for trees containing a node nl with label PX and grammatical function 0A-MOD , a node n2 with label VF that dominates nl , a node n3 with label MF and a node n4 with label NX and grammatical function 0A that is immediately dominated by n3 .
Implementation $ 5 $ 137 $ 3 $ The generation/TERM component/TERM consists/DEF of the following subcomponents : Decomposition and lexlcal selection First , primitive LCSes for words in the target language are matched against CLCSes , and tree structures of covering words are selected ./O
_Tagging_NuLL-marker_CDM_pairs_ ( via $ 8 $ 127 $ 57 $ RDMs/TERM is a/DEF table searching process ./O
The_Structure_of_a_Relational $ 3 $ 91 $ 17 $ A link/TERM file/TERM consists/DEF of two columns only , one identifying the entity , the other identifying the filler (/O the name of the attribute is provided in the first line of the file , see figure 3 ) .
Analyzing_the_Reading $ 4 $ 137 $ 38 $ the first answer among the list of possible answers for each question is the correct one ) .
Information_Structures $ 3 $ 80 $ 3 $ The informaton structure of this example consists of two parts , the dependency relations and the HowNet definitions .
OUT $ -1 $ 0 $ 0 $ Japanese Dependency Structure Analysis Based on Support Vector Machines Taku Kudo and Yuji Matsumoto Graduate School of Information Science , Nara Institute of Science and Technology {taku-ku , matsu}@is , aist-nara , ac .
Learning_validation_and_results $ 4 $ 174 $ 36 $ A Perl program presents to one expert all the N-V pairs that appear in one sentence in a part of the corpus and include one of the studied nouns .
Memory-based_learning $ 3 $ 74 $ 17 $ The first quantity is normalized with the a priori probabilities of the various feature values of feature F : H ( C ) Eveva es ( F ) P ( v ) × H ( QF=v] ) ( 6 ) Here , H/TERM (/TERM C/TERM )/TERM is the/DEF class entropy , defined as H ( C ) =~ P ( c ) log 2P ( c ) ./O
System_Design $ 3 $ 73 $ 22 $ 33 Korean Documents Parser Tagged l Korean Documents ( LexiconK°rean 1 ~ Syntactic . . . . . . Eaglish Grammar Structure ( English ) RealPro English Lexicon / ' S~'ntactic Realizer Sentence ( English ) t Parsed Document ~ : : i~i°~'~vii~i?
Building_Spoken_Dialo~te_Systems $ 4 $ 112 $ 7 $ ( case head ) It means that the case feature is used and it is a head feature 3 .
_KNOWLEDGE_EXTRACTION $ 3 $ 158 $ 100 $ The knowledge/TERM sources/TERM for future identification of organizations are the/DEF accumulated lists of the organization names , the proper names of organizations and the organization types ./O
Qualitative_Evaluation_of_the $ 4 $ 158 $ 57 $ A substitution/TERM represents a/DEF case in the string metrics in which not only a word is in the wrong place ,/O but the word that should have been in that place is somewhere else , Therefore , substitutions , more than moves or insertions or deletions , represent grave cases of word order anomalies.
Experiments $ 4 $ 81 $ 2 $ DIMIN is a Dutch diminutive formation task derived from the Celex lexical database for Dutch ( Baayen et al. , 1993 ) .
_Results_and_comparison_of_ambiguities $ 3 $ 102 $ 21 $ when one lexeme represents at least 80% of the class.
Modelling_Phonological_Dyslexia $ 4 $ 84 $ 3 $ FL was unable to supply a sound for single letters ( which argues that the abstract rulebased route is impaired ) although she could read non-words normally ( which contradicts the 15 Table 1 : Reading performance of patient WB and versions of faulty and non-faulty PbA .
Implementing_Embedded_MT $ 2 $ 116 $ 59 $ The first of these is the problem of API's from COTS systems and GOTS systems.
Determining_a_User ' s_Line_of $ 3 $ 65 $ 8 $ Hence , presenting them to the user for selection is a reasonable course of action .
Setting $ 3 $ 87 $ 8 $ From these corpora , a group of 21 words which frequently appear in the WSD literature has been selected to perform the comparative experiments ( each word is treated as a different classification problem ) .
System_Design $ 3 $ 56 $ 5 $ Extraction/TERM Pattem/TERM Library/TERM -which contains/DEF the set of extraction patterns learned in the lab , one set per scenario template -to extract specific types of information from the input Korean documents , once parsed ./O
Abstract $ 0 $ 1 $ 0 $ This paper describes a framework for multidocument summarization which combines three premises : coherent themes can be identified reliably ; highly representative themes , running across subsets of the document collection , can function as multi-document summary surrogates ; and effective end-use of such themes should be facilitated by a visualization environment which clarifies the relationship between themes and documents .
The_CGS_system $ 4 $ 79 $ 11 $ There are ' two , situations where the text planning module helps specifically in the generation of referring expressions : ( 1 ) when the complexity for expressing a graphic demands an example and 5 " Graphemes are the basic building blocks for constructing pictures .
OUT $ -1 $ 17 $ 14 $ Starting with C , EVIUS/TERM reduces/DEF set b/Of unlearned concepts iteratively by selecting subset P C/g formed by the primitive concepts in/.4 and learning a rule set for each c E P 4/O For instance , the single colour scenario 5 in fig3With EuroWordNet ( http : //www.hum.uva.nl/-ewn/ ) synsets.
Background $ 2 $ 78 $ 58 $ withdraw/TERM (/TERM p/TERM )/TERM : system/DEF withdraws from dialogue for reason p ./O
Summary_and_Future_Work $ 4 $ 208 $ 4 $ If the model-based and language-specific features are aggregated as a single feature vector , the recall and precision of errors are 83% and 35% , respectively , which are the same if we just use language-specific features .
The_Complexity_of_Extracting_a $ 2 $ 55 $ 43 $ The type of attack could appear as a level-0 fact as in `` the Medellin bombing ' ' ( assuming that the network is built at the phrase level ) because in this case both the attack designator ( bombing ) and the modifier ( Medellin ) occur in the same node .
Description_of_the_algorithm $ 2 $ 16 $ 2 $ We assume that in the spoken dialog a sentence consists of slightly related subphrases.
MIMIC's_Concept-to-Speech $ 4 $ 60 $ 23 $ INFERRABLE information is not explicitly exchanged between the system and 2In the examples , small capitalization denotes a word is accented .
_KNOWLEDGE_EXTRACTION $ 3 $ 89 $ 31 $ The samples of organizations from the CKIP dictionary As we observed , the morphological structure of an organization name usually is a compounding of a proper name and a organization type .
The_Feasibility_of_the_STL $ 3 $ 58 $ 16 $ For instance , if a sentence does not have a relative clause , it will not express parameters that concern only relative clauses ; if it is a declarative sentence , it won ' t express the properties peculiar to questions ; and so on .
Abstract $ 0 $ 15 $ 5 $ Within each of these types , there are a number of conceptual/TERM primitives/TERM of that type , which are the/DEF basic building blocks of LCS structures ./O
Overview_of_the_Approach $ 2 $ 25 $ 5 $ We explain the features of the Geoquery representation language through a sample query : Input : "W'hat is the largest city in Texas?
OUT $ -1 $ 79 $ 33 $ Forty-nine of the OCR-ed " words " are treated as " not/DEF found words "/O ( NFWs/TERM ) by the MT engine , even though they may in fact be actual Spanish words .
Learning_Verb_Rules $ 3 $ 79 $ 30 $ Thus we have to check whether the values are the same or the conditions of polite way of addressing are satisfied.
OUT $ -1 $ 71 $ 53 $ The other rules used by Quarc look for a vari~We used a stopword list containing 41 words , mostly prepositions , pronouns , and auxiliary verbs .
Attribute_Grammars $ 3 $ 61 $ 8 $ At-/TERM tribute/TERM Evaluation/TERM is the/DEF process of computing values for every attribute instance in the tree according to the semantic rules defined for each production ./O
Implementation $ 4 $ 221 $ 50 $ In the document graph , each node represents a document vector , and two nodes have an edge between them if and only if the similarity between the two document vectors is above a threshold.
Abstract $ 0 $ 3 $ 2 $ The parsers combine lexical indices such as discourse/DEF markers with formatting instructions (/O HTML/TERM tags/TERM ) for analyzing enumerations and associated initializers .
Using_linguistic_constraints $ 3 $ 70 $ 10 $ Head OK filters out rules ' , where the LHS has a head category which is not found on the RHS .
Results $ 3 $ 54 $ 5 $ In the double-pass method finding the most likely tag for each word was split in finding chunk boundaries and assigning types to the chunks .
Related_Work $ 5 $ 129 $ 11 $ As for the second point , we doubt the appropriateness to use the word ' s distribution as a measure of combination of two models .
Utterance_Units $ 4 $ 39 $ 0 $ In the transcription , an utterance/TERM is defined as a/DEF continuous speech region delimited by pauses of 400 msec or longer ./O
Dialogue_Management_Strategies $ 3 $ 131 $ 43 $ This reduction is based on a hierarchy of quality measures for each flight , beginning with any stated or inferred preference ( e.g. , a particular airport in the case of cities with multiple airports , or a particular airline in the case of a multi-leg booking where one leg has already been established ) and including number of stops and length of flight .
Abstract $ 0 $ 1 $ 0 $ Conversational/TERM grunts/TERM , such/DEF as uhhuh , un-hn , rnrn , and oh are ubiquitous in spoken English , but no satisfactory scheme for transcribing these items exists ./O
Evaluation_Measures $ 2 $ 98 $ 73 $ Thus , recall-based measures are likely to violate both properties ( i ) and ( ii ) , discussed at the beginning of Section 2.
Explaining_Probabilistic_Methods $ 3 $ 54 $ 6 $ This section defines a learning algorithm and a class of hypotheses with some generalization properties , that capture many probabilistic learning methods used in NLP .
Complexity_Formulas $ 4 $ 56 $ 0 $ Now that the domain is specified , we can analyze its semantics by estimating the number of bits of information conveyed by referring to each different aspect of the domain .
Human_Annotation_of $ 2 $ 66 $ 1 $ We measured stability/TERM ( the/DEF degree to which the same annotator will produce an annotation after 6 weeks )/O and reproducibility/TERM ( the/DEF degree to which two unrelated annotators will produce the same annotation )/O , using the Kappa/TERM coefficient/TERM K/TERM ( Siegel and Castellan , 1988; Carletta , 1996 ) , which controls/DEF agreement P ( A ) for chance agreement P ( E ) : K = P{A ) -P ( E ) 1-P ( Z ) Kappa/O is 0 for if agreement is only as would be expected by chance annotation following the same distribution as the observed distribution , and 1 for perfect agreement .
The_results_reported_in_this_paper_are_part_of_a_more $ 1 $ 53 $ 42 $ ( Which is the longest world river?
Bridging_Natural_Language_and $ 4 $ 82 $ 3 $ One of the most notable computational inadequacies of the finite-state model is the absence of a pushdown mechanism to suspend the processing of a constituent at a given level while using the same grammar to process an embedded constituent ( Woods , 1970 ) .
Introduction $ 1 $ 15 $ 8 $ which makes the crucial distinction between nucleus/TERM , which is the/DEF most important part of a message ,/O and satellite/TERM , which is the/DEF peripheral part of the message ./O
_Pronominalise_the_Cb_only_after_a_Continue $ 4 $ 90 $ 51 $ concession approve ( fda , elixir-plus ) cause NUCL~ S~LITE ban ( fda , elixir ) contain ( elixir , gestodene ) Figure 2 : Rhetorical structure The text planner has been developed within ICONOCLAST/TERM , a/DEF project which investigates applications of constraint-based reasoning in Natural Language Generation using as subjectmatter the domain of medical information leaflets ./O
Dialog_Management $ 3 $ 65 $ 4 $ If a user is using the system for the first time , a good indication of the initial user expertise level is the level of detail and technical complexity of the initial query.
_Features $ 1 $ 104 $ 33 $ recall x precision ) of detecting language model errors by examining the logarithm conditional probabilities on the maximum likelihood path .
Stochastic_Surface_Realization $ 2 $ 79 $ 21 $ act-query content depart_time depart_city New York arrive_city San Francisco depart_date 19991117 } Figure 4 : an input frame to NLG The generation engine uses the appropriate language model for the utterance class and generates word sequences randomly according to the language model distributions .
Memory-based_learning $ 3 $ 75 $ 18 $ ( 7 ) cEClass/TERM H/TERM (/TERM C[F=v]/TERM )/TERM is the/DEF class entropy computed over the subset of instances that have v as value for Fi ./O
Dialogue_Management $ 2 $ 51 $ 7 $ The reasoning/TERM model/TERM consists/DEF of two functionally linked parts : 1 ) a model of human motivational sphere ; 2 ) reasoning schemes ./O
Framing_the_generation_problem $ 3 $ 64 $ 14 $ Our model of prominence is a simple local one similar to ( Strube , 1998 ) .
_Interaction_Recorder : _for_the_recording $ 9 $ 171 $ 33 $ Our results indicate that machine learning is an effective approach to improving the accuracy of discourse marker tagging .
Introduction $ 1 $ 11 $ 3 $ The question of intellectual attribution is important for researchers : not understanding the argumentative status of part of the text is a common problem for nonexperts reading highly specific texts aimed at experts ( Rowley , 1982 ) .
OUT $ -1 $ 180 $ 143 $ In addition , by no means is the UNL system committed to event replication as it is the case of human translation .
Generation_and_linguistic_representation $ 4 $ 105 $ 33 $ Our device for this is a construction/TERM SYNC/TERM which pairs/DEF a description of a gesture G with the syntactic structure of a spoken constituent c :/O SYNC ( 2 ) G C The temporal interpretation of ( 2 ) mirrors the rules for surface synchrony between speech and gesture presented in ( Cassell et al. , 1994 ) .
Abstract $ 0 $ 61 $ 0 $ scription as a set of constraints -each constraint/TERM is an/DEF atomic formula with free variables that specifies the requirement that some lexical meaning contributes to the description ;/O the variables/TERM are placeholders/DEF for the discourse entities that the description identifies ./O
Evaluation $ 4 $ 135 $ 31 $ As Levin does not classify verbs on basis of their sentential complement-taking properties , more classification work is required before we can obtain accurate SCF estimates for this type of verb .
Related_Work $ 5 $ 135 $ 17 $ Reductions in perplexity relative to a bigrmn model were 10.5 % for the entire text and 33.5 % for the target vocabulary .
Interaction_Grammars $ 3 $ 55 $ 2 $ Context-free grammars and choice trees Let's consider the following context-free grammar for describing simple "addresses" in English such as "Paris , France" : s address --> city , " ," , country.
Using_CST_for_information_fusion $ 5 $ 152 $ 15 $ A graph-based/TERM operator/TERM defines/DEF a transformation on a multi-document graph ( MDG ) G which preserves some of its properties while reducing the number of nodes ./O
_KNOWLEDGE_EXTRACTION $ 3 $ 102 $ 44 $ However such a method only identifies the organizations of the known organization types and provides new proper names only .
The_TransType_model $ 2 $ 77 $ 62 $ The passive/TERM vocabulary/TERM is a/DEF large dictionary containing over 380 ,000 word forms ./O
BOV_Stem_NE_Defaults_Qspecific_41 $ 5 $ 53 $ 23 $ The first of these is the " bag/TERM of/TERM verbs/TERM " ( BOV/ACR ) technique .
ALLiS $ 2 $ 5 $ 0 $ ALLiS/TERM (/TERM Architecture/TERM for/TERM Learning/TERM Linguistic/TERM Structure/TERM )/TERM ( D6jean , 2000a ) , ( D6jean , 2000b ) is a/DEF symbolic machine learning system ./O
Introduction $ 1 $ 19 $ 8 $ The user's/TERM query/TERM is a/DEF formal statement of user's information need ./O
_Background $ 2 $ 35 $ 19 $ The structure of Mandarin ( base ) syllables is ( CG ) V ( X ) , where ( CG/TERM ) the/DEF syllable onset C/TERM the/DEF initial consonant ,/O G/TERM is the/DEF optional medial glide ,/O V/TERM is the/DEF nuclear vowel ,/O and X/TERM is the/DEF coda ( which may be a glide , alveolar nasal or velar nasal ) ./O
Introduction $ 1 $ 10 $ 4 $ noun phrases that do not contain other noun phrase descendants ( Church , 1988 ) .
Introduction $ 1 $ 11 $ 5 $ PbA requires a dictionary in which text and phonemics have been aligned , so that pronunciations corresponding to matching orthographic substrings can be identified .
Abstract $ 0 $ 7 $ 2 $ The individual FB1 scores for NPs were 92 .19% , VPs 92 .70% and PPs 96 .69% .
Maximum_Entropy_models $ 2 $ 13 $ 9 $ e~ i Aifi ( h , w ) P ( wlh ) = Z ( h ) where fi/TERM (/TERM h/TERM ,/TERM w/TERM )/TERM refers to a/DEF ( binary valued ) feature function that describes a certain event ;/O Ai/TERM is a/DEF parameter that indicates how important feature fi is for the model and/O Z/TERM (/TERM h/TERM )/TERM is a/DEF normalisation factor ./O
Evaluation_Measures $ 2 $ 99 $ 74 $ These inherent weaknesses in recall-based measures will be further explored in Section 4 .
Statistical_Semantic_Parsing $ 4 $ 101 $ 2 $ A parse/TERM state/TERM consists of a/DEF stack of lexicalized predicates and a list of words from the input sentence ./O
Dialog_Management $ 3 $ 61 $ 0 $ The Dialog/TERM Manager/TERM can be broadly classified into two main modules : Content/DEF Selection and Content Realization ./O
Abstract $ 0 $ 38 $ 11 $ The two-and-a-half-year VIRPI project consists of three parts .
The_MATE_Approach $ 2 $ 66 $ 30 $ Building on this specification , the MATE markup framework and the selected coding schemes , a java-based workbench has been implemented ( Isard et al. , 2000 ) which includes the following major functionalities : The MATE best practice coding modules are included as working examples of the state of the art .
OUT $ -1 $ 158 $ 116 $ Two measures that can be used in generating confidence scores are proposed in this section .
Topic_Analysis $ 4 $ 109 $ 0 $ In segmentation , we first identify candidates for points of segmentation within the given text .
_General_Outline_of_the_Method $ 2 $ 35 $ 19 $ IG-Tree/TERM is a/DEF compressed representation of the training set that can be processed quickly in classification process ./O
Implementing_PbA $ 3 $ 52 $ 11 $ An arc is placed from node i to node j if there is a matched substring starting with Li and ending with Lj .
Shortcomings $ 5 $ 149 $ 10 $ frequency/TERM of/TERM answers/TERM : The/DEF frequency of occurrence of facts in a collection of documents has an impact on the performance of systems ./O
Abstract $ 0 $ 19 $ 15 $ A more error-driven approach is the use of hybrid language models , in which some detection mechanism ( e . g .
Introduction $ 1 $ 13 $ 4 $ UCE/TERM filtering/TERM is a/DEF text categorization task ./O
Out-of-Vocabulary_Words $ 4 $ 140 $ 32 $ He presents a demonstration of the magnitude of the OOV problem for a wide range of multilingual natural language corpora and shows that some tasks can require vocabularies larger than 100,000 words to reduce the OOV rate below 1 % .
Abstract $ 0 $ 8 $ 2 $ idf , but the fact that there are so many variants of this formula in the literature suggests that there remains considerable uncertainty about these assumptions .
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 131 $ 96 $ ( 7 ) denotes that t frequently appears in the particular paragraph pj rather than the document di which includes pj.
Complexity_Formulas $ 4 $ 68 $ 12 $ The object complexity of object j is the sum of all its attributes' complexities : OC°bj$ "~E ACatt~ ,obji i A simple sum is used because identifying one object uniquely corresponds to knowing each of its attributes .
Conclusions $ 6 $ 137 $ 0 $ The use of error-correcting/TERM output/TERM codes/TERM ( ECOC/ACR ) for representing natural language classes has been empirically validated for a suite of linguistic tasks .
Discussion $ 5 $ 203 $ 27 $ ) Presumably , the quality of most generation systems can only be assessed at a system level in a task/Oriented setting ( rather than by taking quantitative measures or by asking humans for quality assessments ) .
Stochastic_Machine_Translation $ 2 $ 46 $ 4 $ The target string IfV~ is then chosen from all possible reorderings 2 of I?VT = argmax P ( Ws , WT ) ( 2 ) WT [TV~ = arg max P ( I~VT I AT ) ( 3 ) WTE~W T where AT/TERM is the/DEF target language model and/O AWT/TERM are the/DEF different reorderings of WT ./O
Dialogue_manager $ 6 $ 143 $ 35 $ In this dialogue , the user utterance U2 is a modification of U1 , indicating "How can I reply emails by Mew?".
_The_Problem $ 1 $ 64 $ 58 $ We have the option to view ( i ) as a composite object with a first class object representing each of its variable bindings in its own right , as in ( iii ) which is the unreduced binding of the amount of money to the amount variable of the object we named Cap-1 in ( i ) .
Learning_RRE_Rules $ 4 $ 74 $ 21 $ The root node corresponds to the null RRE , and so the position/TERM set/TERM consists of the/DEF beginning of each string in the training set ./O
Abstract $ 0 $ 29 $ 4 $ Another cause of error is the lack of information about abstract nominals .
Abstract $ 0 $ 4 $ 2 $ We find that GA feature selection always significantly outperforms the MBLP variant without selection and that feature ordering and weighting with CA significantly outperforms a situation where no weighting is used .
Comparison_experiment $ 3 $ 170 $ 72 $ For each question k we obtained three sets VKm .k , VKXS , k and VKCS , k of ( pos , assessment ) pairs corresponding to the three search methods , where pos/TERM is the/DEF position • of the document in the ordered list returned by the search method ,/O and assessment/TERM is the/DEF assessment of one participant ./O
The_Generator_in_an_Applied $ 3 $ 121 $ 5 $ The analysis component contains a morphological analyser , and it is the base forms of sit is likely that a modest increase in speed could be obtained by specifying optimisation levels in Flex and gcc that are higher than the defaults .
Results $ 3 $ 62 $ 8 $ For NP , the basic and reverse model produce accuracies which can compete with the highest published non-combination accuracies so far.
Abstract $ 0 $ 3 $ 1 $ On a new data set we have constructed for the task , while we were disappointed not to find parsing improvement over a traditional parsing model , our model achieves a recall of 84 . 0 % and a precision of 67 . 3 % of exact synset matches on our test corpus , where the gold standard has a reported inter-annotator agreement of 78 . 6 % .
Construction_of_Features $ 3 $ 87 $ 19 $ 166 words in a sentence , and posi_v/TERM represents the/DEF region in which a word lies ./O
Resolution_Procedures $ 2 $ 71 $ 40 $ If it is a definite noun phrase , then the head noun string is matched against that of previous NPs.
_Annotation_Guidelines_I : $ 3 $ 97 $ 51 $ both Nd and Ne are elaboration of N ) , then the basic category is the default inheritance category for the mother .
_Annotation_Guidelines_I : $ 3 $ 63 $ 17 $ GP/TERM : A/DEF GP/ is a phrase headed by locational noun or locational adjunct ./O
Approach $ 2 $ 39 $ 33 $ The first is the singlepass method .
Global_View_on_the_DE $ 2 $ 20 $ 3 $ The generation/TERM process/TERM consists/DEF of a series of structure mappings between adjacent strata until the SMorph stratum is reached ./O
Implementation $ 3 $ 131 $ 13 $ In table 2 are some example inputs and outputs , a 1/TERM represents activation/DEF on an input or output node ./O
Abstract $ 0 $ 147 $ 4 $ But if computational modeling is going to eventually lay claim to a model which accurately mirrors the human process of language acquisition , years of fine grinding are necessary .
Learning_RRE_Rules $ 4 $ 67 $ 14 $ Say we have a training corpus C . For every string C[j]~ C , Tmth[C[j]] ~ {0 ,1 } is the true label of C[j] and Guess[C[j]] is the current guess of the label of C[j] .
OUT $ -1 $ 72 $ 46 $ This is particularly worrisome in our case , since in our evaluation scenario the generated sentence is a permutation of the tokens in the reference string .
Abstract $ 0 $ 4 $ 2 $ Given an argument generated by the system and an interpretation of a user ' s rejoinder , the generation of the rebuttal takes into account the intended effect of the user ' s rejoinder , determined on a model of the user ' s beliefs , and its actual effect , determined on a model of the system ' s beliefs .
Previous_work $ 2 $ 19 $ 15 $ Certain dialog contributions are explained by the speaker's rhetorical goals , rather than by task goals.
Introduction $ 1 $ 13 $ 7 $ Our CommandTalk dialogue system was designed for a highly specialized domain with little available data , so finding ways to build a robust system with * This research was supported by the Defense Advanced Research Projects Agency under Contract N66001-94-C-6046 with the Space and Naval Warfare Systems Center .
Genetic_Algorithms_for_Assigning $ 2 $ 61 $ 43 $ The different values for crossover ranged from 0.65 to 0.95 , in steps of 0.05 .
Related_Work $ 6 $ 174 $ 14 $ REA/TERM has a/DEF working implementation , which includes the modules described in this paper , and can engage in a variety of interactions including that in ( 5 ) ./O
Conclusions $ 8 $ 250 $ 1 $ Topic/TERM analysis/TERM consists of two/DEF main tasks : text segmentation and topic identification ./O
Dialogue_examples $ 4 $ 207 $ 0 $ The example represents a dialogue where the computer plays A ' s role and is implementing the tactic of enticement .
The_Verbmobil_treebanks $ 2 $ 37 $ 3 $ The corpus consists of spoken texts restricted to the domain of arrangement of business appointments.
Vector_Space_Model : _Western_and_Asian $ 4 $ 135 $ 4 $ The vector/TERM simply consists/DEF of an ordered list of terms ,/O and therefore , the contextual cues have also disappeared .
Stochastic_Surface_Realization $ 2 $ 75 $ 0 $ 2.2 Generating Utterances The input to NLG from the dialogue manager is a frame of attribute-value pairs .
Comparative_analysis_~of_Japanese_and $ 3 $ 102 $ 14 $ However , in Chinese , gl~/TERM represents postal/DEF stamp and/O the constituent/TERM characters/TERM represent "postal"/DEF and "ticket" , respectively ./O
Learning_Verb_Rules $ 3 $ 63 $ 14 $ In our example it is the verb zdSastnit that is removed.
_Our_Classification_Algorithm_for $ 2 $ 85 $ 21 $ The following is the set of collocational features .
Abstract $ 0 $ 38 $ 7 $ ( 1998 ) describes a method based on text coherence which models text in terms of macro-level relations between clauses or sentences to help determine the overall argumentative structure of the text .
Introduction $ 1 $ 11 $ 4 $ While extrinsic evaluation measures such as these are often very Concrete , the act of designing the task and scoring the results of the task introduces bias and subject-based variability.
Introduction $ 1 $ 4 $ 0 $ GTAG/TERM is a/DEF multilingual text generation formalism derived from the Tree Adjoining Grammar model ( ( Joshi/O and al. , 1975 ) , ( Shabes and Shieber , 1994 ) ) .
Corpus_comparison_based_on $ 6 $ 156 $ 47 $ On the other hand , as the coverage for class features pairs increases , so does the part of the test set that is covered with the given feature set .
Discussion $ 6 $ 205 $ 27 $ What is the net worth of Bill Gates?
Introduction $ 1 $ 15 $ 8 $ An important component of the view developed is the observation that most methods use the same simple knowledge representation .
Levels_of_Annotation $ 4 $ 104 $ 10 $ Complex schemes like ToBI could be used in this way , or simpler schemes providing labels to distinguish types of accents , associated with words , and types of intonation boundaries .
POS_Assignment $ 3 $ 79 $ 19 $ In our implementation , we limited the values of Cat to Noun , Verb and Adjective , since they are the main open class categories and therefore the POSes of most new words .
Proposed_Architecture $ 1 $ 42 $ 20 $ Because this is a specific lessons learned about the CyberTrans experience , it is beyond the scope of this paper to compare this architecture with other architectures.
Introduction $ 1 $ 28 $ 18 $ Based on the assumption that most concepts and conceptual structures of the domain as well the company terminology are described in documents , applying knowledge acquisition from text for ontology design seems to be promising .
The_Interactional_Framework $ 2 $ 22 $ 2 $ The/TERM preferences/TERM of/TERM an/TERM agent/TERM are expressed as functions/DEF which map states , represented as sets of attribute-value pairs , to real numbers ;/O an overall utility function , which consists of the weighted sum of the individual functions , expresses the utility of reaching the state depicted by a certain configuration of attributes , according to the results of the multi-attribute utility theory ( Haddawy and Hanks , 1998 ) .
Introduction $ 1 $ 14 $ 6 $ In other words , they provide the/DEF conditional probability of a word given with the previous word sequence ,/O P/TERM (/TERM wilw~-l/TERM )/TERM , which shows the prediction of a word in a given context .
Introduction $ 1 $ 12 $ 7 $ A keyword extraction method ( e . g . , that using tf-idf ( Salton and Yang , 1973 ) ) generally extracts from a text key words which represent topics within the text , but it does not conduct segmentation .
Results_and_Discussion $ 5 $ 160 $ 29 $ one is the direct hypernym of the other ) .
Corpus_comparison_based_on $ 6 $ 150 $ 41 $ These values are calculated for pairs of an NE class and features , and averaged for the n-fold experiments .
Verb_Frequency $ 2 $ 46 $ 1 $ We would expect factors such as corpus genre ( Business for WSJ vs. mixed for BNC and Brown ) , American vs. British English , and the era the corpus sample was taken in to influence word frequency.
Abstract $ 0 $ 4 $ 0 $ Word order and accent placement are the primary linguistic means to indicate focus/background structures in German .
Abstract $ 0 $ 15 $ 14 $ The size of a topic continuity is empirically determined to be about 50 words with some variation .
Feature_Selection_and_Extraction $ 2 $ 25 $ 3 $ As a consequence , word segmentation is a major issue in Chinese document processing.
Abstract $ 0 $ 3 $ 1 $ Conventional/DEF parsing techniques based on Machine Learning framework ,/O such as Decision/TERM Trees/TERM and Maximum/TERM Entropy/TERM Models/TERM , have difficulty in selecting useful features as well as finding appropriate combination of selected features.
Tree_Generalization_using_Tree-cut $ 2 $ 48 $ 2 $ A thesaurus/TERM tree/TERM is a/DEF hierarchically organized lexicon where leaf nodes encode lexical data 21 ( i.e. , words ) and internal nodes represent abstract semantic classes ./O
Abstract $ 0 $ 6 $ 2 $ Content-based/TERM measures/TERM increase/DEF the correlation of rankings induced by synonymous ground truths , and exhibit other desirable properties ./O
OUT $ -1 $ 6 $ 6 $ In this way , a PAR schema for the action enter may actually translate into an animation PAR for walking into a certain area.
_The_straightforward_unpacking_of_feature $ 3 $ 18 $ 10 $ The/DEF most basic metric for patterns with symbolic features is/O the Overlap/TERM metric/TERM given in equation 1; where A ( X , Y ) is the distance between patterns X and Y , represented by n features , wi is a weight for feature i , and 5 is the distance per feature.
Abstract $ 0 $ 39 $ 38 $ To the author 's knowledge , this parser is one of the largest scale Chinese parser ever implemented in the world .
Tree_Generalization_using_Tree-cut $ 2 $ 79 $ 33 $ The best model is the one with the tree-cut [AIRCRAFT , ball , kite , puzzle] indicated by a thick curve in the figure .
_Annotation_Guidelines_II : $ 4 $ 121 $ 2 $ In other words , the only empirical evidence for the existence of a thematic relation is a realized argument.
Building_Spoken_Dialo~te_Systems $ 4 $ 115 $ 10 $ ( l-gatsu ichigatsu month nil i ) The first three elements are the identifier , the pronunciation , and the grammatical category of the word .
Related_Research_and_Motivation $ 1 $ 22 $ 2 $ From the un-delimited sequence of characters , words must be exlIacted first ( this process is known as segmentation ) .
Experimental_Setup $ 5 $ 116 $ 3 $ After replacing the words with part-of-speech tags , the vocabulary size of the corpus is reduced to 47 tags .
Abstract $ 0 $ 131 $ 36 $ Pronouns and definite NPs , for example , typically refer to given entities , and therefore are compatible with the grammatical relation ST.
The_MATE_Markup_Framework $ 3 $ 88 $ 0 $ The MATE/TERM markup/TERM framework/TERM is a/DEF conceptual model which basically prescribes ( i ) how files are structured , for instance to enable multi-level annotation , ( ii ) how tag sets arc ; represented in terms of elements and attributes , and ( iii ) how to provide essential information on markup , semantics , coding purpose etc ./O
OUT $ -1 $ 238 $ 48 $ The user just selects the utterance to nark up and then clicks on the violation type palette , or , in case it is a new type , clicks on the violated cooperafivity guideline which means that a new violation type is added and text can be entered to describe it , el .
Introduction $ 1 $ 11 $ 2 $ SGML is well established as the coding scheme underlying most Translation/TERM Memory/TERM based/TERM systems/TERM ( TMBS/ACR ) , and has been proposed as the cod-it~g scheme for the interchange of existing Translation Memory databases Translation Meinories eXchange , TMX ( Melby , 1998 ) .
Task-inherent_and_Technical $ 2 $ 40 $ 11 $ For example , there is a separate module on the level of speech recognition which deals with hesitations and self-corrections .
Global_View_on_the_DE $ 2 $ 83 $ 66 $ For illustration , one of the rules , namely date , has been selected for application : tile highlighted arcs and nodes of tile input structure are the part to which date is applicable.
Tree_Generalization_using_Tree-cut $ 2 $ 59 $ 13 $ Therefore , in general , m when clusters C1 . . Cm are merged and generalized to C according to the thesaurus tree , the estimation of a probability model becomes less accurate .
Comparing_Taggers $ 1 $ 10 $ 3 $ POS/TERM tagging/TERM is a/DEF useful first step in text analysis , but also a prototypical benchmark task for the type of disambiguation problems which is paramount in natural language processing : assigning one of a set of possible labels to a linguistic object given different information sources derived from the linguistic context ./O
Experimental_Setting $ 4 $ 110 $ 9 $ The corpus consists of 168 parallel news ( i.e.
Experiment $ 3 $ 110 $ 28 $ As expected , when one computes the recall and precision figures with respect to the nuclearity and relation assignments , one also factors in the nuclearity status and the rhetorical relation that is associated with each span .
The_interplay_of_focus_and_word $ 2 $ 26 $ 2 $ has either been selected from a set of alternative beliefs ascribed t . o the listener , or it is a revision of certain beliefs ( in case of contrastive focus ) , or the focused phrase expresses ' new ' information the listener does not know or is not able to infer from his beliefs [Halliday , 1967] .
OUT $ -1 $ 166 $ 112 $ Concerning answers , the principal measures for the AE task must be recall and precision , applied to individual answer sentences .
_NTCIR_Data_Analysis $ 2 $ 121 $ 51 $ If the phrasal term AB has a high MI ( AB ,rel ) value in contrast with MI ( A , rel ) and MI ( B ,rel ) , this is the ease where phrasal terms are effective.
Context_Distributions $ 3 $ 22 $ 1 $ that similar words occur in similar contexts , I formalise this in a slightly different way : each word defines a probability/TERM distribution/TERM over/TERM all/TERM contexts/TERM , namely the/DEF probability of the context given the word ./O
OUT $ -1 $ 126 $ 0 $ 4.2 Active Learning To demonstrate the usefulness of obtaining probabilities from a transformation rule list , this section describes an application which utilizes these probabilities , and compare the resulting performance of the system with that achieved by C4.5.
Introduction $ 1 $ 32 $ 19 $ Attributes in their system assist the realization by propagating information down a tree that specifies the complete syntactic structure of the output text .
Introduction $ 1 $ 14 $ 10 $ ( 2 ) where p/TERM ( w[hi ) is a/DEF language model ,/O p/TERM ( wli , s ) is a/DEF translation model ,/O and A/TERM E/TERM [0 , 1] is a/DEF combining weight ./O
Extending_a_Grammar_to_Enable $ 4 $ 84 $ 11 $ The top text in a rectangle specifies a slot name , and the bottom text is the name of a template ~kssigned to this slot , .
Task_Structure_and_Scripts $ 3 $ 94 $ 49 $ AI : U2 : A3 : U4 : A5 : A6 : U7 : A8 : U9 : A10 : Ul1 : A12 : U13 : A14 : A15 : U16 : A17 : U18 : AI9 : Hello .
Maximum_entropy-based_parse $ 2 $ 38 $ 9 $ In ideal circumstances , where the distribution of features in the training data accurately represents the true probability of the features , the performance of the model should increase asymptotically with each iteration of training until it eventually converges .
Utterance-Level_Robustness $ 4 $ 50 $ 7 $ We collected four measures of performance : • Recognition time , measured , in multiples of CPU/DEF real/DEF time/DEF ( CPURT/TERM ) .
Conceptualizing_Events $ 2 $ 149 $ 119 $ ( Even the last MOVE of a sequence of MOVE events contains a STOP event , because aircraft stop at the beginning of the runway , which is the last event of the taxiing , before they commence the takeoff.
Results $ 8 $ 178 $ 17 $ These results seem to indicate that atelicity is a fairly good cue for present tense.
Results $ 3 $ 76 $ 24 $ The rightmost column gives the total number of values times the number of classes .
Abstract $ 0 $ 1 $ 0 $ Word/TERM Sense/TERM Disambiguation/TERM (/TERM WSD/TERM )/TERM is a/DEF central task in the area of Natural Language Processing ./O
Reading_Comprehension_Tests $ 2 $ 47 $ 29 $ A mummy/TERM is a/DEF body wrapped in sheets ./O
Conclusion $ 5 $ 194 $ 10 $ The last phase of the project will deal with the real use of the N-V pairs obtained by the machine learning method within one information retrieval system and the evaluation of the improvement of its performances .
Multilinguality $ 5 $ 180 $ 0 $ The generation of dialogue scripts and result summaries is fully implemented in VERB~VIoBIL for German and English .
OUT $ -1 $ 62 $ 9 $ The/DEF problem of identifying the words string in a character sequence is/O known as the segmentation/TERM / tokenization/TERM problem/TERM .
Evaluation_Measures $ 2 $ 89 $ 64 $ As such , these measures produce different scores , but the same ranking of all the K-sentence extracts from the document.
Introduction $ 1 $ 4 $ 0 $ Semantically annotated linguistic data are important resources for natural language processing , and have been used in many NLP areas , e . g . , parsing , word sense disambiguation , co-reference resolution and information extraction , etc .
Tree_Structures $ 2 $ 29 $ 13 $ The lower part represents the phonological phrases into which the whole sentence is divided by the binary structure , and uses the same representation levels as in the syntactic structure .
Whole_Sentence_Maximum $ 2 $ 22 $ 2 $ The probability/TERM distribution/TERM is the distribution/DEF p/DEF that/DEF has/DEF the/DEF maximum/DEF entropy/DEF relative/DEF to/DEF a/DEF prior/DEF distribution/DEF P0/DEF (/O in other words : the distribution that minimize de divergence D ( pllpo ) ) ( Della Pietra et al. , 1995 ) .
Evaluation $ 4 $ 83 $ 7 $ The best heuristic according to 145 the precision is the maximum similarity heuristic .
Related_Work $ 6 $ 161 $ 21 $ Finite/TERM mixture/TERM models/TERM have been used/DEF in a variety of applications in text processing (/O e.g. , ( Li and Yamanishi , 1997; Nigam et al. , 2000; Hofmann , 1999 ) ) , indicating that they are essential to text processing.
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 78 $ 43 $ In Document level , there is a small number of sample news documents about the same topic .
_IBM_expected_[SBAa_each_employee_to $ 5 $ 46 $ 34 $ Here are the first few elements generated by the model for the tree of Figure 1 : 1.
Surface_realization_as_grammatical $ 5 $ 142 $ 0 $ competition The resulting input for grammatical competition is a blend of semantic and pragmatic information .
Tree_Structures $ 2 $ 35 $ 19 $ B1 , B2 and B3 are the break-related nodes .
Surface_realization_as_grammatical $ 5 $ 145 $ 3 $ While tile/TERM realization/TERM of/TERM the/TERM focus/TERM domain/TERM is the/DEF task of converting the complete focus into one phrase ,/O word order will be determined by LP-rules that pick up the pragmaticall2 , ' motivated literals on topichood , identifial ) ility , and referential movement .
Evaluation $ 3 $ 120 $ 29 $ MLE thresholding produced better results than the two statistical tests used .
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 177 $ 142 $ Let $1 : -' , S , , be all the other training documents ( where m/TERM is the/DEF number of training documents which does not belong to the target event )/O and Sx/TERM be a/DEF test document which should be classified as to whether or not it discusses the target event ./O
EXOT $ 9 $ 81 $ 9 $ One document `` Barbie/TERM '' in the Jang ( 1997 ) collection has a/DEF total of 1 ,468 words comprised of 755 content words and 713 function words ./O
_Features $ 1 $ 150 $ 79 $ Figure 7 shows that if there are hidden words , the language model accuracy dropped from 60 % to 25 % .
OUT $ -1 $ 137 $ 95 $ The following describes the active learning algorithm used in the experiments : TBLDT ) to obtain chunk probabilities on the rest of the training data ; ing set , specifically the samples that optimize an evaluation function f , based on the class distribution probability of each sample ; fication 3 to the training pool and retrain the system ; 5 .
Hyperonymy_in_lexical_semantics $ 5 $ 139 $ 42 $ This divergence points to the need for a distinction between conceptuaJ , and lexicalg : ranularitv and inheritance : The WordNet chain represents rather a series of concepts than of words entering the lexical choice process , which appears to be better represented by a Cruse-type chain with few designated levels ( but needs to be augmented with near-synonyms for tile 'horing , . . . . . . . . . . . . . . ffntity creature . . . . `` . . . . . . . . li~q form animal , beast , . . . . . . . . animal c > rdate ve~ebrate ~ammal p~cental c~71ivore 7~_1 ine dog J / .
OUT $ -1 $ 102 $ 60 $ The chunk tags are derived from the parse tree constituents , and the part/Of-speech tags were generated by the Brill tagger ( Brill , 1995 ) .
Abstract $ 0 $ 12 $ 9 $ The approach taken in this thesis , however , explores generation/TERM as .a .classification/DEF task whereby the representation that describes the intended meaning of the utterance is ultimately to be classified into an appropriate surface form ./O
_Department_of_State ,_State_Department ,_State $ 5 $ 133 $ 1 $ We will pay special attention to localcontent/TERM collocations/TERM , as they are the/DEF strongest , and also closer to strict definitions of collocation ./O
Interclausal_Coherence $ 4 $ 128 $ 38 $ The Panasonic/TERM LC90S/TERM is a/DEF 19"-display ./O
Abstract $ 0 $ 128 $ 4 $ These studies frequently involve the construction of an idealized/TERM language/TERM sample/TERM which is ( at best ) an/DEF accurate subset of sentences that a child might hear ./O
Future_Research $ 4 $ 134 $ 3 $ The result is a set of tasks for which the system 's output appears to be suitable .
_Our_Classification_Algorithm_for $ 2 $ 64 $ 0 $ Verbg Situation 2 .1 Guiding Thoughts ( 1 ) Our algorithm is for information processing eMa[5] uses three pairs of phase features in classifying , but from which we can not get an automatic classification algorithm for computers; the classification can only be done manually .
MALIN $ 4 $ 112 $ 6 $ XMALIN/TERM (/TERM Multi-modal/TERM Application/TERM of/TERM LINLIN/TERM )/TERM is a/DEF refinement of the LINLINsystem ( Ahrenberg et al . , 1990 ; JSnsson , 1997 ) to handle also multi-modal interaction and more advanced applications ./O
_Modeling $ 4 $ 111 $ 1 $ We have instead provided a prose description of the process for a very few examples and many questions of just what constitutes a displacement or how one might know that a relation reached in the traversal Should be unpacked remain unanswered .
Abstract $ 0 $ 7 $ 3 $ The paper also describes the interactions between the dialogue component and the other servers of the system , mediated via a central hub.
Experiments $ 4 $ 124 $ 3 $ verbs , and overall , are the best results for each case are printed in boldface.
Algorithms_and_Implementation $ 2 $ 26 $ 6 $ The base/TERM model/TERM defines the/DEF distance between a test item and each memory item as the number of features for which they have a different value ./O
The_model $ 1 $ 29 $ 6 $ The hotel/TERM Regina/TERM is a/DEF small hotel ./O
Implementation $ 4 $ 339 $ 168 $ Among the most important wins over the traditional " piping " approach to filter assembly is the ability to impose build-time restrictions on the component assembly , disallowing " illegal " compositions .
Abstract $ 0 $ 32 $ 2 $ Since a primary goal of annotated corpora is to serve as the empirical base of linguistic investigations , it is desirable to annotate structure divisions that are the most commonly shared among theories .
Abstract $ 0 $ 36 $ 34 $ P ( t l ) and P/TERM ( t 2 ) are the/DEF occurrence probabilities of term t I and t 2 in a sentence ./O
Grammar_Induction $ 3 $ 61 $ 0 $ The degree of difficulty of the task of learning a grammar from data depends on the quantity and quality of the training supervision .
Abstract $ 0 $ 20 $ 16 $ perplexity measures [Keene and O ' Kane , 1996] or topic detection [Mahajan et al . , 1999] ) selects or combines with a more appropriate language model .
_Otherwise ,_add_to_the_current_context_new $ 6 $ 156 $ 73 $ Still , the probability of node 1 is quite low ( i.e. , there is a high belief in its negation ) .
Interaction_Grammars $ 3 $ 104 $ 51 $ The first rule is then read : "if C is a tree of type city , and Co a tree of type country , then adr ( Co ,C ) is a tree of type address" , and similarly for the remaining rules.
_STRUCTURES_OF_ORGANIZATION $ 2 $ 45 $ 4 $ If it is a company , to be more informative the line of business usually goes with the key word '~] company' , for instances '~ff~.t..~J food company' , '~ ~ computer company' , ' ~M ~='~ ~ investment consultant company' , but in most cases the keyword '~B] company' will be ignored , such as ~--~ ( President food ) .
Introduction $ 1 $ 8 $ 2 $ In fact , the criteria regarding language style may differ for each search and therefore due to the large number of texts there is a requirement to perform style categorisation in an automated manner .
OUT $ -1 $ 100 $ 58 $ The data used in all of these experiments is the CoNLL-2000 phrase chunking corpus ( CoNLL , 2000 ) .
Approach_for_Chunk_Identification $ 3 $ 28 $ 1 $ Each chunk type belongs to I or B tags .
Integration_Process $ 4 $ 92 $ 4 $ SURGE/TERM ( Elhadad and Robin , 1996 ) is a/DEF comprehensive English Grammar written in FUF ./O
The_Complexity_of_Extracting_a $ 2 $ 60 $ 48 $ • In S , the date of the murder of the two employees is an example of a level-2 fact .
Introduction $ 1 $ 26 $ 18 $ 9 Are the scientific statements expressed in this sentence attributed to the authors , the general field , or specific other n work / Other Work Does this sentence contain material that describes the specific aim of the paper?
Abstract $ 0 $ 38 $ 36 $ P ( tl ) = n ,__~_ ( 2 ) N P ( t2 ) = n ,2 ( 3 ) N P ( tl , t2 ) = n , , , ,~ ( 4 ) N Where nt~/TERM , nt2/TERM is the/DEF individual term frequency of term t I and t 2 respectively if either of them occur in a sentence of the collection ,/O ntt/TERM is the/DEF co-occurrence frequency of term t I and t 2 if they are all in a sentence of the collection ./O
How_to_generate_technical $ 2 $ 89 $ 38 $ o The lexicalized/TERM grammar/TERM in G-TAG is compiled/DEF from the recta-grammar designed and implemented by M.H ./O
OUT $ -1 $ 59 $ 17 $ Let R/TERM (/TERM z/TERM )/TERM to be the/DEF set of rules r that applies to the state el ( z ) ,/O R ( z ) = {ri ~ 7~Ir~ applies to si ( z ) } An equivalence/TERM class/TERM consists/DEF of all the samples z that have the same R ( z ) ./O
OUT $ -1 $ 146 $ 95 $ What we need is a combination that is coherent enough for people to understand.
_Pronominalise_the_Cb_only_after_a_Continue $ 4 $ 163 $ 124 $ Of course we are not satisfied that this metric is the best ; an advantage of the generation approach is that different evaluation methods can easily be compared .
Introduction $ 1 $ 60 $ 52 $ The parent class is a hypernym of its children classes .
Results $ 3 $ 78 $ 24 $ The next most affected phrase type is the ADJP , which can often be joined with or removed from the NP .
Discussions $ 6 $ 79 $ 10 $ We can see that the redundant ratio obviously decreases by using the revisional distance , and the result that has the lowest redundant ratio corresponds of the minimum value of the objective function .
Introduction $ 1 $ 11 $ 7 $ The objective of standardizing efforts in discourse is to promote interactions among discourse researchers and thereby provide a solid foundation for corpus-based discourse research , dispensing with duplicating resource making efforts and increasing sharable resources .
Introduction $ 1 $ 13 $ 5 $ This corpus was derived from the Singapore Primary School Chinese Language Textbooks .
Statistical_Semantic_Parsing $ 4 $ 99 $ 0 $ 4.1 The Parsing ModeL A parser/TERM is a/DEF relation Parser C_ Sentences x Queries where Sentences and/O Queries/TERM are the/DEF sets of natural language sentences and database queries respectively ./O
Description_of_the_algorithm $ 2 $ 15 $ 1 $ First step is to make automatically labeling and to chunk each sentence from spoken dialog corpus into a set of short subphrases.
Introduction $ 1 $ 16 $ 5 $ A textual/TERM document/TERM is a/DEF sequence of terms ./O
Generation_of_Crisp_Descriptions $ 3 $ 79 $ 25 $ In the present context , however , the limitation is inessential , since what is crucial here is the interaction between an Adjective and a ( possibly complex ) Common Noun following it : in more complex constructs of the form ' NP and the Adj CN ' , only CN affects the meaning of Adj .
Content_Presentation $ 4 $ 98 $ 5 $ Instead other types require additional re-generation : for the topic of the document template the generation/TERM procedure/TERM is as follows : (/DEF i ) the verb form for the predicate in the Predicate slot is generated in the present tense ( topical information is always reported in present tense ) , 3rd person of singular in active voice at the beginning of the sentence ; ( ii ) the parsed sentence fragment from the N ' hat slot is generated in the middle of the sentence ( so the appropriate case for the first element ) ./O
Extending_a_Grammar_to_Enable $ 4 $ 97 $ 24 $ ( If there is neither possessor nor determiner then the grammar considers the np-type : if it is COMMON , it uses NO ( for indefinite ) and if it is PROPER , it uses NOART The number feature passed is the value passed from the determiner , if there is one , or the value from the current template .
_Preliminary_Evaluation $ 3 $ 80 $ 25 $ GIZA/TERM is an/DEF intermediate program in a statistical machine translation system ,/O EGYPT .
Learning_Algorithms_Tested $ 2 $ 72 $ 31 $ LazyBoosting/TERM ( Escudero et al. , 2000a ) , is a/DEF simple modification of the AdaBoost.MH algorithm , which consists of reducing the feature space that is explored when learning each weak classifier ./O
OUT $ -1 $ 130 $ 59 $ 27 z~ejm~ stay Sb , V ) zmi~nit Figure 4 : The DMCS of the sentence from Figure 2 .
OUT $ -1 $ 188 $ 62 $ This is a good performance for single features .
Introduction $ 1 $ 9 $ 0 $ Word/TERM Sense/TERM Disambiguation/TERM ( WSD/ACR ) is the/DEF problem of assigning the appropriate meaning ( sense ) to a given word in a text or discourse ./O
_The_identity_of_the_speaker ,_denoted_as_the $ 6 $ 129 $ 19 $ To that end , three different approaches were used : ( i ) the full/TERM model/TERM : all/DEF variables were used to determine the discriminant functions ;/O ( ii ) the forward/TERM model/TERM : starting/DEF from an empty model , variables were introduced in order to create a reduced model , with a small number of variables ;/O ( iii ) the backward/TERM model/TERM : starting/DEF from the full model , variables were eliminated to create a reduced model ./O
See_ ( Chu-Carroll_and_Carberry_1998 ) _tbr_an $ 4 $ 146 $ 37 $ Third , a measure of argument effectiveness can also be derived by explicitly questioning the user at the end of the interaction , about the rationale for her decision.
Statistical_Semantic_Parsing $ 4 $ 119 $ 20 $ The equation states that the probability that a given query is a correct meaning for I is the same as the probability that the final state ( reached by parsing l ) is a good state .
_KNOWLEDGE_EXTRACTION $ 3 $ 111 $ 53 $ The X/TERM is the/DEF initial two-characters of the keyword and/O Y/TERM is the/DEF remained characters ./O
Introduction $ 1 $ 29 $ 22 $ The interface is intended to be intuitive for users already familiar with keyword IR but adds the extra functionality of sortal , linguistic and positional constraints that are more common in IE .
Evaluation $ 3 $ 99 $ 8 $ Following Briscoe and Carroll ( 1997 ) , we calculated precision/TERM ( percentage/DEF of SCFS acquired which were also exemplified in the manual analysis )/O and recall/TERM ( percentage/DEF of the SCFs exemplified in the manual analysis which were acquired automatically )/O .
OUT $ -1 $ 154 $ 100 $ There is a number of important areas of research that ExtrAns and WebExtrAns , and by extension any AE system , has to focus on .
Abstract $ 0 $ 211 $ 45 $ " evaluate ( theatre=Ridge ) + requestValue ( time ) Usr : "I don't know .
Conclusion $ 4 $ 56 $ 2 $ We are at present checking that the behavior also holds for other quality measures as the precision and recall of parses of sentences that express strong equivalence between the model and the data .
Comparing_Three_Treebank $ 4 $ 70 $ 3 $ ( a ) Frequency ( b ) Coverage Figure 5 : Etree template types and template tokens in the Penn English Treebank ( X-axes : ( a ) and ( b ) template types Y-axes : ( a ) log frequency of templates ; ( b ) percentage of template token covered by template types ) from the three Treebanks .
Conclusion $ 5 $ 160 $ 5 $ The EMCL can be defined monolingually , multilinguality being obtained through NLG .
Analysing_Czech_texts $ 3 $ 102 $ 31 $ The automatically created ATS/TERM is a/DEF labelled oriented acyclic graph with a single root ( dependency tree ) ./O
Semantic_Lexicon $ 3 $ 50 $ 4 $ The semantic/TERM zone/TERM maps/DEF a sense into an ontological concept in the case of single sense , or to several concepts in the case of multiple senses ./O
Abstract $ 0 $ 154 $ 7 $ If a new unit contains no referential expression then the algorithm makes no prediction.
ADAM : _Architectural_Principles $ 3 $ 60 $ 37 $ According to our view , an annotation/TERM meta-scheme/TERM is a/DEF general descriptive framework in which different annotation schemes can be accommodated ./O
Classifiers $ 2 $ 174 $ 10 $ ( x _/~ , ) log l , I +2 log p ( w c ) g , ( x ) = ( x-lee ) r Z ,-~ ( x-/~ , ) log] Z~ ] +2log p ( w , ) Pc/TERM and ,ue/TERM are the/DEF mean vectors of the class wc and we ,/O respectively , C/TERM (/TERM Wc/TERM ,/TERM We/TERM )/TERM are the/DEF covariance matrices of the class wc and we ,/O respectively , and 1-I is the determinant .
Analysis_module $ 2 $ 42 $ 2 $ SUPAR/TERM is a/DEF computational system focused on anaphora resolution ./O
DTD_abstraction $ 2 $ 52 $ 0 $ SGML/TERM mark-up/TERM determines/DEF the logical structure of a document and its syntax in the form of a context-free grammar ./O
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 158 $ 123 $ In Table 1 , 'Event type' illustrates the target events defined by the TDT Pilot Study.
Complexity_Formulas $ 4 $ 70 $ 14 $ Since objects can be grouped together into classes , a class/TERM complexity/TERM is the/DEF number of bits conveyed by distinguishing one type of object from that class , plus the maximum object complexity that occurs in that class :/O CC. ,...
OUT $ -1 $ 183 $ 129 $ The ideal/TERM answer/TERM is a/DEF full sentence that contains the information given by the question and the information requested ./O
Feature_merging_and_overfitting_reduction $ 3 $ 57 $ 0 $ The idea behind feature/TERM merging/TERM is to/DEF reduce overfitting through changes made directly to the model ./O
Abstract $ 0 $ 17 $ 15 $ Another view of this is the DARPA Translingual/TERM Information/TERM Detection/TERM , Extraction/TERM and Summarisation/TERM effort ( TIDES/ACR ) .
Conclusion $ 6 $ 184 $ 17 $ For this DM input consists of grammatical structures , rather than sets of semantic objects .
Abstract $ 0 $ 161 $ 14 $ When clauses are considered to be the elementary unit size , the predictions are correct in up to 81 % of the cases in which the algorithm makes a prediction -under the pre-condition that intra-sentential units axe handled first .
Introduction $ 1 $ 9 $ 3 $ In English BNP/ACR ( base/TERM noun/TERM phrase/TERM ) is defined as simple/DEF and non-nesting noun phrases ,/O i.e.
Construction_of_Features $ 3 $ 84 $ 16 $ The position value posi_v of the ith word wi is calculated as pos _v = r × R] , where n/TERM is the/DEF number of words and/O R/TERM represents/DEF the number of regions in the sentence ./O
Experimental_Results $ 5 $ 151 $ 1 $ The first is the U.S. Geography domain.
Experimental_Results $ 5 $ 93 $ 19 $ Recall/TERM is the/DEV same as coverage : the ratio between the number of correct parses produced by the specialized grammar and the total number of correct parses ( equalling the total number of sentences in the test set ) ./O
ALLiS $ 3 $ 36 $ 16 $ The TTT formalism seems to be the most appropriate ( rules are easy to generate and the resulting parser is fast ) .
Introduction $ 1 $ 35 $ 22 $ 2 An Overview of YAG YAG/TERM (/TERM Yet/TERM Another/TERM Generator/TERM )/TERM ( Channarukul , 1999; McRoy et al. , 1999 ) is a/DEF template-based textrealization system that generates text in real-time ./O
Evaluation $ 4 $ 95 $ 7 $ But interestingly this practical ew~luation confirmed our theoretical evaluation that atranslation can be produced with TRANSTYPE by typing less than 40 % of the characters of a translation .
Learning_Frameworks $ 2 $ 31 $ 11 $ A different , distribution free inductive principle that is related to the pac model of learning is the basis for the account developed here .
Resolution_Procedures $ 2 $ 37 $ 6 $ A TMR/TERM includes/DEF , among other representational objects , instantiations of object types , relation types and property types ./O
OUT $ -1 $ 120 $ 18 $ MI/TERM ( Mutual/TERM Information/TERM ) is a/DEF measure of word association , and used under the assumption that a highly associated word n-gram is more likely to be a compound noun ./O
Experimental_results $ 3 $ 56 $ 2 $ Corpus/TERM A/TERM consists of local/DEF news with more than 325 million characters ./O
ALLiS $ 2 $ 26 $ 21 $ LT/TERM TTT/TERM is a/DEF good trade/Off between the rapidity of CASS and the rich formalism of XFST ./O
Abstract $ 0 $ 9 $ 8 $ We can confirm that a term is good/TERM to/TERM discriminate/TERM subject/TERM concepts/TERM if/DEF relevant documents contain such terms and non-relevant documents do not contain them and/O that a term is noisy/TERM if/DEF the situation is the opposite ./O
The_MATE_Markup_Framework $ 3 $ 177 $ 89 $ Example : The declaration Occursln : href ( lxanscription , u ) allows an attribute used as , e . g . , Occursln= ' ' base~_123 ' ' , where base is a coding file using the transcription module and u_123 is the value of the id attribute of a t~ element in that file .
The_Verbmobil_treebanks $ 2 $ 47 $ 13 $ A sample annotation conformant to the Verbmobil annotation scheme is the annotation of ( 1 ) shown in Fig .
Joining_TM2_and_DTD $ 3 $ 93 $ 14 $ The <rs>/TERM tag/TERM can be considered to be the/DEF name of the varying element ./O
System_Overview $ 3 $ 101 $ 65 $ Furthermore , Eset is the only tree set that satisfies all the following conditions : ( C1 ) Decomposition/TERM : The/DEF tree set is a decomposition of T* , that is , T* would be generated if the trees in the set were combined via the substitution and adjunction operations ./O
Abstract $ 0 $ 94 $ 1 $ The second formalism that we introduce is the multi-document graph .
Error-driven_Learning $ 4 $ 120 $ 3 $ the lexicon : F~ ( e i ) = F : rr°r ( e i ) o+Ao Here , F ,~ r~°r ( el ) is the chunking error number of the lexical entry e i for the old lexicon r~ Error / x and r~ ,+~ te i ) is the chunking error number of the lexical entry e i for the new lexicon + AO where e~ e A~ ( A~ is the list of new lexical entries added to the old lexicon ~ ) .
OUT $ -1 $ 76 $ 30 $ From these basic count measures , we define four derived percentage measures in section 5 and summarize these cases across our three systems in figure 3 of that section.
_The_identity_of_the_speaker ,_denoted_as_the $ 6 $ 116 $ 6 $ Corpus/TERM H/TERM is a/DEF subset of Corpus I ./O
Reinterpretation_of_CGS_in_RAGS $ 5 $ 109 $ 9 $ Text Planner The input to the Longbow text planner discussed in section 4 above is a representation of a picture in SAGE/TERM format/TERM ( which has been annotated/DEF to indicate the types of complexity of each grapheme )/O together with a goal/TERM , which can/DEF typically be interpreted as "describe" ./O
Conclusion $ 7 $ 200 $ 11 $ by referential means .
The_computation_of_the_velocity_is_easily ,_done_from $ 4 $ 171 $ 18 $ MOVE/TERM is a/DEF label for complex events that consists of maximally three sub-events , namely START , CHPOS ( CHANGE OF POSITION ) , and STOP , where the first and the last sub-event are optional and the middle event can be any kind of movement along a trajectory ./O
Qualitative_Evaluation_of_the $ 4 $ 171 $ 70 $ We therefore divide by this score to assure that a perfect sentence gets a score of 1 .
Abstract $ 0 $ 123 $ 28 $ For Encarta , the addition of [InformationStatus] yielded only a modest improvement in accuracy .
OUT $ -1 $ 0 $ 0 $ Multilingual Summary Generation in a Speech-To-Speech Translation System for Multilingual Dialogues* Jan Alexandersson , Peter Poller , Michael Kipp , Ralf Engel DFKI GmbH Stuhlsatzenhausweg 3 66123 Saarbrficken {alexanders son , poller , engel , kipp}@dfki , de
Abstract $ 0 $ 56 $ 52 $ We suspect that some of these errors may be systematic due to the properties of the language model used or due to language specific properties .
Abstract $ 0 $ 10 $ 8 $ As we begin the 21 ~ ' century , users of online materials are faced with having to process , utilise and exploit documents that may be in one of many languages or a combination of languages .
_NTCIR_Data_Analysis $ 2 $ 112 $ 42 $ This explains poor performance of precoordinated longer phrase based indexing that utilizes phrases as replacements of single words .
Translations_selection_and_phrase $ 2 $ 51 $ 1 $ This method , however , results in too many ambiguities during the query translation and offers no path to select appropriate ones among the translations .
Introduction $ 1 $ 10 $ 3 $ TIVIR/TERM captures/DEF the meanings of words in the text and represents them in a set of ontological concepts interconnected through ontological relations ./O
Introduction $ 1 $ 10 $ 2 $ Generally , dependency structure analysis consists of two steps.
OUT $ -1 $ 117 $ 75 $ Since C4.5 generates probabilities for each classification decision , they can be redirected into the input for the next position .
The_model $ 1 $ 93 $ 70 $ The output of NN 11 is now a linear" structure where we Iozou , the phrase types .
Conclusion $ 5 $ 140 $ 8 $ Second , LexTract can build derivation trees for each sentence in the corpora .
System_Overview $ 3 $ 104 $ 68 $ ( C3 ) Target/TERM grammar/TERM : Each/DEF tree in the set falls into one of the three types as specified in Section 3 . 1 ./O
Abstract $ 0 $ 4 $ 3 $ By coordinating thesaurus semantic/TERM categories/TERM (/TERM SEMCATs/ACR )/TERM of the long run words to the semantic categories of paragraphs , we conclude that for paragraphs containing both long runs and short runs , the SEMCAT weight of long runs of content words is a strong predictor of the semantic coherence of the paragraph .
Abstract $ 0 $ 103 $ 42 $ Schwarzschild's covering proposal and van den Berg's assignment-set proposal are perfectly compatible.
Corpus_comparison_based_on $ 6 $ 113 $ 4 $ When a feature F is given , the conditional/TERM entropy/TERM for NE classes H ( CIF ) is defined by p/DEF ( ~ , f ) logs p ( cll ) H ( C]F ) cEC fEF ./O
Abstract $ 0 $ 38 $ 5 $ Face to face with the reality of use this realization has been most widely accepted in areas of linguistics which deal with language acquisition and teaching .
OUT $ -1 $ 64 $ 39 $ `` +1/TERM '' means that the/DEF algorithm needs one bit to indicate whether the collocational relationship between the two clusters exists ./O
Experiments $ 6 $ 118 $ 23 $ * ~DT MIDDLE DOT IN ( 2 ) past ~ passed if ( ~to ) * NN MIDDLE The first rule says to change the disambiguation guess to << passed >> if the word before is not a determiner and the word after is a preposition.
Resolution_Procedures $ 2 $ 39 $ 8 $ For instance , the Spanish verb comprar ( to buy ) might be associated with the ontological concept named PURCHASE/TERM which is a/DEF generic frame structure corresponding to purchasing events ./O
Information_Structures $ 3 $ 103 $ 26 $ C1 immediately dominates C2 , indicating that C1 is the governor and C2 the dependent.
Overview_of_Ontology $ 2 $ 27 $ 0 $ An ontology/TERM is a/DEF body of knowledge about the world ./O
PAR_as_anIL $ 4 $ 18 $ 7 $ English and other Germanic languages are considered satellite-framed/TERM languages/TERM , expressing/DEF the path in the satellite ;/O Spanish/TERM , among other Romance languages , is a/DEF verb-framed language and expresses the path in the main verb ./O
PAR_as_anIL $ 4 $ 26 $ 15 $ Motion/TERM is a/DEF type of framing event where the path is in the main verb for VFLs and in the satellite for SFLs ./O
OUT $ -1 $ 142 $ 71 $ An SDR/TERM consists/DEF of two words and a dependency type ./O
_Instrumentalists_not_including_string_players $ 8 $ 50 $ 38 $ As we mentioned before , the clusters applied on the EWN InterLingual Index which relied solely on hierarchical information in Wordnet , produced a slight decrease of retrieval performauce in an experiment using 1LI records as indexing units .
Introduction $ 1 $ 48 $ 39 $ A "meta-chain"/TERM is a/DEF representation of every possible lexical chain that can be computed starting with a word of a given sense ./O
Abstract $ 0 $ 4 $ 3 $ REXTOR/TERM (/TERM Relations/TERM EXtracTOR/TERM )/TERM is an/DEF implementation of this model ; in one uniform framework , the system provides two separate grammars for extracting arbitrary patterns of text and building ternary expressions from them ./O
Abstract $ 0 $ 4 $ 2 $ One of the important design decisions following these criteria is the encoding of thematic role information.
OUT $ -1 $ 88 $ 87 $ In these strategies , the compellingness/TERM of/TERM an/TERM objective/TERM measures/DEF the objective's strength in determining the overall value difference between the two alternatives , other things being equal ./O
Introduction $ 1 $ 14 $ 8 $ These uncertainty measures are useful in situations where both the classification of an sample and the system's confidence in that classification are needed .
Content_Planning $ 1 $ 45 $ 20 $ Hence , we built a two-stage statistical model of human-human dialogues using the CMU corpus .
Selective_Sampling_Evaluation $ 4 $ 101 $ 25 $ To normalize for sentence length , we define an evaluation function that computes the similarity between the actual probability distribution and the uniform distribution for a sentence of that length .
The_REXTOR_System $ 5 $ 141 $ 19 $ A relation/TERM rule/TERM takes the following form : EntityType/DEF : => <atoml atom2 acorn3> ;/O The EntityType/TERM is the/DEF trigger for the relation ,/O i.e. , the rule is applied whenever a string of that type is extracted .
OUT $ -1 $ 73 $ 72 $ Guideline ( a ) Given the reader 's AMVF , it is straightforward to establish what represent supporting or opposing evidence for an argument with a given argumentative intent and a given subject .
_The_+END+_ ( null ) _postmodifier_con $ 8 $ 55 $ 2 $ ( Note that many words effectively get generated high up in the tree; in this example sentence , the last words to get generated are the two the's ) More formally , the lexicalized PCFG that sits behind the parsing model has rules of the form Figure 1.
Generation_of_Multiple_Quantifiers $ 5 $ 185 $ 0 $ When there are two distinct roles across the propositions , the algorithm tries to use a universal quantifier for one role and an existential quantifier for another .
_The_co-reference_problem_in_summarization $ 4 $ 55 $ 31 $ Clustering/TERM : The/DEF ability to cluster similar documents and passages to find related information ./O
_NTCIR_Data_Analysis $ 2 $ 102 $ 32 $ P ( occirel ) is replaced by log ( p ( occlrel ) /p ( occ ) ) in order to illustrate this borderline.
Storing_the_corpus_in_a_database $ 4 $ 121 $ 0 $ As already mentioned , the general idea of the query/TERM tool/TERM is to/DEF store the information one wants to search for in a relational database and then to translate an expression in the query language presented in the previous section into an SQL expression that is evaluated on the database ./O
Previous_Schemes_for_Grunt $ 2 $ 58 $ 30 $ In our corpus , only 1 % of the grunts were negative in meaning , and these were all in contexts where a negative answer was expected or likely , so this distinction is a strange choice for a top-level dividing principle .
Bridging_Natural_Language_and $ 4 $ 80 $ 1 $ We argue that a finite-state model of natural language with ternary expressions is currently the most suitable combination for this task .
_Features $ 1 $ 74 $ 3 $ The recall/TERM is the/DEF number of errors identified by a particular feature divided by the total number of errors ./O
Abstract $ 0 $ 42 $ 41 $ Section 2 describes in details the block-based dependency parsing approach.
Joining_TM2_and_DTD $ 3 $ 79 $ 0 $ TM2 specifically stores a type of translation segment class , which we have tagged <segl> , <seg2> . . . <segn> , <title> and <rs> , and which is relevant to the DTD .
_Amanda_Huggenkiss ,_she_proposes_to_meet_you $ 2 $ 58 $ 51 $ Our context model is a first proposal in that direction.
Introduction $ 1 $ 10 $ 4 $ One of the main advantages of probabilistic/TERM methods/TERM , on the other hand , is that they include/DEF a measure of uncertainty in their output ./O
Methods $ 3 $ 45 $ 19 $ 5The MS/TERM tagset/TERM tends/DEF to follow the MULTEXT lexical description for French , modified within the GRACE action ( http : //www.limsi.fr/TLP/grace/doc/GTR-32.1.tex ) ./O
OUT $ -1 $ 55 $ 13 $ Soft decision-making is also useful when the system is one of the components in a/DEF larger decision-malting process ,/O as is the case in speech/TERM recognition/TERM systems/TERM ( Bald et al . , 1989 ) , or in an/DEF ensemble system like/O AdaBoost/TERM ( Freund and Schapire , 1997 ) .
Introduction $ 1 $ 33 $ 28 $ Dunning ( 1993 ) reports that we should not rely on the assumption of a normal distribution when performing statistical text analysis and suggests that parametric analysis based on the binomial or multinomial distributions is a better alternative for smaller texts.
Introduction $ 1 $ 21 $ 15 $ The LCS/TERM represents predicate/DEF argument structure abstracted away from languagespecific properties of semantics and syntax ./O
The_results_reported_in_this_paper_are_part_of_a_more $ 1 $ 42 $ 31 $ Both of them try to expand a `` basic-keyword/TERM ' ' , that is a/DEF keyword direcdy derived from a natural language question ./O
Comparing_Three_Treebank $ 4 $ 67 $ 0 $ Grammars In this section , we describe our methodology for comparing Treebank grammars and the experimental results .
Mapping_a_document_collection_into $ 2 $ 118 $ 71 $ Document maps topic-document relevance shown by document proxy placement and color gradation : In a document map , the horizontal placement of each dot represents the degree of relevance of the corresponding document to the topic .
Abstract $ 0 $ 5 $ 4 $ The user has the option of using one or more of these specific terms to reformulate the next round of searches .
_INTRODUCTION $ 1 $ 20 $ 14 $ For instance , identification of names of Chinese people is very much relied on the surnames , which is a limited set of characters .
Prosody_Prediction $ 4 $ 62 $ 0 $ The tree representations and the metrics can now be used to predict the prosody of a sentence .
OUT $ -1 $ 114 $ 72 $ In our case , the samples are dependent; the classification of sample i is a feature for sample i + 1 , which means that changing the classification for sample i affects the context of sample i + 1.
Abstract $ 0 $ 20 $ 18 $ Unlike previous approaches , their system summarizes a series of news articles on the same event , producing a paragraph consisting of one or more sentences .
Dialogue_examples $ 4 $ 240 $ 33 $ The example represents a dialogue where the computer is implementing the tactic of threatening .
Complexity $ 3 $ 95 $ 12 $ The DM knows about ( and thus can treat or react on ) M different semantic objects .
Support_Vector_Machines $ 2 $ 31 $ 3 $ Formally , we can define the pattern recognition problem as a learning and building process of the decision function f : lq.
Introduction $ 1 $ 5 $ 2 $ The task of VERBMOBIL is the multi-lingual ( German , English , Japanese ) speaker-independent translation of spontaneous speech input that enables users to converse about the scheduling of a business appointment including travel , accommodation , and leisure time planning in a multi-lingual dialogue .
HMM-based_Chunk_Tagger $ 1 $ 40 $ 7 $ ) The second item in the above equation is the mutual information between the tag sequence Tin and the given token sequence G~ .
_Tagging_NuLL-marker_CDM_pairs_ ( via $ 8 $ 104 $ 34 $ If D~ satisfies a stopping criterion , the tree for Dr is a leaf associated with the most frequent class in D~ .
Discussion_and_Conclusions $ 6 $ 115 $ 15 $ Stolcke and Segal ( 1994 ) describe a method for combining a context-free grammar with an n-gram model generated from a small corpus of a few hundred utterances to create a more accurate n-gram model .
_The_+END+_ ( null ) _postmodifier_con $ 8 $ 130 $ 77 $ Even though the model is a top-down , generative one , parsing proceeds bottom-up.
Findings $ 6 $ 135 $ 26 $ ~UPenn CTB at 80 ,000 tokens , 146 New token+types observed : AD~ , hD~ig , AD~ , ADrift , AD~ , ADI]~ , AD~ , CD 1 " 2 ~ , CD 1 7 1 6~ , CD2 0 0 , CD2 0 0~ , CD2 1 0 0~ , CD2 8 0 0~' , CD4 00~j' , CD 5 5 , jj~..~d~ : , JJ~.~E , JJ : : ~ ,~ , JJ~-- , JJ~J~ , LC~ , LC~ , LC~ , LC~ , M~ , M~ , NN~ , NN~ , NN~ , NN~ , NN "~ , NN~ , NN~ , NN~-~ , NN~ , NN~ , NN~ NN~J~ , NN~ , NN~]~ , NN~J NN~ , NN~ , NN~ , NNJ~ , NN~ NN~ NN~; , NN~ , NN~J~ , NN~J~ NN~ , NN~]~ NN~ , NN~ NN~ , NN~ , NN~ , NN~ NN~ , NN~ , NN~ , NN~ NN~ NN/J~ , NN]~I~ , NN~ , NN~ , NN~ , NN ~ , NN~ NN~ , NN~t , NN~ , NN~ , NN~ , NN~ , NR~ , NR~ , NRP~~ , NR~ , NR~~ , NR~ , NR~ , NR~ , NR~-~ , NR~X~ , NR~'~-~ " ~ , NR~~ , NR~ , NR~ " ~~ , NR~J~ , NR~ , NRJ~ , NR-~ , NR~ , NR~ , NR~ , NR~ , NR~ , NR~ , NTI 1 ~ , NTI 8 ~ 3~E , NTI 8~ , NT1 9 ~ 9~ E , NTI ~ 8 2~E , NTI ~ 8 6~E , NTI ~ 8 9~E , NTIS , PUt'i , P~ , e~ , VA~-~ , VV~ ,. , vvI~K vv~ , vv3~ , vv~lK vvt/~lii , vv : ~& vv-~l~ , vv-~ , vv'~'~- , vv~-.k vv : ~' , vv~ , vv~ , vv~t , vv~a , vv~J~ , vv~~l , vv~~ , vv~~J , vv~l~A~ , wE , vv~ , vv~ , vv~#L.t~ , vv~ffff~. , , vv--~'~J= , vv~~! , vv~l; Tota tags : 12 , rota new items : 146.
Introduction $ 1 $ 17 $ 8 $ This approach reports experimental results , using the SRI Core Language Engine , ( Alshawi , 1992 ) , in the ATIS domain , of more than a 3-fold speedup at a cost of 5 % in grammatical coverage , the latter which is compensated by an increase in parsing accuracy .
From_Prepositional_Phrases_to $ 3 $ 83 $ 26 $ But actually the situation/TERM HAS-PART-STATE/TERM is a/DEF state in which only one is present ,/O which is obviously `` little '' .
Statistics_Based_Hybrid_Approach_to $ 2 $ 21 $ 3 $ CutTenfly , we are considering 7 Chinese base phrases in our research , namely base/TERM adjective/TERM phrase/TERM ( BADJP/ACR ) , base/TERM adverbial/TERM phrase/TERM ( BADVP/ACR ) , base/TERM noun/TERM phrase/TERM ( BNP/ACR ) , 73 base/TERM temporal/TERM phrase/TERM ( BTN/ACR ) , base/TERM location/TERM phrase/TERM ( BNS/ACR ) , base/DEF verb/TERM phrase/TERM ( BVP/ACR ) and base/TERM quantity/TERM phrase/TERM ( BMP/ACR ) Though theoretically definitions for these base phrases are still unavailable , Appendix I lists the preliminary illustrations for them in BNF format ( necessary account for POS annotation can also be found ) . . To frame the identification of Chinese base phrases , we fm'ther develop the following concepts : Definition 1 : Chinese/TERM based/TERM phrases/TERM are recognized as atomic/DEF parts of a sentence beyond words that posses certain functions and meanings ./O
Conclusions_and_Future_Work $ 5 $ 259 $ 3 $ Measures which gauge content similarity produce more highly correlated rankings whenever ground truths do not disagree in focus .
Generation_of_Crisp_Descriptions $ 3 $ 54 $ 0 $ Generation of descriptions covers a number of tasks , one of which consists of finding a set L of properties which allows a reader to pick out a given unique individual or set of individuals .
OUT $ -1 $ 200 $ 129 $ This is a way to decrease or eliminate the negative influence of synonyms in relevant documents .
Qualitative_Evaluation_of_the $ 4 $ 110 $ 9 $ We employ the supertag-based tree model whose parameters consist of whether a lexeme ld with supertag sd is a dependent of lexeme 1 , , , with supertag s , , , .
The_hyperonym_problem $ 2 $ 33 $ 15 $ on the other hand , it can usually be ignored , as most of today ' s practical applications either do not require the production of a more general word ( i.e. . there is a one-to-one mapping from concept to word ) or can rely on fairly simple mechanisms that . ,avoid ,lexical repetitions bv choosing from a fixed , pre-defined set of near-synonyms .
Tree_Structures $ 2 $ 30 $ 14 $ The only difference comes from a simplification performed by joining the words into phonological words ( composed of one content word noun , adjective , verb or adverb and of the surrounding function words ) .
Introduction $ 1 $ 11 $ 4 $ In such cases , the synonymy relations that hold in the specific domain are only a restricted portion of the synonymy relations holding for a given language at large .
Introduction $ 1 $ 9 $ 0 $ There is a strong relation between a learning strategy , its formal learning framework and its representational theory.
Introduction : _multi-document $ 1 $ 43 $ 40 $ The next section describes the multi-dimensional space for the document collection .
Introduction $ 1 $ 37 $ 26 $ Section 3 describes features incorporated into the model and the process of identifying potential segmentation positions .
OUT $ -1 $ 131 $ 77 $ a QA system will return : cp However , an AE/TERM system/TERM will return/DEF all the sentences in the text that directly answer the question ,/O among them ( 1 ) .
Introduction $ 1 $ 13 $ 9 $ So TRANSTYPE/TERM is a/DEF specialized text editor with an embedded Machine translation engine as one of its components ./O
_Phrasal_Indexing $ 1 $ 25 $ 0 $ For the baseline run experiments , we utilized the engine of Coneeptbase/TERM Search/TERM 1.2/TERM , a/DEF commercial based search engine adopting vector space model approach ./O
Manual_Tagging_Process $ 2 $ 40 $ 18 $ Written Chinese consists of rurming texts without word delimiters ; the first step is is to segment the text into Chinese word sequences .
Abstract $ 0 $ 19 $ 17 $ MT supports the translingual aspects of the effort and can be viewed as an embedded tool which facilitates other technologies .
Introduction $ 1 $ 8 $ 0 $ Word/TERM Sense/TERM Disambiguation/TERM ( WSD/TERM ) is the/DEF problem of assigning the appropriate meaning ( or sense ) to a given word in a text or discourse ./O
Robustness $ 4 $ 92 $ 1 $ Before the semantic representation is handed to microplanning , the robustness preproeessing module of the generator checks the input , inspecting its parts for known problems .
_Otherwise ,_add_to_the_current_context_new $ 6 $ 111 $ 28 $ However , in the current implementation , each Qi consists of one proposition only .
Shortcomings $ 5 $ 140 $ 1 $ Although the measure is a big step up from the measures used earlier , it has a number of shortcomings .
Overview_of_the_system $ 2 $ 17 $ 9 $ Si/TERM and Sf/TERM stand for the/DEF initial and the final state of chunk whose descriptor is Si ./O
Abstract $ 0 $ 1 $ 0 $ Audio/TERM comprehension/TERM tests/TERM are designed/DEF to help evaluate a listener's understanding of a spoken passage and are frequently a key component of language competency exams ./O
OUT $ -1 $ 179 $ 53 $ Text classification is therefore not a solution .
Middle $ 6 $ 149 $ 112 $ ( 1 ) IfSj/TERM is the/DEF only one syuset that has been mapped to Cilin tags ,/O we choose a Cilin tag and map Si to it .
Abstract $ 0 $ 23 $ 21 $ Conversation/TERM agent/TERM is a/DEF kind of intelligent agent a computer program that is able to communicate with humans as another human being ./O
_Amanda_Huggenkiss ,_she_proposes_to_meet_you $ 2 $ 77 $ 70 $ Topic , the role of aboutness is attributed to a discourse referent that is identifiable and more or less active .
Markov_Modeling $ 2 $ 23 $ 0 $ HMM/TERM is a/DEF probabilistic finite state automaton used to model the probabilistic generation of sequential processes ./O
Introduction $ 1 $ 14 $ 7 $ The syntactic and part/Of-speech informations were obtained from the part of the corpus processed in the Penn Treebank project ( Marcus et al. , 1993 ) .
Experimental_Design $ 3 $ 182 $ 16 $ Content-based measures which depend on a single ground truth gi compute the summary-ground truth similarity sim ( s , gi ) .
_The_session_was_also_attended_by_observers_from_the_following_international_organizations : $ 7 $ 193 $ 142 $ It is fairly simple to extract the details of the items from initializers with this basic form , as the modification of the hypernym takes the form of a relative clause , a prepositional phrase or an adjectival phrase .
OUT $ -1 $ 61 $ 35 $ The first metric , simple/TERM accuracy/TERM , is the/DEF same string distance metric used for measuring speech recognition accuracy ./O
Rule_set_learning $ 3 $ 59 $ 21 $ In ( Turmo et al. , 1999 ) , the concept of S-set/TERM has been presented as a/DEF syntactic relation generalization ,/O and a distance measure has been based on this concept.
Unfolding_and_Specialization $ 2 $ 27 $ 0 $ The initial grammar is the grammar underlying the subset of correct parses in the training set.
Evaluation_Measures $ 2 $ 137 $ 112 $ However , the scores produced by such evaluation measures cannot be used reliably to compare summaries of drastically different lengths , since a much longer summary is more likely than a short summary to produce a term frequency .vector which is similar to the full document's "tf vector , despite the normalization of the two vectors .
Generation_and_linguistic_representation $ 4 $ 79 $ 7 $ Once the user ' s words have been interpreted , a layer of production rules constructs obligations for response ( Traum and Allen , 1994 ) ; then , a second layer plans to meet these obligations by deciding to present a specified kind of information about a specified object .
Generation_of_Vague_Descriptions $ 4 $ 92 $ 11 $ The result would be a new list L = {yellow ,chihuahua ,largestl} , where 'largestt'/TERM is the/DEF property 'being the unique largest element of C' ./O
Abstract $ 0 $ 4 $ 2 $ MT has been used to facilitate cross-language information/DEF retrieval/TERM ( IR/TERM ) , topic detection and other , wide-scoped scenarios .
SYSTEM : _U_S-air_flight_63_57_departs_la $ 9 $ 41 $ 34 $ The output is a tree structure which represents the hierarchy of constraint information that is deemed most useful to convey to the user.
Results $ 4 $ 70 $ 1 $ The word list captions scored 74.6% on our crossing measure , while the sentence captions scored 89.5% .
Methodology $ 2 $ 29 $ 1 $ In order to set the terms of the problem , we find it useful to partition the set of synonymy relations defined in WordNet into three classes : .
Generation_of_Crisp_Descriptions $ 3 $ 63 $ 9 $ 4 Informally and forgetting about the special treatment of head nouns what happens is the following : Tile algorithm iterates through a list P in which the properties appear in order of ' preference ' ; for each attribute , it checks whether specifying a value for that attribute would rule out at least one additional member of C ; if so , the attribute is added to L , with a suitable value .
Conclusion $ 4 $ 86 $ 4 $ This work represents a novel approach to translation modeling which is most appropriate for applications like TransType/TERM which need/DEF to make rapid predictions of upcoming text ./O
Related_Work $ 2 $ 37 $ 10 $ He defines also a new measure , called success/TERM rate/TERM which indicates/DEF if a question has an answer in the top ten documents returned by a retrieval system ./O
Abstract $ 0 $ 3 $ 2 $ However , an important area of research that has not been given the attention it deserves is a formal analysis of the parameters affecting the performance of the learning task faced by these systems .
Generation_of_Multiple_Quantifiers $ 5 $ 206 $ 21 $ Figure 3 : Sentences with two quantifiers a patient is a human and a human has a left arm and a right arm .
Complexity $ 3 $ 94 $ 0 $ The basic/TERM entity/TERM is a/DEF semantic object ( S ) which is an atomic item treated by the DM ./O
_Annotation_Guidelines $ 4 $ 102 $ 14 $ Another example is 23 ) , it denotes an subordinate feature , annotated as 31 ) 7.
OUT $ -1 $ 0 $ 0 $ Using Summarization for Automatic Briefing Generation Inderjeet Mani .
Background : _The_STOP_System $ 2 $ 19 $ 2 $ Internally , STOP/TERM is a/DEF fairly conventional shallow NLG system , with its main innovation being the processing used to control the length of leaflets (/O Reiter , 2000 ) .
Empirical_Evaluation $ 4 $ 147 $ 36 $ Therefore , a classifiefs ability of learning from a small training pattern set is a major concern.
Robust_Name_Finding $ 3 $ 62 $ 3 $ A common definition of the extended name finding task , known as the "named entity" task , also includes numeric phrases , such as dates , times , monetary amounts , and percents , which are often the answers to other common questions When?
Knowledge_base $ 5 $ 104 $ 18 $ • Matching of the depth of the phrases in parse trees : 1 point • Matching of the type of the phrases ( phrase types differ depending on surface cases and verb conjugations , etc ) : 1 point user question are summed up and normalized by the maximum/TERM matching/TERM score/TERM (/TERM MMS/TERM )/TERM as follows ( the/DEF MMS is the similarity score with the same sentence )/O : The sum of scores of~ 2 phrase similarities ] The MMS of ~ ( The MMS of~ the user question] × \the KU case] The above score is given to the KU as its certainty score .
Multiple_heuristics_for_word_sense $ 2 $ 63 $ 38 $ And we uses the 25 semantic tags of WordNet as sense tag : n6 ( s , ) = max p ( t , I x ) xeDef with p = , Z ( l_a ) /2 × ~ ' ( 1~ ~ ) p ( ti l x ) = Freq ( ti , x ) Freq ( x ) In this formula , Defis the set of content words of a Korean definition sentence , t is a semantic tag corresponding to the synset s and n refers to Freq ( x ) .
OUT $ -1 $ 0 $ 0 $ The hyperonym problem revisited : Concep , tual and : : lexical .
Introduction $ 1 $ 17 $ 4 $ A sentence planner is then used to select an appropriate syntactic structure .
See_the_report_entitled_ $ 3 $ 33 $ 23 $ 6 As we sought to measure the performance of each component in the systems , it quickly became apparent that not all available measures may be equally applicable for our filtering task.
Abstract $ 0 $ 73 $ 12 $ The process of extension/TERM simply consists of deriving/DEF a more elaborate form with a richer meaning using the generator's linguistic resources --/O it is useful to think of obtaining this by carrying out a step of derivation in a lexicalized grammar ( Stone and Doran , 1997 ) --and then consulting the model of the context to obtain an updated interpretation .
Comparing_the_five_approaches $ 4 $ 89 $ 19 $ ~The Kappa/TERM statistic/TERM ( Cohen , 1960 ) is a/DEF better measure of inter-annotator agreement which reduces the effect of chance agreement ./O
Introduction $ 1 $ 7 $ 2 $ A difference/TERM coefficient/TERM defined by Yule ( 1944 ) showed/DEF the relative frequency of a word in the two corpora ./O
Abstract $ 0 $ 47 $ 2 $ Semantic ( Concrete ) semantic representations provide a complete notation for " logical forms " where there is no longer any reference to ,the knowledge base .
The_machine_learning_method $ 3 $ 137 $ 78 $ We now present a method to detect what the "good" c , lauses are , that is , the clauses that explain the concept that we want to learn , and a measure of the "quality" of the learning that has been conducted.
Previous_Work : _Richer_Features $ 1 $ 16 $ 4 $ ( 1989 ) describe a language model that builds a decision tree that is allowed to ask questions about the history up to twenty words back .
Research_focus : _content_aggregation $ 2 $ 134 $ 73 $ buildFactoringStrategy/TERM (/TERM Matrix/TERM )/TERM : returns/DEF inside a list a pair ( Dim , increasing ) where Dim is the matrix ' s dimension ( i.e. , column ) with the lowest number of distinct values ./O
Reasons_for_cross-corpor_a_degradation $ 9 $ 230 $ 4 $ Section 8 shows that sharing genre/topic is a key factor; as the WSJ corpus attains better results on the press : reportage category than the rest of 213 the categories on the BC itself.
OUT $ -1 $ 155 $ 84 $ An MCE/TERM is a/DEF pair consisting of a word and a dependency type ./O
The_Argument_Generator $ 1 $ 22 $ 2 $ An evaluation , which is a number in the interval [0 ,1] where , depending on the subject , 0 means "terrible" or "much worse" and 1 means "excellent" or "much better" ) .
Data_Collection_and_Evaluation $ 4 $ 176 $ 30 $ IBR/TERM measures/DEF the average number of new attributes introduced per user query ./O
Introduction $ 1 $ 38 $ 31 $ using sortal constraints such as company and location , and restrictive constraints such as subject_of or same sentence .
Applications_of_LexTract $ 4 $ 184 $ 38 $ T To summarize , we have just showed that , 7The number 97 . 2% is the sum of two numbers : the first one is the percentage of matched template tokens ( 82 . 1% from Table 2 ) .
The_TABULATE_ILP_Method $ 3 $ 94 $ 28 $ The size/TERM of/TERM a/TERM theory/TERM is the/DEF sum of the sizes of its clauses ./O
The_TABULATE_ILP_Method $ 3 $ 97 $ 0 $ 3.3 Noise Handling A clause needs no further refinement when it meets the following criterion ( as in RIPPER ( Cohen , 1995 ) ) : P - . __ . 2_ > ( 6 ) p+n where p/TERM is the/DEF number of positive examples covered by the clause ,/O n/TERM is the/DEF number of negative examples covered ./O
Middle $ 6 $ 78 $ 41 $ It shows that semantic tagging is a challengeable problem in Mandarin Chinese .
Acquiring_Lexical_Translations $ 3 $ 63 $ 12 $ The probability/TERM P/TERM (/TERM Ws/TERM ,/TERM WT/TERM )/TERM is computed/DEF in the same way as n-gram model : where wl E LsUe , zi E LTUe , e is the empty string and wi_zi is the symbol pair ( colons are the delimiters ) drawn from the source and target language ./O
Tree_Metrics $ 3 $ 44 $ 4 $ Therefore the tree/TERM distance/TERM can be defined as the/DEF cost of the sequence minimizing this sum ./O
Related_work $ 6 $ 165 $ 16 $ In Wirth's work the resolvent of the PPT represents the partial proof and a more general purpose metainterpreter is used .
The_NJFun_System $ 2 $ 100 $ 86 $ After the user's response , the next state represents that N JFun has now greeted the user and obtained the activity value with high confidence , by using a nonrestrictive grammar.
Treebank_Representation $ 4 $ 54 $ 0 $ A folded/TERM treebank/TERM is a/DEF representation of a set of parse trees which allows an immediate assessment of the effects of inhibiting specific rule combinations ./O
Abstract $ 0 $ 3 $ 2 $ The integration is a multi-step unification process.
_Semantic_classes_of_the_head_of_the_NP : _If $ 8 $ 81 $ 15 $ The first , IB1/TERM is a/DEF k-nearest neighbour algorithm ./O
Accommodation_in_GoDiS $ 3 $ 72 $ 21 $ The check/TERM operator/TERM "answer-to/TERM (/TERM A/TERM ,/TERM Q/TERM )/TERM "/TERM is true/DEF if A is a relevant answer to Q given the current information state ,/O according to a ( domain-dependent ) definition of question-answer relevance.
Selection_of_candidate_strings $ 2 $ 25 $ 1 $ Of the 85 ,135 words in our system's dictionary , 9217 of them are monosyllabic , 47778 are disyllabic , 17094 are m-syllabic , and the rest has four or more characters .
Abstract $ 0 $ 163 $ 16 $ This algorithm is only a useful approximation towards a complete account of a text ' s discourse structure .
Related_Work $ 5 $ 159 $ 1 $ For example , revision/TERM ( Robin , 1994 ) is a/DEF technique for building semantic inputs incrementally ./O
The_lexicon_size_of_a_typical_large-vocabulary $ 9 $ 97 $ 18 $ This is a Chinese to English translation resource that was manually compiled by a team of linguists from more than 250 text sources , including special and general-purpose print dictionaries , and other text sources such as newspapers .
Conclusion $ 7 $ 197 $ 8 $ Returning to the question posed at the beginning of the paper what is the appropriate size of elementary discourse units the answer is twofold : first of all , coherence relations can be found to hold between phrases and the clause containing them , so one should indeed start looking for discourse units at the phrase level .
Construction_of_Features $ 3 $ 110 $ 42 $ Following is an example of a feature .
Introduction $ 1 $ 35 $ 28 $ REXTOI~ also provides a playground and testbed for future experimentation in linguistically-motivated indexing schemes .
How_to_generate_technical $ 2 $ 65 $ 14 $ describes a model of the speaker's activity in which choices in the What to say component are conscious , while choices in the How to say it component are automatic.
The_Objective_Function $ 3 $ 41 $ 0 $ Previous research approached the task of determining which rule combinations to allow either by a process of manual trial and error or by statistical measures based on a collection of positive examples only : if the original grammar produces more than a single parse of a sentence , only the " correct " parse was stored in the treebank .
Information_extraction $ 3 $ 40 $ 5 $ The latter situation is the most difficult to handle since time evolution needs to be considered.
ALLiS $ 2 $ 18 $ 13 $ If the formalism is powerful enough , the main problem with XFST is the number of transducers generated by ALLiS .
Levels_of_Annotation $ 4 $ 95 $ 1 $ In spite of the number of levels considered , and their sometimes conflicting requirements , we tried to develop a coherent , unitary approach to design and application of annotation schemes .
Introduction $ 1 $ 16 $ 8 $ Therefore , they offer the following approximation : P ( w . ilw -1 ) wiJwi_N+l ) ( I ) A common value for N is 2 ( bigram language model ) or 3 ( trigram language model ) ; only a short local context of one or two words is considered .
Introduction $ 1 $ 6 $ 0 $ Pustejovsky ( Pustejovsky , 1995 ) proposed the theory of a generative lexicon as a framework by which meanings of words are expressed in one unified representation .
Results $ 3 $ 85 $ 28 $ Keyword totals in the same grouping are not statistically different .
Hyperonyms_in_NLG_systems $ 4 $ 72 $ 13 $ The latter are explicitly represented in his system as a . list/TERM of/TERM attributes/TERM 'to communicate about an entity' , which is a/DEF subset of the overall knowledge the system has of that entity ./O
Solving_the_generation_problem $ 5 $ 144 $ 4 $ As we have seen , ( 5b ) represents a common pattern of description; this particular example is motivated by an exchange two human subjects had in our study , cf .
OUT $ -1 $ 209 $ 155 $ Our test/TERM queries/TERM are real/DEF world queries that express a concrete information need ./O
Levels_of_Annotation $ 4 $ 112 $ 0 $ 4.2 The Morpho~Syntactic and Syntactic Levels The ADAM proposal for the morphosyntactic/TERM level/TERM is a/DEF two-layer annotation structure , containing respectively information on word category and morphosyntactic features ( pos tagging ) , and non recursive phrasal nuclei ( called chunks ) ./O
Related_Work $ 5 $ 122 $ 4 $ Our model is the most related to Coccaro and Jurafsky ( 1998 ) , in that a reduced vector space approach was taken and context is represented by the accumulation of word cooccurrence vectors .
Introduction $ 1 $ 21 $ 14 $ `` Section 2 discusses the properties of numerical evaluation measures , points out several drawbacks associated with intrinsic measures and introduces new measures developed by the authors .
OUT $ -1 $ 60 $ 34 $ We employ two metrics that measure the accuracy of a generated string .
Abstract $ 0 $ 132 $ 71 $ Meanings of referring expressions should therefore appeal to a condition @ p which describes F iff there is a C for which @ cP describes F . Clearly , if @ p describes F and @ p describes F ' then @ p describes FU I ' ~ .
Conceptualizing_Events $ 2 $ 142 $ 112 $ In addition to the cascaded processes there is a concept lexicon , accessible via a concept matcher : these modules , which are called by the construction/TERM process/TERM , find/DEF best matches for structures that can either be subsumed by a more complex concept or may represent still incomplete concepts ./O
_Background $ 2 $ 47 $ 31 $ Aside from homographs and homophones , another source of ambiguity in the Chinese language is the definition of a Chinese word.
The_CGS_system $ 4 $ 97 $ 29 $ ( 4 ) The color shows the neighbourhood and ( 5 ) shape shows the listing agency .
Evaluation_Measures $ 2 $ 32 $ 7 $ To compare two evaluation measures , whose scores may have very different ranges and distributions , one must compare the order in which the measures rank various summaries of a document.
Memory_based_Learning $ 4 $ 60 $ 11 $ Given one of the N most probable chunk sequences extracted by the error-driven HMMbased chunk tagger , we can extract a set of chunk/TERM patterns/TERM , each of them with/DEF the format : XP 1 n n+l r~+l = poroPlrn Pn+l , where is the structural relation between Pi and Pi+l ./O
_Conclusion $ 4 $ 162 $ 1 $ The method provides a unified framework based on MBL .
Introduction $ 1 $ 5 $ 0 $ Discourse/TERM refers to any/DEF form of language-based communication involving multiple sentences or utterances ./O
Abstract $ 0 $ 3 $ 2 $ It integrates lexical , textual and world knowledge into a single hierarchical framework .
Abstract $ 0 $ 81 $ 14 $ Given a sentence• S and a type of information T the system verifies if the sentence matches some of the patterns associated with type T. For each matched pattern , the system extracts information from the sentence and instantiates a template of type T. For example , the Content slot of the problem identification template is instantiated with all the sentence • : ( avoiding references , structural elements and parenthetical expressions ) while the What slot 'of the topic of the document template is instantiated with a parsed sentence fragment • to the left or to the right of the make known relation depending on the attribute voice/TERM of/TERM the/TERM verb/TERM ( active/DEF vs. passive )/O .
Chunk_Types $ 3 $ 28 $ 11 $ " ( VP loves ( NP Mary ) ) " above , or ADJPs and PPs below.
Fundamental_Data_Differences $ 2 $ 39 $ 0 $ 2.2 Uncertainty in Speech Transcriptions One of the primary factors that distinguishes textbased language processing tasks , such as reading comprehension , from spoken-language processing tasks , such as audio comprehension , is the uncertainty inherent in the word sequence output by the speech recognizer .
Block-based_Chinese_dependency_analysis $ 2 $ 75 $ 15 $ We define 11 kinds of blocks as explained below .
Sample_Selection $ 2 $ 43 $ 15 $ Although there exist abundant collections of raw text , the high expense of manually annotating the text sets a severe limitation for many learning algorithms in natU is a Set of unlabeled candidates .
_System_Configuration $ 2 $ 56 $ 32 $ If his choice is the latter , he can move up to the headline or the leading text that offer more about the document content.
LTAGs_and_Extraction $ 3 $ 39 $ 12 $ Figure 2 shows the etrees , the derived tree , and the derivation tree for the sentence underwriters still draft policies .
See_ ( Chu-Carroll_and_Carberry_1998 ) _tbr_an $ 4 $ 117 $ 8 $ Task efficacy is the method we have adopted in our evaluation framework .
Selective_Sampling_Evaluation $ 4 $ 91 $ 15 $ Entropy/TERM measures/DEF the uncertainty of assigning a value to a random variable over a distribution ./O
Search $ 2 $ 59 $ 17 $ The retrieval status value is shown in a bold black font after the URL title .
The_Interactional_Framework $ 2 $ 31 $ 11 $ If this is the case , it is possible that agent B comes to choose a plan to satisfy this goal , even if it does not yield any direct utility to him.
Abstract $ 0 $ 135 $ 74 $ In particular , consider a description L that consists of a list of constraints/TERM @Li/TERM (/TERM x/TERM )/TERM formulated/DEF in terms of a tuple of variables x and atomic conditions on those variables Li ( x ) ./O
Introduction $ 1 $ 24 $ 18 $ The sentence " A nurse inspected each patient .
Only_the_relevant_attributes_of_each_semantic_role $ 5 $ 164 $ 73 $ Once the input text has been split into clauses after applying the heuristic H1 , the next problem consists of the detection of the omission of the subject from each clause.
Evaluation $ 7 $ 202 $ 18 $ For example , a user sometimes asks "what is the difference between A and B" , or when the system asks "select from A and B" , a user answers "I don't know".
OUT $ -1 $ 115 $ 114 $ The opposing evidence ( i .e . , ContrastingSubObjectives ) , that must be considered , but not in detail , is also expressed in natural language .
OUT $ -1 $ 11 $ 7 $ These rules are given as recommendations or prohibitions for both the lexicon and the grammar .
Impact_of_Lexicon_Size $ 9 $ 185 $ 47 $ 12 Conclusions and Future Work We proposed an approach to cross-lingual IR based on hidden Markov models , where the system estimates the probability that a query in one language could be generated from a document in another language .
Learning_RRE_Rules $ 4 $ 64 $ 11 $ Very/TERM Reduced/TERM Regular/TERM Expression/TERM (/TERM VRRE/TERM )/TERM : Given/DEF a finite alphabet E , the set of very reduced regular expressions over that alphabet is defined as : ( 1 ) 'v'a~ E : a is a VRRE and denotes the set { a } ( 2 ) ./O
Example_Dialogue $ 3 $ 65 $ 42 $ That is the effect of adding ( M1 ) to the Grammar : Since the Parsebank and the Prediction Models are updated on-line , the presence of the word retrieve in subsequent utterances becomes a strong indicator of LIST and , associatively , of [listMa±l].
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 268 $ 233 $ ' Rec/TERM ' denotes the/DEF demonstrate~ that the criterion , domain dependency ratio of the documents judged YES that were also of words effectively employed ,/O i evaluated as YES , and Tree/TERM is the/DEF percent of the documents that were evaluated as YES which corretion Tradeoff ./O
Abstract $ 0 $ 29 $ 2 $ Virtual/TERM prototyping/TERM is a/DEF technique which has been suggested for use in , for example , telecommunication product development as a high-end technology to achieve a quick digital model that could be used in the same way as a real prototype ./O
Architecture_of_WIT-Based_Spoken $ 3 $ 95 $ 61 $ If the system holds the initiative , the module executes the initial function of the phase .
Analysis_module $ 2 $ 49 $ 9 $ SUPAR/TERM allows/DEF to carry out either a full or a partial parsing of the text , with the same parser and grammar ./O
_The_session_was_also_attended_by_observers_from_the_following_international_organizations : $ 7 $ 114 $ 63 $ Universities ( 28 . 1 ) , celebrities ( 53 . 0 ) and countries ( 36 . 5 ) are the most productive NEs in their categories while international agencies ( 4 . 0 ) , film directors ( 4 . 4 ) and states ( 8 . 7 ) are the less productive ones .
OUT $ -1 $ 165 $ 123 $ classifier is of its classification .
Approach $ 2 $ 46 $ 40 $ In case a word is classified as belonging to more than one chunk type , preference will be given to the chunk type that occurs most often in the training data .
Error-driven_Learning $ 4 $ 119 $ 2 $ For a new lexical entry e i , the effectiveness/TERM F~/TERM (/TERM e/TERM i/TERM )/TERM is measured/DEF by the reduction in error which results from adding the lexical entry to -~ Error ( e , ) ./O
Dialogue_Management $ 2 $ 76 $ 32 $ D's pleasant aspects overweight unpleasant ones ) , then the neededand must-factors will first be checked from the point of view of their negative aspects ( "to what harmful consequences or punishments D would lead?
Generation_of_Multiple_Quantifiers $ 5 $ 192 $ 7 $ In " Each patient is given a high severity rating " , performing universal quantification on the patients ( ARG3 ) is a separate decision from the existential quantification of the severity ratings ( ARG2 ) .
_IBM_expected_[SBAa_each_employee_to $ 5 $ 32 $ 20 $ Inspired by the PP-attachment work of ( Stetina and Nagao , 1997 ) , we use WordNet vl.6 ( Miller et al. , 1990 ) as our semantic dictionary , where the hypernym structure provides the basis for semantically-motivated soft clusters .
Robust_Name_Finding $ 3 $ 73 $ 14 $ However , the second version in Figure 2 shows that if we know that "OUR STRAWS" is a location phrase and that "YEAR BEHIND IT" is a person phrase ( albeit incorrectly transcribed ) , we could at least know where in the passage to find the answer to the Who?
Automatic_Extraction_of $ 4 $ 63 $ 12 $ The rule extraction scope is limited to the end of a sentence or , if there is a conjunctive/TERM ending/TERM ( eCC/ACR ) in the sentence , only to the conjunctive ending of the sentence .
OUT $ -1 $ 169 $ 118 $ This sequence is then evaluated and ordered in the populat ion.
Highlight $ 3 $ 142 $ 91 $ This set is then passed to the IE component which deals with relational constraints such as same sentence , and interactions between constraints which have to be calculated on the fly such as precedes which are not indexed during preprocessing .
Issues_and_proposals $ 3 $ 95 $ 52 $ However , example 5 , in which turns 2-7 are the answer to the question in utterance 1 , shows that this is not the case.
Concepts $ 2 $ 57 $ 32 $ According to MDL , the best/TERM probability/TERM model/TERM for a given set of data is a/DEF model that uses the shortest code length for encoding the model itself and the given data relative to it ./O
Introduction $ 1 $ 13 $ 6 $ The prosodic/TERM information/TERM consists/DEF of ToBI labeling of accents and breaks (/O Silverman et al . , 1992 ) .
The_Classifiers $ 3 $ 106 $ 69 $ Based on the keyword feature table , the second phase of rule insertion translates each rule into a M-dimensional vector a and a N-dimensional vector b , where M/TERM is the/DEF total number of features in the keyword feature table and/O N/TERM is the/DEF number of categories ./O
OUT $ -1 $ 0 $ 0 $ Sentence generation and neural networks 2 .
Shallow_Parsing $ 4 $ 74 $ 4 $ Our modeling of the problem is a modification of our earlier work on this topic that has been found to be quite successful compared to other learning methods attempted on this problem ( Mufioz et al . , 1999 ) and in particular , better than the IO modeling of the problem ( Mufioz et al . , 1999 ) .
_Instrumentalists_not_including_string_players $ 8 $ 111 $ 99 $ As it is said in ( Seto , 1996 ) , ~ ( . . ) There often is a one-tone correspondence between different languages in their lexiealization behaviour towards metonyrny , in other words , metonymically related word senses are often translated by the same word in other languages " .
Empirical_Evaluation $ 4 $ 128 $ 17 $ Precision/TERM ( P/ACR ) is the/DEF percentage of the predicted documents for a given category that are classifted correctly ./O
_Features $ 3 $ 46 $ 23 $ The mention/TERM is a/DEF child of a relative clause ./O
Experimental_Setup $ 3 $ 51 $ 3 $ The tagging experiments were performed on the LOB-corpus ( Johansson et al , 1986 ) .
Setting $ 3 $ 59 $ 0 $ A number of comparative experiments has been carried out on a subset of 21 highly ambiguous words of the DSO/TERM corpus/TERM , which is a/DEF semantically annotated English corpus collected by Ng and colleagues (/O Ng and Lee , 1996 ) .
OUT $ -1 $ 183 $ 0 $ 4.4 Perplexity and Cross Entropy Cross/TERM entropy/TERM is a/DEF goodness measure for probability estimates that takes into account the accuracy of the estimates as well as the classification accuracy of the system ./O
Maximum_Entropy_Modeling $ 2 $ 52 $ 10 $ We build a probability distribution p ( ylx ) , where y/TERM • { 0 , 1 } is a/DEF random variable specifying the potential segmentation position in a context x ./O
Tree_Generalization_using_Tree-cut $ 2 $ 49 $ 3 $ A tree-cut/TERM is a/DEF partition of a thesaurus tree ./O
System_Overview $ 3 $ 38 $ 2 $ The core of LexTract is an extraction/TERM algorithm/TERM that takes/DEF a Treebank sentence such as the one in Figure 5 and produces the trees ( elementary trees , derived trees and derivation trees ) such as the ones in Figure 3 ./O
Conceptualizing_Events $ 2 $ 104 $ 74 $ Figure 2 sketches such a cascade of dependent parallel processes in our model of the conceptualizer : The cascade/TERM consists/DEF of the processes construction , selection , linearization , and pvm-generation ( preverbal-message-generation ) ./O
Introduction $ 1 $ 20 $ 14 $ Section 4 discusses additional robustness techniques at the recognizer level , and Section 5 describes dialogue-level robustness techniques .
The_machine_learning_method $ 3 $ 84 $ 25 $ This corpus has been POS-tagged with the help of annotation tools developed in the MULTEXT project ( Armstrong , 1996 ) ; sentences and words are first segmented with MtSeg ; words are analyzed and lemmatized with Mmorph ( Petitpierre and Russell , 1998 ; Bouillon et al . , 1998 ) , and finally disambiguated by the Tatoo/TERM tool/TERM , a/DEF Hidden Markov Model tagger (/O Armstrong et al . , 1995 ) .
Experiments $ 3 $ 93 $ 40 $ The other experimental variable is the number of chosen features.
Resolution_Procedures $ 2 $ 32 $ 1 $ Here the interlingua we rely on is a variant of Text/TERM Meaning/TERM Representation/TERM ( TMR/ACR ) ( see http : //crl .nmsu .edu/Research/Projects/mikro/in dex .html ) and we focus on a sample Spanish text and its interlingual analysis , which is known to be reproducible automatically .
Accommodation_in_GoDiS $ 3 $ 56 $ 5 $ But it can be figured out since J knows that this is a relevant question .
OUT $ -1 $ 109 $ 72 $ 28 The UNL/TERM system/TERM architecture/TERM consists/DEF of two main processes , the encoder and decoder , and several linguistic resources , each group of these corresponding to a NL embedded in the system ,/O as depicted in Figure 3 .
Abstract $ 0 $ 3 $ 2 $ An evaluative argument is presented in the context of a decision task and measures related to its effectiveness are assessed .
Conclusion_and_Future_Work $ 6 $ 211 $ 6 $ Another factor that affects the onfission of information is the trade off between accuracy and conciseness .
OUT $ -1 $ 186 $ 132 $ The overlap/TERM of/TERM the/TERM predicates/TERM ( overlap henceforth ) of two sentences is the/DEF maximum set of predicates that can be used as part of the logical form in both sentences ./O
Collaborative_Agents $ 1 $ 103 $ 99 $ " "What is the workPhone?
Social_Goals_and_Conversational $ 3 $ 44 $ 0 $ In this section , we exploit the framework described above to model the complex dynamics of goals and social preferences that underlies examples like [1] .
Introduction $ 1 $ 3 $ 0 $ As conversational systems move from the realm of science fiction and research labs into people ' s everyday life , and as they evolve from the plain , systemdirected interactions ~ la press or say one of socalled interactive voice response systems based on isolated-word recognizers and fixed-menu navigation , to the more open , mixed-initiative dialogues carried out in spoken dialogue systems based on large-vocabulary continuous speech recognizers and flexible dialogue managers ( see , e . g . , ( Allen et al . , 1996 ; Denecke , 1997 ; Walker et al . , 1998 ; Rudnicky et al . , 1999 ; Zue et al . , 2000 ) ) , the overall experiential quality of the human-computer interaction becomes increasingly important .
Background $ 2 $ 115 $ 95 $ Speech acts have no propositional content , thus in the context of the current dialogue history and the state of the application description , they are translated into dialogue primitives , which have content , for example , the name of a parameter and a potential value for this parameter.
Context-dependent_Lexicons $ 3 $ 84 $ 1 $ In this section , we will attempt to add more contextual information to approximate P ( t i/G~ ) .
Learning_Algorithms_Tested $ 2 $ 50 $ 30 $ Decision/TERM Lists/TERM were one/DEF of the most successful systems on the 1st Senseval competition for WSD (/O Kilgarriff and Rosenzweig , 2000 ) .
Towards_building_a_parallel_corpus $ 2 $ 36 $ 4 $ At the elementary unit level , the correspondence between Japanese sentence ( 4 ) and its English translation ( 6 ) can be represented as in ( 7 ) , where jC-e/TERM denotes the/DEF fact that the semantic content of unit j is realized fully in unit e ;/O jD-e/TERM denotes the/DEF fact that the semantic content of unit e is realized fully in unit ./O
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 87 $ 52 $ Domain/TERM dependency/TERM of/TERM words/TERM is a/DEF measure showing how greatly each word features a given set of data ./O
Abstract $ 0 $ 76 $ 15 $ is a suggestion : REINTERPRET_data.structures/TERM like ( 5 ) as compatible/DEF with descriptions of collections as well as singletons ./O
Background : _The_STOP_System $ 2 $ 68 $ 51 $ Again this is a standard expert-system technique for KA.
_Annotation_Guidelines $ 4 $ 88 $ 0 $ In general , Chinese/TERM phrases/TERM can roughly be classified/DEF into five categories , i.e. , subpredicate , verb-object , modifier-center , verbcomplement , and coordinate ./O
Abstract $ 0 $ 4 $ 0 $ This paper describes a dialog helpsystem which advises users in using computer facilities and software applications provided by the Center for Information and Multimedia Studies , Kyoto University .
Abstract $ 0 $ 3 $ 2 $ The structure reflects the choices made by the author during the top-down stepwise refinement of the document under control of a DTD grammar .
Analysis_module $ 2 $ 45 $ 5 $ In pronominal anaphora resolution in both the Spanish and English languages , the system has achieved an accuracy of 84% and 87% respectively .
OUT $ -1 $ 86 $ 68 $ A PROPER__NOUN/TERM is defined as a/DEF noun phrase in which all words are capitalized ./O
_Rare_w~_contains_a_hyphen $ 9 $ 78 $ 23 $ Model Overall Unknown Word Accuracy Accuracy Baseline , 96.72 % 84.5 % J Ratnaparkhi 96.63 % 85.56 % ( 1996 ) Table 3 Baseline model performance This table also shows the results reported in Ratnaparkhi ( 1996 : 142 ) for COnvenience .
_Proposed_method $ 2 $ 46 $ 16 $ In WordNet 1.6 , bank has 10 senses , the 3 topmost frequent senses are : I. a financial institution that accepts deposits and channels the money into lending activities 2. sloping land ( especially the slope beside a body of water ) 3. a supply or stock held in reserve especially for future use ( especially in emergencies ) shore has two senses listed : 1. the land along the edge of a body of water ( a lake or ocean or r/vet ) 2. a beam that is propped against a structure to provide support One would expect that the distance between sense #2 of bank and sense #1 of shore to be smaller than the latter's distance from the other two senses of bank.
Experimental_Results $ 4 $ 131 $ 12 $ The alignment form D is the best .
_Instrumentalists_not_including_string_players $ 8 $ 87 $ 75 $ In EWN , each monolingual database is linked , via CrossLanguage equivalence relations , to the InterLingual/TERM Index/TERM (/TERM ILI/TERM )/TERM which is the/DEF superset of all concepts occurring in all languages ./O .
System_Overview $ 2 $ 37 $ 11 $ A bottom-up algorithm then constructs a lattice that encodes the strings represented by each level of the derivation tree .
Introduction $ 1 $ 10 $ 1 $ Resolving the ambiguity of words is a central problem for large scale language understanding applications and their associate tasks ( Ide and V4ronis , 1998 ) , e . g . , machine translation , information retrieval , reference resolution , parsing , etc .
Experimental_Results $ 5 $ 154 $ 4 $ The second application is the restaurant query system illustrated in Figure 1.
Error-driven_Learning $ 4 $ 120 $ 3 $ the lexicon : F~ ( e i ) = F : rr°r ( e i ) o+Ao Here , F/TERM (/TERM el/TERM )/TERM is the/DEF chunking error number of the lexical entry e i for the old lexicon r~ Error / x and/O r~/TERM ,/TERM +~/TERM te/TERM i/TERM )/TERM is the/DEF chunking error number of the lexical entry e i for the new lexicon + AO where/O e~/TERM A~/TERM is the/DEF list of new lexical entries added to the old lexicon ~/O ) .
Phrase-Representation_Summarization $ 1 $ 25 $ 3 $ To avoid the burden of reading such long and complex sentences , we have developed the phrase-representation/TERM summarization/TERM method/TERM , which represents/DEF the outline of a document by a series of short and simple expressions ( "phrases" ) that contain key concepts ./O
Abstract $ 0 $ 2 $ 0 $ The Maximum/TERM Entropy/TERM principle/TERM (/TERM ME/TERM )/TERM is an/DEF appropriate framework for combining information of a diverse nature from several sources into the same language model ./O
Conclusions_and_Future_Work $ 5 $ 272 $ 16 $ When human-generated ground truths are available , perhaps some combination of recall and the content-based measures could be used .
Evaluation $ 3 $ 114 $ 23 $ For both BHT and LLR there was an increase in FNs at high frequencies , and an increase in FPs at medium and low frequencies , when compared to MLE.
Experiments $ 5 $ 105 $ 22 $ We believe that this is the main factor of the significant difference in performance .
Dependency_Analysis_using $ 3 $ 67 $ 4 $ Statistical/TERM dependency/TERM structure/TERM analysis/TERM is defined as a/DEF searching problem for the dependency pattern D that maximizes the conditional probability P ( DIB ) of the in20 put sequence under the above-mentioned constraints ./O
Discourse_coherence_and $ 1 $ 22 $ 14 $ In many NLG systems , aggregation/TERM is a/DEF post planning process whose preferences are only partially taken into account by the text planner ./O
Dialogue_Management $ 2 $ 46 $ 2 $ Because we consider the model of natural human reasoning as one of the important components in attaining naturalness of dialogue as a whole , we will discuss our model of reasoning in some detail .
Theoretical_Ideas , $ 2 $ 57 $ 32 $ 3.1 Levels of Representation The real PSA/TERM is a/DEF miniature robot currently being developed at NASA Ames Research Center , which is intended for deployment on the Space Shuttle and/Or International Space Station ./O
Knowledge_base $ 5 $ 103 $ 17 $ For each phrase in the user question , the most similar phrase in the KU case part is looked for based on the following criteria : • Matching of content words : 3 points • The second or more matching of content words ( when the phrase contains two or more content words ) : 1 point 144 3+0+1+1= -5 3+0+1+1=5 The user question ( The maximum matching score : 15 ) A knowledge uait ( The maximum matching score : 20 ) The c~tainty score ( 5+5+5 : 15 x 20 x 100 = 75 ( % ) Figure 2 : Matching of the user question and a knowledge unit .
The_Generation_System $ 3 $ 110 $ 68 $ The keyword SEQ/TERM specifies/DEF that what follows it is a list of words in their correct linear order ./O
Introduction $ 1 $ 77 $ 73 $ We will also discuss how the backtracking can be partly avoided taking into account some properties of the algorithm , and using a minimum constraint propagation technique , 233 3 . 3 The semi-reeursivealgorithm • ( Danlos , 1996 ) emphasizes on the problems tied to the use of a recursive depth-first algorithm in the area of text generation .
Results $ 5 $ 133 $ 37 $ ture hyperspace the number of class boundaries to be learned per bit function reduces .
OUT $ -1 $ 29 $ 29 $ Our users rely on machine generated summaries ( single document , either generic or query-based , with user adjustment of compression rates ) to judge relevance of full documents to their information need.
Introduction $ 1 $ 12 $ 5 $ Other KA techniques used in the past for building NLG systems include letting domain experts specify content rules in pseudo-code ( Goldberg et al . , 1994 ) and ethnographic techniques such as observing doctors and patients in real consultations ( Forsythe , 1995 ) .
Introduction $ 1 $ 24 $ 16 $ One thing , however , is constant across all articles : the argumentative aim of every single article is to show that the given work is a contribution to science ( Swales , 1990; Myers , 1992; Hyland , 1998 ) .
OUT $ -1 $ 151 $ 133 $ We also found that that the words `` this '' and `` story '' were strong indicators that the dateline is the best answer ( rules # 3 and # 4 ) .
Generation_of_Vague_Descriptions $ 4 $ 116 $ 35 $ The step from the superlative descriptions of case i to the analogous 'absolute' descriptions is a small one .
Introduction $ 1 $ 22 $ 15 $ Section 3 then defines a notion of.structural/TERM compatibility/TERM that : is weaker/DEF than isomorphism; section/O 4 shows that we can find plausible counterexamples even to this weaker formulation , and discusses why these passages occur.
Applications $ 5 $ 135 $ 2 $ We can indicate which topic is from which text or even which block of a text .
Clustering $ 3 $ 68 $ 8 $ The TF/TERM column/TERM indicates/DEF the average term frequency of a given term within the cluster ./O
Abstract $ 0 $ 3 $ 1 $ Reading/TERM comprehension/TERM tests/TERM are specifically/DEF designed to evaluate human reading skills ,/O and these require vast amounts of world knowledge and common-sense reasoning capabilities .
Introduction $ 1 $ 13 $ 5 $ Here , an event/TERM is the/DEF subject of a document itself ,/O i .e .
Verb_Frequency $ 2 $ 45 $ 0 $ Because word frequency is known to vary with corpus genre , we used the frequency differences for our target verbs as a measure of corpus difference .
OUT $ -1 $ 150 $ 143 $ GNote that , in c&~e the object marked for high interest is the person , a more abbreviated sentence construct iotl is appropriate : ' . losEP ARcos is in your neighbourh~md ' .
_Features $ 3 $ 23 $ 0 $ Nineteen linguistic features were annotated , along with information about the referent of each mention .
Eckert_and_Strube's_Algorithm $ 3 $ 55 $ 6 $ ES99 define the following *I predicates ( Eckert and Strube , 1999b ) [p. 40] : Equating/TERM constructions/TERM where a/DEF pronominal referent is equated with an abstract object ,/O e.g. , x is making it easy , x is a suggestion.
Future_Research_Issues $ 5 $ 128 $ 6 $ Inductive logic programming ( MDR94 ; Coh95 ) is a natural paradigm for this .
Levels_of_Annotation $ 4 $ 94 $ 0 $ The ADAM' s five levels of annotation were mainly chosen in consideration of their interest for practical applications of the annotated material .
Introduction $ 1 $ 10 $ 7 $ This paper describes EVIUS/TERM , a/DEF multi-concept learning system for free text that follows a multi-strategy constructive learning approach ( MCL ) ( Michalshi , 1993 ) and supports insufficient amounts of training corpora ./O
Related_Work $ 5 $ 138 $ 20 $ Our model does not use such intermediate topics , but accesses word cg-occurrence information directly aald represents a context as the accumulation of this information .
Tagged_Text $ 3 $ 115 $ 101 $ • PP/TERM rules/TERM for/TERM word-sense/TERM disambiguation/TERM : For/DEF some nouns ( propernouns ) which are the object of a preposition , the intersection of the semtype value sets of the preposition word and its object determines their semtype ./O
OUT $ -1 $ 108 $ 57 $ Bad embeddings are all those left , for example , if there is no available syntactic slot for the embedded part .
Applications $ 3 $ 67 $ 7 $ The same technique has more recently been applied to compare corpora analysed at the semantic level in a systems engineering domain and this is the main focus of this section .
The_TABULATE_ILP_Method $ 3 $ 66 $ 0 $ This section discusses the ILP method used to build a committee of logical control hypotheses for each action .
Proper_name_recognition $ 4 $ 85 $ 11 $ Besides part of speech , the only other information used by the recognizer is the lexical status of words , i.e .
Rules_as_features $ 1 $ 25 $ 17 $ fl , f2 , and f3 represent the three features , c/TERM represents the/DEF class label ./O
Conclusions_and_loose_ends $ 5 $ 192 $ 68 $ The experiment suggests that the converse of the hypothesis might also be true , in which it is claimed that expressions of the form 'the n large CN ' can not be employed to refer to the set S unless S consists of the n largest objects in D : Hypothesis ( .¢= ) : In a situation in which the domain D represents the set of percept_.. : ... ... ... . ~ ~.t.ually : relevmtt , ob_jects > a~ : -expressionof t~he .
Introduction $ 1 $ 18 $ 13 $ The SIFAS/TERM ( Syntactic/DEF Marker based Full-Text Abstraction System )/O system has been implemented to use discourse markers in the automatic summarization of Chinese ( T'sou et al.
Tagged_Text $ 3 $ 23 $ 9 $ The Partial/TERM Parser/TERM Module/TERM then takes/DEF this updated text and breaks it into phrases while attempting to lexically disambiguate the text ./O
BOV_Stem_NE_Defaults_Qspecific_41 $ 5 $ 79 $ 49 $ If the first NP/TERM ( noun-phrase/DEF ) in the sentence following the match is a pronoun , choose that sentence : Q : Why did Chris write two books of his own?
OUT $ -1 $ 13 $ 9 $ A writer expects that the checking tool should not only detect errors but also propose a CL conformable expression .
_Amanda_Huggenkiss ,_she_proposes_to_meet_you $ 2 $ 110 $ 103 $ Thus we will search in the hearer ' s discourse and physical context which are the activated instances of the concepts event , person , keyword and time/location .
Feature_Selection_and_Extraction $ 2 $ 35 $ 13 $ During keyword extraction , the document is first segmented and converted into a keyword frequency vector ( t fl , t f2 , . . . , t . f M ) , where tfi/TERM is the/DEF in-document term frequency of keyword wi ,/O and M/TERM is the/DEF number of the keyword features selected ./O
The_MATE_Markup_Framework $ 3 $ 134 $ 0 $ 3.1.1 Elements The basic/TERM markup/TERM primitive/TERM is the/DEF dement ( a term inherited from TEI and SGML ) which represents a phenomenon such as a particular phoneme , word , utterance , dialogue act , or communication problem ./O
Related_Work $ 4 $ 117 $ 26 $ Among its effects , there is the fact that B comes to know A's choice ( refused and grounded attribute are set to "true" ) .
The_interplay_of_focus_and_word $ 2 $ 69 $ 45 $ `` In our approach we derive the information necessary for the use of LP-rules from a discourse/TERM model/TERM that relates/DEF various aspects of a discourse to one another ./O
Abstract $ 0 $ 3 $ 1 $ KeyWords compares a word list extracted from what has been called 'the study corpus ' ( the corpus which the researcher is interested in describing ) with a word list made from a reference corpus .
Introduction $ 1 $ 13 $ 5 $ form WSD.
_Head_countability_preferences_of_the_head $ 7 $ 61 $ 1 $ We anticipate that this is a useful feature because singular indefinite countable nouns normally take the article a/n , whereas singular indefinite uncountable nouns normally take no article : a dog vs water .
Abstract $ 0 $ 2 $ 1 $ An XML/TERM document/TERM is a/DEF mixture of structure ( the tags ) and surface ( text between the tags ) ./O
Selective_Sampling_Evaluation $ 4 $ 96 $ 20 $ The entropy/TERM H/TERM ( V/TERM ) is the/DEF expected negative log likelihood of random variable V : H ( V ) = -EX ( logdv ( V ) ) ) ./O
The_Complexity_of_Extracting_a $ 2 $ 39 $ 27 $ • A level-0/TERM fact/TERM consists/DEF of a single node ./O
_The_straightforward_unpacking_of_feature $ 3 $ 22 $ 14 $ In the absence of information about feature relevance , this is a reasonable choice.
Results $ 6 $ 210 $ 32 $ What ..differs are the word order and/Or the position of the nucleus accent.
The_machine_learning_method $ 3 $ 71 $ 12 $ For our experiment , 3A semantic operation that converts an argument to the type which is expected by a function , where it would otherwise result in a type error .
Corpus_Building_Tools $ 7 $ 122 $ 0 $ In the experiments , various tools for transcription and annotation were used.
Middle $ 6 $ 38 $ 1 $ Of these , 5 ,922 words are polysemous/TERM , i.e. , they/DEF have more than one sense ./O
Psycholinguistic_production $ 3 $ 50 $ 15 $ When a lexical concept is activated , the mechanism of activation spread : ing ensures that ~the : : ~directly : . . ecm : nected : : lemma . . . . receives tile highest activation , and not a lemma associated with a hyperonym of the lexical concept ( which is connected by an ISA-link ) .
Accommodation_in_GoDiS $ 3 $ 68 $ 17 $ IS : PRIVATE : SHARED : PLAN : STACKSET ( AcT1ON ) AGENDA : STACK ( ACTION ) PaL : SET ( PRoP ) I BEL : SET ( PRoP ) QUD : STACK ( QUESTION ) TMP : [ SPEAKER : LU : [ MOVES : BEL : SET ( PRoP ) QUD : STACKSET ( QUESTION ) SPEAKER : PARTICIPANT LU : MOVES : PARTICIPANT ASsoCSET ( MOvE , BOoL ) ASsOCSET ( MOvE , BooL ) ] ] Figure 1 : The type of information state we are assuming This dialogue is incoherent if what is being discussed is when the child Maria is going to be picked up from her friend 's house ( at least under standard dialogue plans-that we might have for such a conversation ) .
Related_Work $ 4 $ 128 $ 37 $ As explained in Section 2.1 . , an agent ' s utility/TERM function/TERM is a/DEF weighted sum of individual utility functions , which represent the preference assume that weights Wi and Wj are set , respectively , to 20 and 10 ./O
_WSJ910702-0053_36_At_a ,_meeting_in_South_Africa_this_week ,_the_African_National_Congress ,_the_major_black $ 9 $ 189 $ 10 $ This paper presented a statistical method of generating extraction based multi-document summaries .
Perfect_Sampling $ 3 $ 44 $ 0 $ In this paper , we propose the/DEF application of another sampling technique in the parameter estimation process of the WSME model which/O was introduced by Propp and Wilson ( Propp and Wilson , 1996 ) : the Perfect/TERM Sampling/TERM ( PS/ACR ) .
Abstract $ 0 $ 144 $ 49 $ Table 3 provides a comparison of the accuracy of decision trees applied across domains compared to those constructed and evaluated within a given domain .
Introduction $ 1 $ 24 $ 16 $ Clearly , the performances of WSD systems are related to a variety of parameters , but the formal nature of these dependencies is not fully understood .
The_formula_is_valid_when_J_>_R_ ( that_is ,_the_judges $ 7 $ 97 $ 11 $ Next , five judges had to indicate for each sentence which other sentence ( s ) , if any , it subsumes s . 5 . 1 CBSU : interjudge agreement Using the techniques described in Section 0 , we computed the cross-judge agreement ( J ) for the 6 clusters for various r ( Figure 3 ) .
Abstract $ 0 $ 4 $ 2 $ The adapted algorithm is tested on four Danish dialogues from two dialogue collections and the results obtained are evaluated.
Analyzing_the_Reading $ 4 $ 128 $ 29 $ Therefore , MRAR/TERM for/TERM a/TERM reading/TERM comprehension/TERM test/TERM is the/DEF sum of the scores for answers corresponding to each question for that test ./O
Theoretical_Ideas , $ 2 $ 120 $ 95 $ There is one script/TERM interpreter/TERM , which functions/DEF both as a script executive and a script evaluator ,/O and one set of rules/TERM which defines/DEF the procedural semantics of script actions ./O
Definitions $ 2 $ 32 $ 7 $ Corpus/TERM : A/DEF corpus is an ordered set of strings ./O
Systems_Foundations $ 3 $ 29 $ 5 $ MIMIC/TERM " provides/DEF movie listing information involving knowledge about towns , theaters , movies and showtimes ,/O as demonstrated in Figure 1 .
_Our_Classification_Algorithm_for $ 2 $ 65 $ 1 $ In linguistics , telicity/TERM is a/DEF phase feature used in classifying ./O
The_Acquisition_of_Word_Order $ 3 $ 111 $ 0 $ We are investigating the acquisition of word order , which reflects the underlying order in which constituents occur in different languages .
Architecture_of_WIT-Based_Spoken $ 3 $ 34 $ 0 $ Dialogue Systems Here we explain how the modules in WIT work by exploiting domain-dependent knowledge and how they interact with each other.
Extraction_and_Maintenance_of $ 3 $ 65 $ 11 $ The top/TERM object/TERM is a/DEF move with two roles : A source location ( which is a city Hanover ) , and a departure time ( which is a date day 1 ) ./O
Learning_validation_and_results $ 4 $ 148 $ 10 $ The theoretical/TERM generality/TERM of a generalized clause is the/DEF number of not generalized clauses ( E + ) that this clause can cover ./O
Background $ 2 $ 122 $ 102 $ The system can respond to the inform with a sub-dialogue : s Dialogue ( sys ) --~ ( request ( sys ) + Inform ( usr ) ) * Inform ( usr ) --+inform ( usr ) + [Dialogue ( sys ) ] The dialogue history reflects all previous negotiations ( here : task theatreinfo ) .
Performance $ 4 $ 176 $ 13 $ Figure 8 is similar but has the additional overhead of around 2/3 of all files having hits where Figure 7 has only 1/20 .
Subcategorization_Frequency $ 3 $ 99 $ 43 $ The verb ' fight " is the only verb that has a different transitivity bias in each of the three corpora; with all other verbs , at least two corpora share the same bias .
Results $ 5 $ 69 $ 24 $ tags chosen by linguists would score very badly against another without this implying any fault as there is no 'gold standard'.
Abstract $ 0 $ 7 $ 5 $ Introduction With the rapid growth of electronic documents and the great development of network in China , there are more and more people touching the Internet , on which , however , English/TERM is the/DEF most popular language being used ./O
_Pronominalise_the_Cb_only_after_a_Continue $ 4 $ 46 $ 7 $ t~rarisi~ ' : '' : : ' : the : sHng'~ : is : p redicted : Am : its : A°cal ' : c°ntext~ but tions between consecutive sentences in a disthe RETAIN is not ; whenever RETAIN is a cheap course segment and favouring sequences which maintain the same center .
Related_work_in_content_aggregation $ 3 $ 158 $ 4 $ The definition of aggregation/TERM that we gave at the beginning of previous section is similar/DEF to those provided by Dalianis and Huang , although it focuses on common feature factorization to insure aggregation remains a proper subset of sentence planning ./O
Bridging_Natural_Language_and $ 4 $ 110 $ 31 $ Furthermore , matching trees and sub-trees is a computationally intensive task , especially since full linguistic parse trees may be relatively deep.
Abstract $ 0 $ 76 $ 15 $ REINTERPRET_data.structures/TERM like ( 5 ) as compatible/DEF with descriptions of collections as well as singletons ./O
Abstract $ 0 $ 3 $ 0 $ When a lexical item is selected in the language production process , it needs to be explained why none of its superordinates gets selected instead , since their applicability conditions are fulfilled all the same .
Introduction $ 1 $ 25 $ 16 $ In the first case the representation theory is first order logic without structural rules , the formal learning theory from a logical point of view is inductive substructural logic programming and an example of a learning strategy in this framework is EMILE/TERM , a/DEF learning algorithm that learns categorial grammars (/O Adriaans , 1992 ) .
Proof_of_concept_generator $ 4 $ 144 $ 11 $ The output text is a sub-task including a title and instructions of different types ( only the first three instructions are given in the Figures ) to be performed by the same person ( e . g .
Introduction $ 1 $ 23 $ 16 $ ( 1995 ) and Beale ( 1997 ) .
EXOT $ 9 $ 94 $ 22 $ Therefore any frequency based measures defined by Boyd et al .
Text_Summarization $ 1 $ 25 $ 5 $ In regard to the position method , Hovy and Lin ( 1997 ) considered the title is the most likely to bear topics .
Pre-processing_design $ 2 $ 35 $ 0 $ Input pre-processing is essential in an embedded real time system , in order to simplify the core processing and make it both timeand memoryeffective.
Automatic_Argumentative $ 3 $ 95 $ 21 $ We use a manually created lexicon for patterns for agents , and a manually clustered verb lexicon for the verbs .
Introduction $ 1 $ 24 $ 15 $ The rest of this paper is organized as follows : Section 2 describes the initial candidate grammar and the operators used to generate new candidate grammars from any given one .
Previous_Research $ 2 $ 27 $ 5 $ For Korean , in one statistical method , ( Lee and Ahn , 1996 ) indexed general Korean nouns using n-grams without linguistic knowledge and the experiment results showed that the proposed method might be Mmost as effective as the linguistic noun indexing.
HMM-based_Chunk_Tagger $ 1 $ 62 $ 29 $ The basic idea of representing the structural tags is similar to Skut and Brants ( 1998 ) and the structural/TERM tag/TERM consists/DEF of three parts : 1 ) Structural relation ./O
Experimental_results $ 3 $ 154 $ 100 $ Further analysis shows that the new lexicon brings positive effect to 10 queries and negative effect to 4 queries .
Degree_of_Polysemy_in_Mandarin_Chinese $ 2 $ 28 $ 0 $ The degree/TERM of/TERM polysemy/TERM is defined as the/DEF average number of senses of words ./O
OUT $ -1 $ 187 $ 133 $ The predicates in boldface in the two examples above indicate the overlap with the ideal answer : 3 for ( 1 ) , and 2 for ( 2 ) .
Learning_Algorithms_Tested $ 2 $ 31 $ 11 $ For k ' s greater than 1 , the resulting sense is the weighted majority sense of the k nearest neighbours --where each example votes its sense with a strength proportional to its closeness to the test example .
Abstract $ 0 $ 135 $ 40 $ The accuracy of the decision trees was 80.45% for Encarta and 78.36% for the Wall Street Journal.
Introduction $ 1 $ 33 $ 21 $ We assert that these N-V links are especially relevant for index expansion in IR systems ( Fabre and S~billot , 1999 ) , and what we call a relevant/TERM N-V/TERM pair/TERM afterwards in the paper is a/DEF pair composed of a N and a V which are related by one of the four semantic relations defined in the qualia structure in GL ./0
The_remaining_pronouns_are_not_in_third_person_or $ 9 $ 274 $ 1 $ J0 For instance : " all the pronouns in third person and singular whose antecedents are proper nouns have boon translated into he ( antecedent with masculine gender ) or she ( antecedent with feminine gender ) ; otherwise they have been translated into it " .
Learning_Algorithms_Tested $ 2 $ 38 $ 18 $ A key point that allows a fast learning is that the winnow nodes are not connected to all features but only to those that 1The Winnow algorithm ( Littlestone , 1988 ) consists of a linear threshold algorithm with multiplicative weight updating for 2-class problems.
MALIN $ 4 $ 209 $ 103 $ 127 Status : Stops : Su~es8 Name : Id : Name : Id : Name : Id : Cen~.~rum ] `` Snickareg .
Recognition $ 9 $ 130 $ 5 $ In Figure 6 , acceptable/TERM is the/DEF sum of perfect and ok scores ,/O s Figure 6 shows the results of the intra-site and inter-site evaluations.
_The_session_was_also_attended_by_observers_from_the_following_international_organizations : $ 7 $ 164 $ 113 $ In this context , there is no clue for deciding whether Ashley Judd is a star or an athlete .
Implementation $ 4 $ 329 $ 158 $ If an audio media object occurs in a frame , the duration of all media objects in that frame is equal to the length of all the audio files in the segment .
Introduction $ 1 $ 12 $ 6 $ To illustrate the operation of BIAS and its rebuttal capability , consider the exchange in Figure 1 , which consists of a preamble that contains background information~ followed by an argument generated by BIAS , a user 's rejoinder and BIAS ' rebuttal .
The_Refinement $ 5 $ 87 $ 31 $ Tag Word left right VBG operating NN VBG operating IN NN VBG operating VBD NN Table 4 : Superfluous rules.
Discourse_coherence_and $ 1 $ 28 $ 20 $ For example , comparing the first two descriptions of . a necklace in Figure 1 , 2 is less coherent than 1 because of the shifting from the description of the necklace to that of the designer , which is a side effect of embedding .
Problem_Space_Modeling $ 2 $ 40 $ 13 $ Remedy/TERM is the/DEF template that is used to generate natural language responses and explanations corresponding to a particular goal ./O
_KNOWLEDGE_EXTRACTION $ 3 $ 59 $ 1 $ One is the CKIP Chinese lexicon and another is the Chinese text from WWW.
Abstract $ 0 $ 30 $ 5 $ I believe that the strategy followed by ES99 is a good starting point for investigating how far one can go in resolving individual and abstract anaphors in dialogues on the basis of the local contexts in which the anaphors occur .
Architecture_and_Principles $ 3 $ 35 $ 12 $ Candidate NEs Initializers and Candtdate ate~ Index pages I Filter I Typed NEs Figure 1 : Architecture trieved through one of the strings ( 1 . a-d ) 3 The first collection is an enumeration and consists of a coordination of three NEs .
_Otherwise ,_add_to_the_current_context_new $ 6 $ 210 $ 127 $ At first glance it appears that premise to goal is the preferred argmnentation strategy.
Unless_necessary ,_we_don ' t_list_relationship_in_the $ 3 $ 86 $ 28 $ Let , _o=Po , Zn-1 be the set of ( n-1 ) -leveI graphs , suppose Pn~ ( , . ouZiu . . . . , Za4 ) , ( PnnXn . 1 ) ¢NIL , En/TERM (/TERM C-Pn/TERM )/TERM is the/DEF set of the edges between points in P~ ,/O Rn/TERM (/TERM C/TERM (/TERM P=x/TERM En/TERM )/TERM )/TERM is the/DEF set of relations between points in P= and edges in En ,/O then : v ) vi ) vii ) viii ) < P~ , E~ , Rn/TERM is a/DEF n-level compositional graph ;/O n-level/TERM concepts/TERM comprise n-level/DEF compositional graphs , n-level point-headed graphs , and n-level edge-headed graphs ./O
SYSTEM : _U_S-air_flight_63_57_departs_la $ 9 $ 71 $ 64 $ Another example is the following : ( i ) there don ' t seem to be any nonstop flights from san francisco to newark new jersey on united which serve breakfast and depart after nine A M on february tenth .
Maximum_Entropy_Modeling $ 2 $ 47 $ 5 $ Maximum/TERM entropy/TERM is a/DEF technique for automatically acquiring knowledge from incomplete information , without making any unsubstantiated assumptions ./O
MALIN $ 4 $ 253 $ 147 $ The Domain/TERM Knowledge/TERM Manager/TERM is functional/DEF utilising a Spatial Reasoner for one sub-area of OstergStland and a Temporal Reasoner ./O
October_2000 $ 7 $ 13 $ 12 $ However it is not clear whether , or where , it is a good measure , and there is some evidence that it does not match our intuitions ( Kilgardff and Rose 1998 , Kilgarriff in press ) .
OUT $ -1 $ 22 $ 18 $ Technical writers find it difficult to comply with the writing rules of a CL which are often hard to justify ( CLA.
Building_Spoken_Dialo~te_Systems $ 4 $ 149 $ 44 $ Below is an example of a phase definition .
Abstract $ 0 $ 2 $ 1 $ The English WoRDNET and its aligned Italian version , MULTIWORDNET , both augmented with domain labels , are used as the main information repositories.
Hyperonymy_in_lexical_semantics $ 5 $ 131 $ 34 $ 2 • Given an activated concept , which more general lexical items are considered in tile choice process ; are there any restrictions on . -lexical inheritance-? . . . . . . . . . o How is the eventual choice from the set of candidate lexical items being made?
OUT $ -1 $ 21 $ 18 $ Texts consists of Spanish descriptions of specimens .
Issues_and_proposals $ 3 $ 76 $ 33 $ A better analysis would be to mark a question-answer relation between utterances 2 and 3 , and a motivation relation between utterance 1 and the unit consisting of utterances 2 and 3 .
Introduction $ 1 $ 10 $ 4 $ Robust parsing is often achieved by combining a full parser with a partial parser and fragment-combining rules , but even then some utterances may be correctly recognized , only to be parsed incorrectly or not at all .
Introduction $ 1 $ 9 $ 3 $ This is the reason for many researchers having focused on massive acquisition of lexical knowledge and semantic information from pre-existing lexical resources as automatically as possible .
OUT $ -1 $ 22 $ 19 $ There is a rich variety of colour descriptions including basic colours , intervals , changes , etc .
_KNOWLEDGE_EXTRACTION $ 3 $ 110 $ 52 $ For the keywords of length 3 ,4 , and 5 , each keyword is divided into two parts X and Y. X/TERM is a/DEF candidate of proper name and/O 17 Y/TERM is a/DEF candidate of organization type ./O
Acquisition_Process $ 4 $ 142 $ 43 $ Unfortunally the stem reference contained in the ontology points to the concept ale , which is a sub concept of alcoholic beverage ) , in this case the stem reference is reassigned to the dictionary concept .
Algorithms_and_Implementation $ 2 $ 46 $ 26 $ The only parameters that are available in the current version are the maximum number of iterations and a value frequency threshold which is set to 2 by default ( values occurring only once are not taken into account ) .
Introduction $ 1 $ 12 $ 5 $ NLG researchers have addressed this issue in various ways , but everyone assumes some kind of structural compatibility between rhetorical structure and text structure .
Collaborative_Agents $ 1 $ 101 $ 97 $ " "What is the eemail address?
Summary_and_Future_Work $ 5 $ 206 $ 3 $ We believe that this is a direct consequence of the use of the dialogue control table .
Building_Spoken_Dialo~te_Systems $ 4 $ 125 $ 20 $ Each definition is a pair comprising a network name and a set of phrase category names .
Information_extraction $ 3 $ 65 $ 30 $ For instance , the third clause means : "Text A deals with imports if it contains a sentence with a subject composed by a NP containing a persona giuridica , the verb of the main sentence is interessare ( to interest ) in finite affirmative mood , and the indirect object consists of a PP containing the word importazione".
The_Classifiers $ 3 $ 98 $ 61 $ Learning/TERM : Once/DEF the search ends , the weight vectors w~ and w~ are updated accordingly ./O
Empirical_Evaluation $ 4 $ 150 $ 39 $ Instead , kNN/TERM performs/DEF online scoring to find the training patterns that are nearest to a test pattern and makes the decision based on the statistical presumption that patterns in the same category have similar feature representations ./O
Interlingua_system_ ( ISS ) $ 3 $ 82 $ 13 $ 4 A clause/TERM could be defined as " a/DEF group of words containing a verb "/O .
Introduction $ 1 $ 36 $ 27 $ Then in section 4 we will show that the learnability result of Valiant for k-CNF boolean concepts can be transformed to a learnability result for a grammar of string patterns denoted by a substructural variant of the k-CNF formulas .
The_Refinement $ 5 $ 78 $ 22 $ ( 3 ) [the_DT reawakening_VBG] of_IN [the_DT abortion-rights_NNS movement_NN] Generalisation/TERM consists/DEF of accepting some sequences of elements which do no correspond to a whole structure ./O
OUT $ -1 $ 0 $ 0 $ An Algorithm for Situation Classification of Chinese Verbs Xiaodan Zhu , Chunfa Yuan State Key Laboratory for Intelligent Technology and System , DepL of Computer Science & Technology , Tsinghua University , Beijing 100084 , P.R.C.
Implementation $ 4 $ 180 $ 9 $ Document # 3 : Mike/TERM Smith/TERM is a/DEF programmer for XYZ Corporation ./O
OUT $ -1 $ 0 $ 0 $ Verb Subcategorization Frequency Differences between BusinessNews and Balanced Corpora : The Role of Verb Sense IDouglas Roland , ~"Danid Jurafsky , "3Lise Menn ,'Susanne Gahl , IElizabeth Elder and IChris Riddoch ~Department of Linguistics , 2Department of Computer Science , 3Institute of Cognitive Science University of Colorado Boulder , CO 80309-0295 { douglas.roland , jurafslQ1 , lise.menn , elizabeth.elder , christopher.b.riddoch } @colorado.edu 'Department of Linguistics Harvard University Cambridge MA 02138 sgahl @ fas.harvard.edu
Discussion $ 4 $ 108 $ 2 $ A marked difference is that in RISE , the rules are the instances in kNN classification ( and due to the careful general±sat±on strategy of RISE , they can be very instance-specific ) , while in RBM , the rules are the features by which the original instances are indexed .
Introduction $ 1 $ 41 $ 33 $ Subjectivity is a property which is related to the attribution of authorship as well as to author stance , but it is just one of the dimensions we consider .
Experiment $ 3 $ 70 $ 2 $ The verbs were chosen at random , subject to the constraint that they occurred frequently enough in corpus data 4 and when applicable , represented different sub-classes of each examined Levin class .
Word_Clustering $ 3 $ 57 $ 13 $ Then as in ( Rissanen , 1996 ) , the SC value of w TM relative to a model I in which the presence or absence of w is independent from those of s ( i.e. , a Bernoulli model ) , is calculated as SC ( w TM : I ) = mH + ~ log ~ + log 7r , where m + denotes the number of l ' s in wm .
Introduction $ 1 $ 11 $ 4 $ In particular we are interested in one-pass decoding and translation of speech as opposed to the more prevalent approach of translation of speech lattices .
Grammar_Induction $ 3 $ 74 $ 13 $ The learning algorithm we use is a variant of the Inside/Outside/TERM algorithm/TERM that induces/DEF grammars expressed in the Probabilistic Lexicalized Tree Insertion Grammar representation (/O Schabes and Waters , 1993; Hwa , 1998 ) .
_Introduction $ 1 $ 26 $ 19 $ The algorithm assumes the availability of a word sense inventory in one of the languages .
Background : _The_STOP_System $ 2 $ 48 $ 31 $ For people who answered Not Sure or Yes , we looked at their decisional/TERM balance/TERM , that is the/DEF number of likes and dislikes they had about smoking ,/O and placed them into Lacks Confidence if their dislikes clearly outnumbered their likes , and Classic Precontemplator otherwise.
Generating_naive_rules $ 2 $ 34 $ 1 $ The details of integrating induction into chart parsing have been described in ( Cussens and Pulman , 2000 ) , here we give just a brief account .
_User : _Ok $ 9 $ 211 $ 25 $ Furthermore , the system allows automated sub-goal utility adjusmaent based on history of interactions with groups of users .
The_hyperonym_problem $ 2 $ 19 $ 1 $ Roelofs [1996 , p. 308] describes a 'lemma'/TERM as a/DEF representation of the meaning and the syntactic properties of a word ,/O and the task of lemma/TERM retrieval/TERM as a/DEF crucial step in the process of grammatical encoding ,/O where buildsituations of utterance.
Clustering_into $ 5 $ 75 $ 6 $ Speech and language technology researchers have used wordbigram and n-gram models in speech recognition , and variants of PoS-bigram models for Part/Of-Speech tagging .
Experimental_Results $ 4 $ 211 $ 12 $ When two evaluation measures produce nearly the same ranking of the summary set , the rank correlation will be near 1 and a scatterplot of the two rankings will show points nearly lying on a line with slope 1 .
Further_Research $ 6 $ 193 $ 20 $ KSD is a long neglected area of research .
Centroid-based_summarization $ 4 $ 80 $ 8 $ Ci/TERM is the/DEF centroid score of the sentence ,/O P~/TERM is the/DEF positional score of the sentence ,/O and F~/TERM is the/DEF score of the sentence according to the overlap with the first sentence of the document ./O
Learning_Frameworks $ 2 $ 28 $ 8 $ For example , when using Hidden/TERM Markov/TERM Model/TERM ( HMM/ACR ) as a generative model for pos tagging , estimating the probability of a sequence of tags involves assuming that the pos tag ti of the word wi is independent of other words in the sentence , given the preceding tag ti-1 .
Abstract $ 0 $ 3 $ 2 $ The design of ADAM architecture is compatible with as many practices of dialogue annotation as possible , as well as approaches to annotation at different levels.
Introduction $ 1 $ 13 $ 6 $ where appropriate target language lexical items are chosen for each source language lexical item and ( b ) reordering phase where the chosen target language lexical items are reordered to produce a meaningful target language string .
SYSTEM : _U_S-air_flight_63_57_departs_la $ 9 $ 35 $ 28 $ Part ( i ) of the system response summarizes the most salient constraints of the user input using the summary script of section 5 s. Part ( 2 ) is a specification of the significant information common to all flights.
Generation_of_Crisp_Descriptions $ 3 $ 67 $ 13 $ ) • 1Note that C contains r , unlike Dale and Reiter ' s ' contrast set ' C , which consists of those elements of the domain from which r is set apart .
Word_Co/Occurrence_Vector $ 2 $ 32 $ 0 $ 2.1 Word-Document Co/Occurrence Matrix Word co/Occurrences are directly represented in a matrix whose rows correspond to words and whose columns correspond to documents ( e.g.
OUT $ -1 $ 41 $ 37 $ Each textual module has a particular communicative goal and a precise theme according to the ATA 100 norms .
Description_of_the_algorithm $ 2 $ 14 $ 0 $ There are four main steps in our approach .
The_CGS_system $ 4 $ 72 $ 4 $ The input to CGS is a picture representation ( graphical elements and its mapping from the data set ) generated by SAGE plus its complexity metric .
Word_Sense_Dis~mbiguation $ 4 $ 198 $ 106 $ This step tries to identify words from SAW which are linked at a distance of maximum 1 with the words from SDW .
Abstract $ 0 $ 2 $ 0 $ IF ( Interchange Format ) , the interlingua/TERM used/TERM by/TERM the/TERM C-STAR/TERM consortium/TERM , is a/DEF speech-act based interlingua for task/Oriented dialogue ./O
Extending_Domain_Semantics_for $ 5 $ 175 $ 18 $ He is the Head of Division and is a professor .
INTRODUCTION $ 1 $ 25 $ 19 $ The third difficulty is the problems of ambiguities , such as structure ambiguities , syntactic ambiguities and semantic ambiguities .
Middle $ 6 $ 56 $ 19 $ A word/TERM token/TERM is an/DEF occurrence of a type in the corpus ./O
Introduction $ 1 $ 30 $ 23 $ The Penn/TERM Treebank/TERM for example consists/DEF of trees with an additional coindexation relation ,/O Negra allows crossing branches and in Verbmobil , an element ( a tree-like structure ) in the corpus might contain completely disconnected nodes.
The_NJFun_System $ 2 $ 82 $ 68 $ Finally , " history/TERM " represents/DEF whether NJFun had trouble understanding the user in the earlier part of the conversation ( bad=0 , good=l ) ./O
Abstract $ 0 $ 29 $ 27 $ This IF representation is neutral between sentences that have different verbs , subjects , and objects such as A double room costs 150 dollars a night , The price of a double room is 150 dollars a night , and A double room is 150 dollars a night .
Evaluation_Measures $ 2 $ 38 $ 13 $ The ranks assigned by an evaluation measure produce equivalence classes of extract summaries; each rank/TERM equivalence/TERM class/TERM contains/DEF summaries which received the same score ./O
Abstract $ 0 $ 1 $ 0 $ RSTTool/TERM is a/DEF graphical tool for annotating a text in terms of its rhetorical structure ./O
The_steps_in_the_strategy_are_marked_with_the $ 7 $ 154 $ 32 $ Finally , ( Klein 1994 ) is the previous work most relevant to our proposal .
Note_that_the_nouns_at_this_point_are_types_not $ 4 $ 140 $ 25 $ For all the experiment conditions , the noun instances that were excluded from the tag set and were in the test set were sense tagged using the default baseline of 67.6% , in order to report the results at 100 % coverage for the test set , the results of which are presented in table 2 below .
Enriching_the_Feature_Set_with $ 2 $ 73 $ 0 $ 2.2 Making Measures of Association Available to the Parser To make the measures of association available to the parser , we started by discretizing each measure , that , is substituting for each continuous measurement a set of binary predicates coarsely describing its approximate value.
Abstract $ 0 $ 180 $ 38 $ This first grammar is fully eqivalent to a XML/TERM DTD/TERM that describes/DEF the structure of a notice ,/O though it distinguishes finer-grained units 1hart traditional l ) TI ) s tends to do.
The_link_between_~i~l~ ( /guniang/ ,_girl ) _and $ 7 $ 106 $ 0 $ ~b~/TERM ( /waimao/ , appearance ) denotes a/DEF relation between concepts and relationships ./O
Concepts $ 2 $ 55 $ 30 $ Thus these clusters of adjectives have great possibility to be combined into one cluster , while the ordinary hierarchical clustering algorithm can not do it .
OUT $ -1 $ 189 $ 63 $ Agent is a strong feature beating both baselines.
Overall_System_Architecture $ 3 $ 48 $ 0 $ The compound/TERM noun/TERM indexing/TERM system/TERM proposed in this paper consists/DEF of two major modules : one for automatically extracting compound noun indexing rules ( in Figure 1 ) and the other for indexing documents , filtering the automatically generated compound nouns , and weighting the indexed compound nouns ( in Figure 2 ) ./O
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 238 $ 203 $ ~Doc/TERM denotes the/DEF number of documents ./O
System_Overview $ 3 $ 47 $ 11 $ X/TERM ° is the/DEF head of X m and the anchor of the etree ./O
OUT $ -1 $ 82 $ 82 $ Underpinning much work in summarization is the view that summaries are time savers .
Our_approach_to_Multilingual $ 2 $ 50 $ 23 $ This nlicro-structure can in general be dealso different styles or modes of communication , lerlniued by studying a corpus of documents and by 25 exposing the structure of choices that distinguish a given document from other documents in this class .
Abstract $ 0 $ 17 $ 5 $ Novice users may have a specific information topic , but due to little or no training in search and retrieval , they don ' t know how to make best use of the available operators and tools .
_Instrumentalists_not_including_string_players $ 8 $ 44 $ 32 $ Next section describes a clustering strategy that adequates to the Information Retrieval criterion : cluster senses if they tend to co-occur in the same Semcor documents .
The_hyperonym_problem $ 2 $ 28 $ 10 $ Hence , if A is the correct lemma , B will ( also ) be retrieved.
Using_CST_for_information_fusion $ 5 $ 155 $ 18 $ In the example , the shaded area represents the summary/TERM subgraph/TERM G " of G that contains/DEF all four cross-document links and only these nodes and edges of G which are necessary to preserve the textual structure of G ' ./O
TMethod $ 2 $ 78 $ 33 $ The binomial log-likelihood ratio test is simple to calculate .
Discourse_coherence_and $ 1 $ 42 $ 34 $ ( Cheng , 1998 ) describes interactions that need to be taken into account in aggregation .
OUT $ -1 $ 20 $ 19 $ Then , we provide details on the measures of argument strength and importance used in selecting and ordering argument support .
Abstract $ 0 $ 201 $ 31 $ The value in column ' Quality ' is the average acceptability for the abstract .
OUT $ -1 $ 0 $ 0 $ From Context to Sentence Form Sabine Geldof Artificial Intelligence Laboratory Vrije Universtiteit Brussel Pleinlaan 2 , 1050 Brussels sabine@arti , vub.ac.be
Abstract $ 0 $ 37 $ 1 $ In terms of this distinction , it is the AbsDocRep that is specified during text planning; graphical markup can be deferred to a later formatting stage.
_Rare_w~_contains_a_hyphen $ 9 $ 64 $ 9 $ The model was trained and tested on the part/Ofspeech tagged WSJ section of the Penn Treebank .
Identifying_Phrase_Structure $ 1 $ 9 $ 1 $ Given an input string O =< ol , 02 , . . . , On > , a phrase/TERM is a/DEF substring of consecutive input symbols oi , oi+l , . . . ,oj ./O
Architecture $ 3 $ 61 $ 10 $ Our architecture consists of four main modules ( see figure 2 ) .
_Relations_that_are_relevant_and_correct_in $ 3 $ 198 $ 164 $ A second test was designed to check whether there is a correlation between the levels of confidence of automatic and manual pruning .
Tagged_Text $ 3 $ 32 $ 18 $ The most important named entities in the Remedia corpus are the names of people and the names of places .
Abstract $ 0 $ 7 $ 6 $ Our conclusions show the importance of the application area in the design of NLP tools .
Research_context : _hypertext $ 1 $ 46 $ 42 $ The part inside the bold sub-frame is the input to the sentence planner Last year , the most atypical sales variations from one month to the next occurred for : ® Birch Beer with a 42 % national increase from September to October ; ® Diet Soda with a 40 % decrease in the Eastern region from July to August .
_Pronominalise_the_Cb_only_after_a_Continue $ 4 $ 83 $ 44 $ That is , the Text/TERM Planner/TERM would plan/DEF the content of Un+l by aiming to realise a proposition in the knowledge base which mentions an entity which is salient in Un ./O
Collaborative_Agents $ 1 $ 68 $ 64 $ AGENT : " There is a conflict of meeting with Brian at three P . M . Thursday with meeting with Irene Landoz at three P . M .
Context-dependent_Lexicons $ 3 $ 83 $ 0 $ In the last section , we only use current part-of-speech as a lexical entry.
Related_Work $ 6 $ 173 $ 13 $ In this paper , we have summarized the evidence for this view of human conversation , and shown how it informs the generation of communicative action in our artificial/DEF embodied conversational agent ,/O REA/TERM .
OUT $ -1 $ 146 $ 141 $ For each question type , we find one word that has the highest positive C value for that question type .
Comparison_experiment $ 3 $ 139 $ 41 $ ( who is the father of the relativity theory ?
Dialogue_system_architectures $ 3 $ 86 $ 39 $ For example , in cases where the background system is distributed and consists of several domain and application system knowledge sources the dialogue system must know which of them to access , in what order , and how the results should be integrated into one answer.
Introduction $ 1 $ 27 $ 22 $ The user then selects which of the hits should be summarized.
Towards_building_a_parallel_corpus $ 2 $ 66 $ 34 $ Assume now that it is the task of an MGEN system to produce from a knowledge base texts ( 4 ) and ( 6 ) .
_INTRODUCTION $ 1 $ 12 $ 6 $ New words are easily constructed by combining morphemes and their meanings/TERM are the/DEF semantic composition of morpheme components ./O
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 112 $ 77 $ In a similar way , Wpit/TERM denotes TF*IDF/DEF of the term t in the i-th paragraph ./O
OUT $ -1 $ 77 $ 76 $ In contrast , if the subject is a comparison between two entities ( e . g . , v ( ed > v ( e_ , ) ) , the value of the subject for an objective o is [vo ( e9 Vo ( e , ) ] , and it is positive when it is greater than 0 ( negative otherwise ) .
Introduction $ 1 $ 29 $ 13 $ Section 5 describes two further experiments very briefly .
Introduction $ 1 $ 15 $ 10 $ In the QA track , each participant is given a list of 200 questions , and the goal is to locate answers to these questions from a document database consisting of hundreds of thousands of documents ( about two gigabytes of text ) .
Abstract $ 0 $ 2 $ 0 $ In the context of language learning , we address a logical approach to information extraction.
_Our_Classification_Algorithm_for $ 2 $ 82 $ 18 $ In this paper , we render an algorithm to classify verbs into different categories , which is the basis of another research . . . . recognition of sentence situation , which will be discussed in future work .
Abstract $ 0 $ 2 $ 1 $ The MT Proficiency Scale project has developed a means of baselining the inherent "tolerance" that a text-handling task has for raw MT output , and thus how good the output must be in order to be of use to that task.
Approach $ 2 $ 32 $ 26 $ Unlike the voting algorithms , the classifiers do not require a uniform input .
Previous_Schemes_for_Grunt $ 2 $ 49 $ 21 $ Second , there is the assumption that the set of conversational grunts is small .
Architecture $ 3 $ 55 $ 4 $ The RAGS/TERM architecture/TERM ( Cahill et al. , 1999 ) is a/DEF reference architecture for natural language generation systems ./O
Introduction $ 1 $ 33 $ 26 $ Since Italian grammar is very different from the English one , some terms do not have an English equivalent and , hence , cannot be translated.
Reading_Comprehension_Tests $ 2 $ 26 $ 8 $ 3 A Rule-based System for Question Answering Quarc/TERM (/TERM QUestion/TERM Answering/TERM for/TERM Reading/TERM Comprehension/TERM )/TERM is a/DEF rule-based system that uses lexical and semantic heuristics to look for evidence that a sentence contains the answer to a question ./O
Abstract $ 0 $ 2 $ 1 $ Quarc/TERM uses/DEF heuristic rules that look for lexical and semantic clues in the question and the story ./O
Comparing_Taggers $ 1 $ 8 $ 1 $ As a contextual clue , we might for instance assume that it is unlikely that a verb will follow an article .
Stochastic_Surface_Realization $ 2 $ 89 $ 31 $ There is the problem , as in speech recognition using n-gram language models , that long-distance dependency cannot be captured .
The_Generation_System $ 3 $ 107 $ 65 $ The result of the linearization/TERM phase/TERM is a/DEF word lattice specifying the sequence of words that make up the resulting sentence and the points of ambiguity where different generation paths are taken ./O
Constraint_Satisfaction_with $ 3 $ 68 $ 9 $ A natural cost function is to use the classifiers probabilities P ( o ) and P ( c ) and define , for a phrase e = ( o , c ) , c ( e ) = 1 P ( o ) P ( c ) which means that the error in selecting e is the error in selecting either o or c , and allowing those 108 to overlap 4 .
Analysing_Czech_texts $ 3 $ 116 $ 45 $ The transduction/TERM of/TERM the/TERM ATS/TERM to/TERM the/TERM DMCS/TERM consists/DEF of the four procedures : elimination of the auxiliary nodes and joining the complex word forms into one node ./O
Implementation $ 4 $ 308 $ 137 $ The motivation for this is obvious : when a summarization filter ( which is a program under our control ) is generating a media object , we can often provide sufficient meta-information about that object to generate a short caption and some running text .
Given_an_input_space_X~*_of $ 3 $ 41 $ 10 $ Because S is a finite sequence , only concepts with a finite number of positive examples can be learned with total success , i.e.
Introduction $ 1 $ 46 $ 37 $ The algorithm takes a part of speech tagged corpus and extracts the nouns.
Stochastic_Surface_Realization $ 2 $ 63 $ 5 $ We replace the gefieration grammar with a simple statistical language model to generate more complex utterances .
Implementation $ 3 $ 82 $ 0 $ From an implementation point of view , the core of TransType relies on a flexible object oriented architecture , which facilitates the integration of any model that can predict units ( words or sequence of words ) from what has been already typed and the source text being translated .
Exploring_the_relationship_between $ 2 $ 42 $ 27 $ For example , one rule describes dialogue contributions whose general function was what we call presentation , to advance the description of the house by introducing a single new object . .
_Background $ 1 $ 28 $ 13 $ 1 . 2 Grounding and Common Ground Units Grounding/TERM is the/DEF process by which information contributed by participants in interaction is taken to have entered the ' common ground ' , or mutual knowledge of the participants (/O Clark & Schaefer 1989 , Clark 1996 , Traum 1994 ) .
Systems_Foundations $ 3 $ 30 $ 6 $ MIMIC/TERM currently utilizes/DEF templatedriven text generation , and passes on text strings to a stand-alone TTS system ./O
Abstract $ 0 $ 147 $ 14 $ Section 2 describes HuDL in greater detail and section 3 discusses system integration and the IMA .
Generation_of_Vague_Descriptions $ 4 $ 85 $ 4 $ We will take them to be of the form n crn , where n/TERM is a/DEF positive natural number ./O
Discussion $ 6 $ 125 $ 11 $ If the extended information is not explicitly presented in the text , the default value provides the assumption based on the world knowledge .
Given_an_input_space_X~*_of $ 3 $ 85 $ 54 $ Given an unknown word w ' occurring in a context represented by f ' k , the WSD algorithm assigns w ' to the category in C that maximizes the similarity between f ' k and one of its members .
A_Machine_Learning_Approach $ 3 $ 83 $ 78 $ In this section , we present our machine learning approach to question answering.
Statistical_Semantic_Parsing $ 4 $ 139 $ 40 $ hk covers s ) , we have PCai ( s ) • o~ I hk ) = pc + O " nc Pc -tnc ( 14 ) where Pc/TERM and ne/TERM are the/DEF number of positive and negative examples covered by hk respectively ./O
_Introduction $ 1 $ 15 $ 8 $ Most of the methods , to date , aim at solving the problem for one language , namely the language with the most available linguistic resources .
OUT $ -1 $ 67 $ 62 $ One exception is the work of ( Radev et al. , 2000 ) at the TREC-8 QA track , which uses logistic regression to rank potential answers using a training set with seven features.
Analysing_Czech_texts $ 3 $ 105 $ 34 $ One of the attributes is the analytic/TERM function/TERM that expresses/DEF the syntactic function of the word ./O
Research_context : _hypertext $ 1 $ 15 $ 11 $ In a nutshell ' , while NLG is the only technology able to completely fulfill the reporting needs of i See Favero ( 2000 ) for further justification for this view , as well as for details on the motivation and technology underlying MATRIKS .
The_Generation_System $ 3 $ 56 $ 14 $ Thus , for example , the following structure ( with some aspects elided for brevity ) represents a node that could be one of three possibilities .
OUT $ -1 $ 101 $ 50 $ A good/TERM embedding/TERM is one/DEF satisfying all the following conditions : strative or a bridging description (/O as defined in ( Poesio et al . , 1997 ) ) .
The_Complexity_of_Extracting_a $ 2 $ 68 $ 0 $ As mentioned earlier , the level of a fact for a piece of text depends on the network constructed for the text .
Learning_Algorithms_Tested $ 2 $ 40 $ 20 $ When classifying a new example , SNoW/TERM is similar/DEF to a neural network which takes the input features and outputs the class with the highest activation ./O
Dialog_Management $ 3 $ 85 $ 24 $ Response/TERM Complexity/TERM : There/DEF is a reward and a punishment associated with each system response that reflects the complexity of the content and realization of the system responses ./O
Results $ 3 $ 69 $ 12 $ Obviously , a total of 13,950 keywords could never have been obtained since die maximum possible number of 10 keywords in the letters corpus is 2,415 , which is the total number of types .
Introduction $ 1 $ 8 $ 2 $ The CoNLL-2000 shared task attempts to fill this gap .
Abstract $ 0 $ 95 $ 2 $ Each graph consists of smaller subgraphs for each individual document ( Figure 4 ) .
Conclusion $ 9 $ 198 $ 16 $ Here is an overview of the chronology of events .
Previous_work $ 2 $ 27 $ 3 $ D~Jean ( 1998 ) uses an approach derived from Harris ( 1951 ) where word-splitting occurs if the number of distinct letters that follows a given sequence of characters surpasses a threshoid .
OUT $ -1 $ 137 $ 100 $ In this respect , we have not been facing many problems in fitting Portuguese structures with UNL ones , since Portuguese/TERM , like English , is an/DEF inflectional language that also employs prepositional constructions ./O
_Rare_w~_contains_a_hyphen $ 9 $ 88 $ 33 $ For example , the number 244 in the ( NN , JJ ) position is the number of words that were NNs but were incorrectly assigned the JJ category .
OUT $ -1 $ 0 $ 0 $ NJFun/TERM : A/DEF Reinforcement Learning Spoken Dialogue System Diane/O Litman , Satinder Singh , Michael Kearns and Marilyn Walker AT&T Labs -Research 180 Park Avenue Florham Park , NJ 07932 USA {diane ,bavej a ,mkearns ,walker} @research . att . com
Discussion $ 8 $ 175 $ 5 $ The argument here is simply that these two closure measures are used to spot when a sublanguage corpus approaches closure--that is , when the curve of new types• and new combinations of type with token begins to flatten at a rate below ten percent.
Discussion $ 4 $ 74 $ 1 $ Thus , a contribution of Gso , is the demonstration that from a simple context-free grammar , with a very lightweight formalism , one can extract enough information ( Ontology , Parsebank , Parser Predictions strategy ) to conduct meaningful clarification dialogues.
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 117 $ 82 $ DispDt/TERM is dispersion/DEF value of term t in the level of Document which consists of m documents , and denotes how frequently t appears across documents ./O
OUT $ -1 $ 110 $ 92 $ Any sentence that contains a proper noun whose head noun matches <x> will be highly rewarded .
Modelling_Phonological_Dyslexia $ 4 $ 101 $ 20 $ Because of the absence of some compound words ( e.g. , gentlelman ) from the dictionary , the simulations concerning "parent words" ( e.g. , father is the parent of.fat and her ) for both Test 1 and Test 2 are not perfect.
Statistical_Semantic_Parsing $ 4 $ 100 $ 1 $ Given a sentence I • Sentences , the set Q ( 1 ) = {q • Queries I ( l , q ) • Parser} is the set of queries that are correct interpretations of I .
Introduction $ 1 $ 20 $ 11 $ This is a result of the need to formalise the huge number of research results that appear in free-text form in online collections of journal abstracts and papers such as MEDLINE for databases such as Swissprot ( Ban : och and Apwefler , 1997 ) and also to search such collections for facts in an intelligent way .
Dialogue_manager $ 6 $ 174 $ 66 $ The KU with the best certainty score and the KUs with the 90% or larger certainty score are called candidate KUs .
Introduction $ 1 $ 19 $ 15 $ o Entities/TERM , representing/DEF objects ( individuals ) of the world ./O
Abstract $ 0 $ 4 $ 3 $ The tool uses a query language that allows to search for tokens , syntactic categories , grammatical functions and binary relations of ( immediate ) dominance and linear precedence between nodes .
Introduction $ 1 $ 48 $ 35 $ In Figure 3 , it was not necessary for the application to specify that the conjunction of two noun phrases is a parallel noun phrase , nor that component/TERM noun/TERM phrases/TERM ( proper/DEF nouns , pronouns , and possessives )/O should not , contain an article .
Abstract $ 0 $ 28 $ 26 $ Since WordSmith Tools is Windows software , it has appealed to a large audience of applied linguists willing to do corpus-based research , to whom this platform is generally the only one that they know how to use .
Unfolding_and_Specialization $ 2 $ 38 $ 11 $ The set of all such best specializations defines the set of candidate successor grammars .
The_steps_in_the_strategy_are_marked_with_the $ 7 $ 137 $ 15 $ His approach to content selection and structuring does not provide a measure of evidence strength , which is necessary to implement several of the guidelines from argumentation literature we have examined.
The_Argument_Generator $ 1 $ 20 $ 0 $ The architecture/TERM of/TERM the/TERM argument/TERM generator/TERM is a/DEF typical pipelined architecture comprising a discourse planner , a microplanner and a sentence real izer ./O
Related_Work $ 6 $ 173 $ 5 $ As prepositions ( and cue-phrases in general ) can signal different coherence relations , the presented computational approach couples a cue-phrase approach like ( Marcu , 1998 ) with inferences using the computed semantic representation.
Explaining_Probabilistic_Methods $ 3 $ 73 $ 25 $ Consequently , the/TERM Bayes/TERM optimal/TERM prediction/TERM is given by : h/DEF ( x ) = argmaxteLH~n=l Pr ( xill ) Pr ( 1 ) , where Pr ( 1 ) denotes the prior probability of l ( the fraction of examples labeled l ) and Pr ( xill ) are the conditional feature probabilities ( the fraction of the examples labeled l in which the ith feature has value xi ) ./O
The_TransType_model $ 2 $ 79 $ 64 $ It relies on the fact that a small number of words account for most of the tokens in a text .
Interaction_Grammars $ 3 $ 99 $ 46 $ fra : : country --> [] .
Generation_of_Vague_Descriptions $ 4 $ 108 $ 27 $ The result is a list that may be written as L = {chihuahua , largesh } , which can be employed to generate the description 'the largest chihuahua'.
Evaluation_Measures $ 2 $ 26 $ 1 $ The scores are used to assess summary quality across a collection of test documents in order to produce an average for an algorithm or system.
Introduction : $ 1 $ 16 $ 11 $ In this paper , we consider the latter approach using sample selection , an interactive learning method in which the machine takes the initiative of selecting potentially beneficial training examples for the humans to annotate .
Transformation_rule_lists $ 2 $ 35 $ 10 $ An often-used method is the difference in performance resulting from applying the rule .
Abstract $ 0 $ 12 $ 10 $ Finally , there is a real demand for working AE systems in techni : cal domains .
_General_Outline_of_the_Method $ 2 $ 21 $ 5 $ The second stage is the extracting process in which Chinese entity names and their relations are extracted using the classifiers learned.
The_Structure_of_a_Relational $ 3 $ 96 $ 0 $ 3 .3 Terminology In the following discussion , we will use the following terminology : * Predicate : each column of an entity file defines a predicate .
OUT $ -1 $ 184 $ 142 $ It measures the performance of a system trained on a set of samples distributed according to the probability distribution p when tested on a set following a probability distribution q .
Abstract $ 0 $ 9 $ 4 $ This algorithm has been implemented in an experimental NLG program using ProFIT .
Lexical_and_Syntactic_Closure $ 3 $ 29 $ 1 $ Moreover , while Chinese tokens can concatenate , Chinese has no extensive morphology like many Indo-European languages .
Identifying_Structure_and_the $ 2 $ 21 $ 0 $ 'Character Set' The initial task , given an incoming bit-stream , is to identify if a language-like structure exists and if detected what are the unique patterns/symbols , which constitute its 'character set'.
Only_the_relevant_attributes_of_each_semantic_role $ 5 $ 179 $ 88 $ So the AGENT of this CLAUSE is the PRONOUN she and it has a link to the ENTITY Ann ( the chosen antecedent ) .
OUT $ -1 $ 145 $ 74 $ For example , ( A , Adv , D ) is a dependency type expressing the relationship between words in expression " very large " where " very " is a depending adverb and " large " is a head adjective .
Generation_of_Multiple_Quantifiers $ 5 $ 218 $ 33 $ What happens if a sentence contains an existential quantifier which has a wider scope than a universal quantifier ?
_Instrumentalists_not_including_string_players $ 8 $ 57 $ 45 $ While in this particular example there is a clear relation between senses ( sense 1 is involved in the action specified in sense 2 ) , it seems extremely difficult to find general clustering techniques based on Word . Net hierarchy to capture all potential IR clusters .
_Topic_Deviation $ 3 $ 158 $ 12 $ 5 ) Informative , neutral and destructive phrasal terms are defined by means of MI ( oce ,rel ) .
Research_focus : _content_aggregation $ 2 $ 97 $ 36 $ This time it is the product dimension that has been left-shifted and that provided the basis for row sorting , row grouping and cell merging .
Dialogue_Management $ 2 $ 126 $ 82 $ Communicative/TERM space/TERM is defined by a/DEF number of coordinates that characterise the relationships of participants in a communicative encounter ./O
OUT $ -1 $ 3 $ 3 $ The Workshop on Syntactic and Semantic Complexity in Natural Language Processing Systems , held on April 30th , 2000 at the Language Technology Joint Conference on Applied Natural Language Processing and the North American Chapter of the Association of Computational Linguistics ( ANLP-NAACL2000 ) was organized around the goals of discussing , promoting , and presenting new research results regarding the question of complexity as it pertains to the syntax and semantics of natural language .
_Measurements $ 3 $ 81 $ 14 $ ) In isolation ( before being specialized to the situation of joint venture capitalization , another cost ) , equal denotes a completely unsaturated relation : k collection ( partition ( measurablestuff ) ) .
Robustness $ 4 $ 124 $ 33 $ A common problem occuring in our system is the occurrence of subordinating predicates with empty obligatory arguments.
HMM_for_Mono-Lingual_Retrieval $ 2 $ 26 $ 8 $ number of occurrences of W in C x • e0e IGx ) = length/TERM of/TERM Cx/TERM which is the/DEF general language probability for word W in language x ./O number of occurrences of W in D • e ( WlD ) = length of D In principle , any large corpus Cx that is representative of language x can be used in computing the general language probabilities .
Overview_of_the_system $ 2 $ 21 $ 13 $ We assumed that P ( WjI ( Ci , Si ) ) = P ( WjlCi ) for every Si E S. Once the integrated transducer has been made , the tagging and shallow parsing process consists of finding the sequence of states of maximum probability on it for an input sentence.
Results $ 3 $ 95 $ 38 $ A reference corpus that is five times as large as the study corpus yields a larger number of keywords than a smaller reference corpus .
Introduction $ 1 $ 14 $ 5 $ The earliest work ( Rayner , 1988; Samuelsson and Rayner , 1991 ) builds a specialized grammar by chunking together grammar rule combinations while parsing training examples.
Background $ 2 $ 88 $ 68 $ 2 inforrnPositive/TERM (/TERM p=v/TERM )/TERM : user/DEF confirms that the value of parameter p is v ./O p E params ( AD ) U { aTask } .
Abstract $ 0 $ 7 $ 2 $ Grammar specialization is here given a novel formulation as an optimization problem , in which the search is guided by a global measure combining coverage , ambiguity and grammar size.
Abstract $ 0 $ 4 $ 2 $ The reasoning/TERM model/TERM is interacting/DEF with the model of communication process ./O
How_to_generate_technical $ 2 $ 95 $ 44 $ G-TAG/TERM thus seems a/DEF good candidate for producing technical documentation complying with the constraints of an ( EM ) CL ./O
Clustering_into $ 5 $ 78 $ 9 $ This can be computed for given word-pair type ( wl , w2 ) by recording each word-pair token ( wl , w2 , d ) in a corpus , where d/TERM is the/DEF distance or number of intervening words ./O
MEAD : _a_centroid-based_multi $ 3 $ 48 $ 0 $ document summarizer We now describe the corpus used for the evaluation of MEAD , and later in this section we present MEAD's algorithm.
Setting $ 3 $ 89 $ 10 $ Table 1 contains information about the number of examples , the number of senses , and the percentage of the most/TERM frequent/TERM sense/TERM ( MFS/ACR ) of these reference words , grouped by nouns , verbs , and all 21 words .
The_Acquisition_of_Word_Order $ 3 $ 117 $ 6 $ These parameters have the prior and posterior probabilities initialised with 0.1 for one value and 0.9 for the other .
ALLiS $ 2 $ 24 $ 19 $ Using XML properties , the grammar has easily access to all the levels/TERM of/TERM the/TERM document/TERM ( word/DEF , tag , phrase , and higher structures )/O .
Our_approach_to_Multilingual $ 2 $ 31 $ 4 $ Thus , the author is terlingua/TERM ( specific/DEF to the class of documents being always overtly working in the language s/he nows ,/O modelled ) , and it is the responsibility of appropribut is implicitly building a language-independent ate "rendering" mechanisms to produce actual text representation of the document content.
The_hyperonym_problem $ 2 $ 29 $ 11 $ " The relation of hyperonymy is generally regarded as transitive : If/DEF A is a hyperonym of B , and B is a hyperonym of C , then A is a hyperonym of C ./O Following common practice , we call A a direct/TERM hyperonym/TERM of B , while it is only an indirect hyperonym of C. The same holds for the inverse relation , hyponymy.
Abstract $ 0 $ 2 $ 1 $ WIT/TERM features/DEF an incremental understanding mechanism that enables robust utterance understanding and realtime responses ./O
Shallow_Parsing $ 4 $ 77 $ 7 $ The SNoW/TERM learning/TERM architecture/TERM learns/DEF a sparse network of linear functions , in which the targets ( states , in this case ) are represented as linear functions over a common feature space ./O
Experimental_Results $ 7 $ 246 $ 79 $ Error/TERM probability/TERM is a/DEF metric for evaluating segmentation results proposed/O in ( Allan et ai. , 1998; Beeferman etal. , 1999 ) .
Abstract $ 0 $ 2 $ 0 $ We introduce CST/TERM (/TERM cross-document/TERM slructure/TERM theory/TERM )/TERM , a/DEF paradigm for multidocument analysis ./O
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 224 $ 189 $ 'F/A ' shows false alarm rate and 'FI/TERM ' is a/DEF measure that balances recall and precision ./O
Architecture_components $ 2 $ 28 $ 19 $ The second stage consists of a combination of the outputs of the five base chunkers , using another WPDV model .
Background $ 2 $ 60 $ 40 $ requestValue/TERM ( p=v ) : system/DEF asks whether the value v of parameter p is correct ./O
Error-driven_Learning $ 4 $ 147 $ 30 $ This paper proposes a new error-driven HMMbased chunk tagger with context-dependent lexicon .
_System_Configuration $ 2 $ 44 $ 20 $ Assigned to each individual term is a different weight so as to reshape a new search emphasis .
Introduction $ 1 $ 25 $ 18 $ The communication/TERM channel/TERM consists/DEF of the trained classifier ./O
Impact_of_Lexicon_Size $ 9 $ 191 $ 53 $ • Rather than translation ambiguity , a more serious limitation to effective cross-lingual IR is incompleteness of the bilingual lexicon used for query translation .
Indexing_and_Retrieval $ 5 $ 283 $ 50 $ What is the combined effect of surface heat and mass transfer on hypersonic flow?
Memory-Based_Language $ 1 $ 7 $ 1 $ These exemplars take the form of a vector of , typically , nominal features , describing a linguistic problem and its context , and an associated class symbol representing the solution to the problem .
_Instrumentalists_not_including_string_players $ 8 $ 96 $ 84 $ Extract the candidate senses that satisfy the parallel polysemy criterion , in three variants : Experiment 1 : sets of senses that have parallel translations in at least two out of the four target languages .
Introduction $ 1 $ 12 $ 7 $ ( 1994 ) , T'sou et al.
Prerequisites $ 2 $ 35 $ 16 $ The deep/TERM translation/TERM track/TERM consists/DEF of an HPSG based analysis , semantic transfer and finally a TAG-based generator ( VMGECO ) ./O
Hyperonymy_in_lexical_semantics $ 5 $ 121 $ 24 $ There is a general rule that requires speakers to use this term in order to obtain an unmarked utterance in a given context : : . unless . this would result in an ' abnormal communication ' , in which case the speaker should deviate from neutral level , but only to the minimum degree required to ensure normality .
Applications_of_LexTract $ 4 $ 157 $ 0 $ 4.1 Evaluating the coverage of hand-crafted grammars The XTAG/TERM grammar/TERM ( XTAG-Group , 1998 ) is a/DEF hand-crafted large-scale grammar for English , which has been developed at University of Pennsylvania in the last decade ./O
Introduction $ 1 $ 17 $ 6 $ When analysing the content of a document , terms are the basic processed units -usually they are words of natural language .
Qualitative_Evaluation_of_the $ 4 $ 170 $ 69 $ We therefore use models that just use the simple tree accuracy and the number of substitutions as independent variables• Second , we note that once we have done so , a perfect sentence gets a score of 0.8689 ( for understandability ) or 0.6639 ( for quality ) .
OUT $ -1 $ 75 $ 74 $ The value of different subjects is measured as follows .
Abstract $ 0 $ 4 $ 0 $ The bigram language models are popular , in much language processing applications , in both Indo-European and Asian languages .
Rules_as_features $ 1 $ 16 $ 8 $ For example , ( Domingos , 1996 ) describes the RISE/TERM system/TERM , in/DEF which rules are ( carefully ) generalised from instances , and in which the k-NN classification rule searches for nearest neighbours within these rules when classifying new instances ./O
Experimental_Results $ 5 $ 156 $ 6 $ The third domain consists of a set of 300 computer-related jobs automatically extracted from postings to the USENET newsgroup austin.jobs.
Abstract $ 0 $ 2 $ 1 $ The annotation/TERM information/TERM consists/DEF of speech , transcription delimited by slash units , prosodic , part of speech , dialogue acts and dialogue segmentation ./O
Introduction $ 1 $ 39 $ 30 $ Once the corpus was segmented the next step was to align it .
Abstract $ 0 $ 88 $ 86 $ An end-to-end evaluation includes an analyzer/TERM , which maps/DEF the source language input into IF and/O a generator/TERM , which maps/DEF IF into target language sentences ./O
_INTRODUCTION $ 1 $ 33 $ 27 $ However an organization ' s full names usually occur at its first mention , unless it is a well-known organization .
The_machine_learning_method $ 3 $ 101 $ 42 $ Here is the form of the positive examples : POSITiVE ( category_before_N , category_after . N , category_before_V , V_type , distance , position ) .
Results $ 3 $ 46 $ 7 $ In the gisting exercise , each user was asked to rate decision points in a translation on a 1-5 scale .
OUT $ -1 $ 0 $ 0 $ Planning Word-order Dependent Focus Assignments* Cornelia Endriss and Ralf Klabunde University of Heidelberg Center for Computational Linguistics Karlstr .
Experimental_Results $ 5 $ 94 $ 20 $ Precision/TERM is the/DEF ratio between the number of correct parses produced by the specialized grammar and the total number of parses produced by the same grammar ./O
Learning_Algorithms_Tested $ 2 $ 57 $ 37 $ LazyBoosting/TERM ( Escudero et al . , 2000a ) is a/DEF simple modification of the AdaBoost ./O MH/TERM algorithm/TERM , which consists/DEF in reducing the feature space that is explored when learning each weak classifier ./O
OUT $ -1 $ 126 $ 89 $ The BP decoder , for example , is able to produce outputs whose literal meaning is preserved in most cases ( Martins et al . , 1998b ) , using handcoded UNL expressions .
Conclusions $ 6 $ 141 $ 4 $ A voting algorithm for learning blocks of bits proves as accurate as an expensive feature-selecting algorithm .
The_TransType_model $ 2 $ 53 $ 38 $ 2.2.1 The evaluator The evaluator/TERM is a/DEF function p ( t[ t ' , s ) which assigns to each target-text unit t an estimate of its probability given a source text s and the tokens t ' which precede t in the current translation of s ./O
Data_Collection_and_Evaluation $ 4 $ 164 $ 18 $ on the "core/TERM dialogue/TERM ," defined as the/DEF interval subsequent to logging on and up until the itinerary is fully specified , but has not yet been priced ./O
Previous_Schemes_for_Grunt $ 2 $ 47 $ 19 $ First , there is the assumption that each grunt has some fixed meaning and some fixed functional role ( filler , back-channel , etc ) .
The_model $ 1 $ 30 $ 7 $ The hotel Regina has thirty single rooms The hotel/TERM Regina/TERM is an/DEF expensive hotel ./O
The_CGS_system $ 4 $ 74 $ 6 $ The output of the planner is a partially ordered plan with speech-acts as leaves .
The_Diversity_of_Semantic $ 2 $ 24 $ 0 $ Relations between "noun -tNO" and their Head Nouns Among Japanese adnominal constituents , " noun + NO" represents a wider range of semantic relations than other adnominal constituents.
Results_and_Discussion $ 5 $ 148 $ 17 $ baseline 2 ) reaches the highest score in both languages , indicating that the combination of domain word frequency ( considered at step 1 of the algorithm ) and domain text frequency ( considered at step 2 ) is a good one .
Definitions $ 2 $ 37 $ 12 $ A corpus/TERM position/TERM for a corpus C is a/DEF tuple ( j ,k ) , meaning the k th symbol in the jtb string in the corpus , with the restrictions : 1 _< j _<[ C[and 0 _< k _<[ CU] [ ./O
Discussion $ 4 $ 76 $ 3 $ 13 Another advantage is the ease with which natural language interfaces can be constructed for new domains : Since all the task and linguistic knowledge is extracted from the grammar , 14 one need only develop a Kernel Grammar that models the domain at ( extracted from the final interpretation of ( U1 ) ) would have been learned too , but its subsumption by existing rule ( R1 ) was automatically detected .
Introduction $ 1 $ 22 $ 17 $ Conversely , a question can have multiple correct answers , where each of several individual sentences is a correct answer .
Issues_and_proposals $ 3 $ 159 $ 116 $ In our annotation scheme the presentational relations are split from the subject-matter relations and annotators are instructed to consider for each set of spans whether there is a subject-matter relation , and also whether there is a presentational relation .
Abstract $ 0 $ 2 $ 0 $ A wide range of natural language problems can be viewed as disambiguating between a small set of alternatives based upon the string context surrounding the ambiguity site .
Conclusions_and_loose_ends $ 5 $ 126 $ 2 $ The numerical data that are the input to our algorithm , for example , take a very different form in the descriptions generated , and yet there is , in an interesting sense , no loss of information : a description has the same reference , whether it uses • ... : ,..exaet~.anforroataon : ( ~he : 3c~zz.mouse.
Introduction $ 1 $ 16 $ 12 $ All ASR transcriptions we use will be actual output from a broadcast news ASR system with a word error rate of 30% .
The_Generation_System $ 3 $ 85 $ 43 $ Nitrogen ' s input , Abstract/TERM Meaning/TERM Representation/TERM ( AMR/ACR ) , is a/DEF labeled directed graph written using the syntax for the PENMAN Sentence Plan Language (/O Penman 1989 ) .
Conceptualizing_Events $ 2 $ 35 $ 5 $ Language/TERM is the/DEF best conceivable means to transfer information as pointedly as possible ./O
Translations_selection_and_phrase $ 2 $ 59 $ 9 $ e1/TERM ,/TERM e2/TERM ,/TERM .../TERM ,/TERM e/TERM , are the/DEF segmented Chinese words of the query after removing the stop words ./O
Models_and_Modifications $ 2 $ 18 $ 5 $ Note that if the modifying nonterminals were generated completely independently , the model would be very impoverished , but in actuality , by including the distance and subcat frame features , the model captures a crucial bit of linguistic reality , viz. , that words often have welldefined sets of complements and adjuncts , dispersed with some well/DEFined distribution in the right hand sides of a ( context-free ) rewriting system .
Enriching_the_Feature_Set_with $ 2 $ 71 $ 1 $ These measures are based on empirical counts of word occurrences and co-occurrences .
ADVF_Far_adverbs_ ( ~ ' l $ 7 $ 102 $ 17 $ Text For an Input : S = blockl , block2 , . . . ,block , , the dependency parsing will generate a set of 3-tuple in the form of {governor , dependant , dependency-relation} , which represents dependency relations between blocks in the given sentence .
OUT $ -1 $ 0 $ 0 $ Demonstration of ILEX 3.0 Michael O'Donnellt ( micko@dai.ed.ac.uk ) , Alistair Knott : ~ ( alik@hermes.otago.ac.nz ) , Jon Oberlandert ( jon@cogsci.ed.ac.uk ) , Chris Mellisht ( chrism@dai.ed.ac.uk ) t Division of Informatics , University of Edinburgh.
Introduction $ 1 $ 6 $ 4 $ A case in point is the interlingua representations used for machine translation and cross-language processing.
Conceptualizing_Events $ 2 $ 106 $ 76 $ One central parameter of incremental processing , which is highly relevant for the format of preverbal messages , is the size of the increments .
OUT $ -1 $ 146 $ 139 $ l ) 3 Amanda Huggenkiss.
OUT $ -1 $ 82 $ 56 $ Consider the subphrase estimate for line represents the operation needed to.
The_query_language $ 3 $ 85 $ 28 $ ( ILl , 79 , 73 , £ , # , rl , a ) is a query model with categories C , edge labels E and terminals Tiff/TERM 1/TERM is a/DEF finite set with Lt n ( C U E U T ) = O , the set of nodes ./O
OUT $ -1 $ 143 $ 138 $ Note that the correlation/TERM metric/TERM C/TERM is the/DEF square root of the X 2 metric ./O
_The_center_is_the_entity_which_is_most_likely $ 3 $ 28 $ 0 $ to be pronominalised : Grosz et al ' s " Rule 1 " in its weakest form states that if any entity is referred to by a pronoun , the Cb must be .
_Can_a_satisfactory_e~perimental_technique_be_devel $ 9 $ 293 $ 3 $ Three measures are used in the evaluation of the system performance : ( 1 ) precision/TERM , dEfined as the/DEF number of relevant documents retrieved over the total number of documents retrieved ;/O ( 2 ) recalL/TERM , defined as the/DEF number of relevant documents retrieved over the total number of relevant documents found in the collection and/O ( 3 ) F-measure/TERM , which combines/DEF both the precision and recall into a single formula : Fmeasure/TERM = 2*R*P/(R+P)/DEF where P is the precision , R is the recall and is the relative importance given to recall over precision ./O
Statistical_Semantic_Parsing $ 4 $ 141 $ 42 $ hk does not cover s ) , we have PCai ( s ) • OS 7 " I hk ) -p " + 8 .n , , Pu +nu ( 15 ) where Pu and nu are the n , ,rnber of positive and negative examples rejected by hk respectively .
OUT $ -1 $ 0 $ 0 $ Reinterpretation of an existing•NLG system in a Generic Generation Architecture L. Cahill , C. Doran~ R. Evans , C. Meilish , D. Paiva , : M. Reape , D. Scott , , N. Tipper .Universities of Brighton and Edinburgh.
Abstract $ 0 $ 6 $ 5 $ Retrieval effectiveness is examined utilizing query weight ratio of these three categories of phrasal terms .
Domain_Terms $ 3 $ 38 $ 7 $ Attributes of an object are the things that the program needs to know about the object in order to use it in the domain .
Introduction $ 1 $ 20 $ 15 $ The widely available corpus is Academic/TERM Sinica/TERM Balanced/TERM Corpus/TERM abbreviated as ASBC/TERM hereafter ( I-Iuang and Chen , 1995 ) , which is a/DEF POS-tagged corpus ./O
Abstract $ 0 $ 25 $ 0 $ Algorithm is based on rules for discriminating among the two types of anaphor based on the predicative contexts in which the anaphors occur .
_Pronominalise_the_Cb_only_after_a_Continue $ 4 $ 65 $ 26 $ We noted above that there is evidence that a RETAIN-SHIFT sequence is the preferred way of introducing a new transition following CONTINUE , another CONTINUE would be cheap as well .
Principles_and_Parameters $ 1 $ 21 $ 14 $ Ambiguity/TERM is a/DEF natural enemy of efficient language acquisition ./O
The_query_language $ 3 $ 94 $ 37 $ Lt/TERM ~/TERM C/TERM is a/DEF total function ./O
Summary_and_Conclusions $ 6 $ 132 $ 6 $ We define tightly/TERM bound/TERM as those/DEF schema that users expect to discuss interchangeably , without explicit shifts in conversational focus ./O
Models_and_Modifications $ 2 $ 48 $ 35 $ Pi/TERM (/TERM c~/TERM )/TERM is the/DEF probability of beginning a derivation with c~ ;/O Ps/TERM (/TERM o~/TERM I/TERM 77/TERM )/TERM is the/DEF probability of substituting o~ at 7 ;/O Pa/TERM (/TERM /~/TERM I/TERM r//TERM )/TERM is the/DEF probability of adjoining ~ at 7/ ;/O finally , Pa/TERM (/TERM NONE/TERM I/TERM 7/TERM )/TERM is the/DEF probability of nothing adjoining at ~/ ./O
Related_work $ 6 $ 164 $ 15 $ In our case the meta-interpreter/TERM is the/DEF chart parser augmented with the generation of needs and/O the partial proof is represented by the chart augmented with the needs.
Introduction $ 1 $ 4 $ 1 $ SVMs/TERM are so-called large/DEF margin classifiers and are well-known as their good generalization performance ./O
CoreLex $ 2 $ 35 $ 11 $ The class artifact / attribute / substance for instance includes a number of nouns ( `` chalk , charcoal , daub , fiber , fibre , tincture '' ) that refer to an object that is at the same time an artifact made of some substance and that is also an attribute .
Experiments $ 5 $ 101 $ 7 $ " Precision/TERM " is the/DEF percentage of correct answers among the answers proposed by the system ./O
Introduction $ 1 $ 29 $ 17 $ In GL formalism , lexical/TERM entries/TERM consist/DEF in structured sets of predicates that define a word ./O
Limitations $ 5 $ 113 $ 0 $ GoDiS/TERM is a/DEF small-scale prototype and/O as such it suffers from the familiar drawbacks of many experimental systems : its lexicons and databases are very small , and the domain knowledge is limited.
_General_Outline_of_the_Method $ 2 $ 103 $ 87 $ Their differences lie in that • MBL/TERM is a/DEF lazy learning algorithm that keeps all training data in memory ./O
Experimental_Results $ 5 $ 184 $ 34 $ Prob-Parser/TERM (/TERM B/TERM )/TERM is the/DEF probabilistic parser using a beam width of B ./O TABULATE is CHILL using the TABULATE induction algorithm with determ ; nistic parsing .
Introduction $ 1 $ 13 $ 10 $ Each story has the form of a newspaper article , including a title and dateline .
Abstract $ 0 $ 2 $ 1 $ Our agent plans each utterance so that multiple communicative goals may be realized opportunistically by a composite action including not only speech but also coverbal gesture that fits the context and the ongoing speech in ways representative of natural human conversation .
Introduction $ 1 $ 23 $ 17 $ Section 2 presents the transformation based learning paradigm ; Section 3 describes the algorithm for construction of the decision tree associated with the transformation based list ; Section 4 describes the experiments in detail and Section 5 concludes the paper and outlines the future work .
Given_an_input_space_X~*_of $ 3 $ 110 $ 79 $ The motivation is that we now assume that a context for a word belonging ( also to ) Ct is a valid context for any word in that category .
Conclusion $ 4 $ 114 $ 9 $ In these ways , YAG/TERM provides/DEF the speed , robustness , flexibility , and maintainability needed by real-time natural language dialog systems ./O
Conclusion $ 5 $ 69 $ 0 $ It is found that the performance with the help of error-driven learning is improved by 2.20% and integration of memory-based learning further improves the performance by 0.35 % to 92.12% .
HMM-based_Chunk_Tagger_with $ 2 $ 20 $ 11 $ The second item is the summation of log probabilities of all the tags .
VALDIA_-_The_Implementation $ 4 $ 140 $ 14 $ In Figure 5 the left row contaius the basic semantic entities , the middle the probability , and the right one the number of occurrences for that particular semantic item in each utterance .
_Pronominalise_the_Cb_only_after_a_Continue $ 4 $ 40 $ 1 $ Strategy 3 is favoured by ( GJW ) who cite psychological evidence that " the Cb is preferentially realised by a pronoun in English and by equivalent forms ( i.e. , , . zero pronouns ) in other languages " ( op cit . , p . 214 ) .
Overview_of_the_Approach $ 2 $ 47 $ 27 $ Let ' s go through a simple trace of parsing the request " What is the capital of Texas?
Definitions $ 2 $ 25 $ 0 $ Below we provide the standard definition for regular expressions , and then define a/DEF less expressive language formafism ,/O which we will refer to as reduced/TERM regular/TERM expressions/TERM .
Learning_Algorithms_Tested $ 2 $ 21 $ 1 $ Naive/TERM Bayes/TERM is intended as a/DEF simple representative of statistical learning methods ./O
Conclusions $ 6 $ 144 $ 5 $ This explanation of the motivations leading to cooperation provides an explicative model that is uniform with the treatment of deontic reasoning in agent theories ( Conte et al . , 1998 ) , ( Boella and Lesmo , 2000 ) and the definition of cooperation proposed in ( Boella et al . , 2000 ) .
Collaborative_Agents $ 1 $ 68 $ 64 $ AGENT : `` There is a conflict of meeting with Brian at three P . M . Thursday with meeting with Irene Landoz at three P . M .
_Invert_the_lexicon_to_make_it_an_English $ 5 $ 65 $ 9 $ One problem is the segmentation of Chinese text , since Chinese has no spaces between words.
Discussion $ 5 $ 201 $ 25 $ Thus , as long as the goal of the realizer/TERM is to/DEF enmlate as closely as possible a given corpus (/O rather than provide a maximal range of paraphrastic capability ) , then our approach can be used for evaluation , r As in the case of machine translation , evaluation in generation is a complex issue.
Introduction $ 1 $ 13 $ 7 $ ( 1996 ) , who study the topic of author identification , apply statistical measures and methods on syntactic rewrite rules resulting by processing a given set of texts.
Collaborative_Agents $ 1 $ 12 $ 8 $ The large window in the background is the shared application , in this case , the Lotus eSuite TM email program .
Discussion $ 5 $ 192 $ 16 $ Other realization and sentence planning tin { ks-which are needed for most applications and which may profit from a stochastic model include lexical choice , introduction of function words and punctuation , and generation of morphology .
Abstract $ 0 $ 57 $ 26 $ Although the effect of discourse markers in other languages might not be too prominent , there is a great necessity to study discourse markers in Chinese in order to capture the major associated rhetorical patterns in Chinese texts .
OUT $ -1 $ 65 $ 39 $ An alignment algorithm using substitution , insertion and deletion of tokens as operations attempts to match the generated string with the reference string .
OUT $ -1 $ 0 $ 0 $ Generation from Lexical Conceptual Structures David Traum and Nizar Habash UMIACS , University of Maryland {traum , habash}@cs , umd .
Related_Work $ 4 $ 122 $ 31 $ The key parameter affecting the level of offence is the cost 13 of the requested actions : the less the cost of the requested action , the greater the offence ; this follows the principle that low-cost actions cannot be refused ( Goffman , 1967 ) , and , if they are , requesters get offended .
Abstract $ 0 $ 121 $ 17 $ Such a person name cannot be recognized in person name extraction because it does not begin with a surname or first character of transliterated person names .
Abstract $ 0 $ 7 $ 4 $ An overview of HowNet and information structure are described in this paper .
Generalisation_operators $ 4 $ 122 $ 9 $ Unfortunately , the fourth sentence needs to use the missing rule twice to get a parse , and it is a fundamental limitation of our approach that a missing rule can only be recovered from a failed parse if it is required only once .
The_TABULATE_ILP_Method $ 3 $ 91 $ 25 $ We measure accuracy using the m-estimate/TERM ( Cestnik , 1990 ) , a/DEF smoothed measure of accuracy on the training data which in the case of a two-class problem is defined as : accuracy ( H ) s + m. p+ = ( 1 ) n , -Irrt where/O s/TERM is the/DEF n-tuber of positive examples covered by the hypothesis H ,/O n/TERM is the/DEF total number of examples covered ,/O p+/TERM is the/DEF prior probability of the class (/O 9 , and m/TERM is a/DEF smoothing parameter ./O
Concepts $ 2 $ 30 $ 0 $ 2.3 Distance between Clusters In order to measure the distance between clusters of the same part of speech , we use the following equations : 1 [ ~'[ `` 1~/I ( 1 ) disa ( Ai , Aj ) and lie , U % l ( 2 ) where O~/TERM is the/DEF distribution environment of ~ and is make up of nouns which can be collocated with distribution environment composed of adjectives collocated with N i ./O
Extending_Domain_Semantics_for $ 5 $ 163 $ 6 $ The expression specification first of all defines which verb to use in the expression .
Abstract $ 0 $ 2 $ 1 $ Multi-document summarization differs from single in that the issues of compression , speed , redundancy and passage selection are critical in the formation of useful summaries .
Topic_Analysis $ 4 $ 67 $ 0 $ 4.1 Input and Output In topic analysis , we use STM to parse a given text and output a topic structure which consists of segmented blocks and their topics.
Note_that_the_nouns_at_this_point_are_types_not $ 4 $ 173 $ 58 $ The most interesting result is the result of the MSp condition in table 1 , which indicates that 81.8% of the target data can be sense tagged with an accuracy of 79% , significantly higher than chance ( 25.6% ) as well as it is higher than the default tagging of 67.6% .
Acquisition_Process $ 4 $ 140 $ 41 $ First , the algorithm checks whether the conflicting dictionary head word denotes an acronym ( e . g .
Abstract $ 0 $ 14 $ 4 $ The type/TERM of/TERM an/TERM LCS/TERM node/TERM is one/DEF of Event , State , Path , Manner , Property or Thing ,/O loosely correlated with verbs prepositions , adverbs , adjectives and nouns .
Experiments_and_Discussion $ 4 $ 133 $ 40 $ Accuracy Table 4 shows the relationship between the dimension of the kernel function and the parsing accuracy under the condition k -5 .
Evaluation $ 5 $ 139 $ 6 $ This is understandable , in the sense that the larger the document , the smaller proportion of fixed sections it will contain .
_Relations_that_are_relevant_and_correct_in $ 3 $ 79 $ 45 $ The final score assigned to each polysemous term tl is the highest scorePolyCQi ,s .
Text_Meaning_Representation $ 5 $ 111 $ 9 $ In the example , ASPECT-7 is applied within the scope of GOVERNMENT-ACTIVITY in which TELIC with value YES indicates the GOVERNMENTACTIVITY is complete that means the action 113 of opening foreign trade policy is done .
Robustness $ 4 $ 93 $ 2 $ For each problem found , the preprocessor lowers a confidence value for the generation output which measures the reliability of our result.
The_CGS_system $ 4 $ 69 $ 1 $ Its architecture is a pipeline with several modules , shown in the left hand part of Figure 1 .
Introduction $ 1 $ 31 $ 22 $ 2This corpus is collected and annotated for the GNOME/TERM project/TERM ( Poesio , 2000 ) , which aims/DEF at developing general algorithms for generating nominal expressions ./O
Implementation $ 4 $ 274 $ 103 $ The structure of the tree represents the rhetorical structure of the briefing.
SYSTEM : _U_S-air_flight_63_57_departs_la $ 9 $ 21 $ 14 $ A constraint on a data record is the condition that some given constraint function has a given value .
PAR_as_anIL $ 4 $ 30 $ 19 $ 5 Independent of which is the source language , the PAR schema selected is motion , the activity/TERM  field/TERM , which determines/DEF how the action is performed (/O in this case , by floating ) , is filled by float/TERM ( the/DEF main verb in English , or the adjunct in Spanish )/O .
HMM_for_Mono-Lingual_Retrieval $ 2 $ 18 $ 0 $ Following Miller et al . , 1999 , the IR/TERM system/TERM ranks/DEF documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) ./O
OUT $ -1 $ 69 $ 43 $ R/TERM is the number of tokens in the target string ./O
Closing_thoughts $ 5 $ 203 $ 11 $ I have shown that using covers to abstract collective and distributive readings -and using sets of assignments to represent plural references -yields a search space for this problem which largely mirrors that for singulars , and which avoids computation and search over sets of collections .
Implementation $ 3 $ 86 $ 4 $ It offers all the classical functions for text edition plus a pop-up menu which contains the more probable words or sequences of words that may complete the ongoing translation .
_Relations_that_are_relevant_and_correct_in $ 3 $ 114 $ 80 $ At the top of the list are the items that receive the lowest score , i.e.
October_2000 $ 7 $ 17 $ 16 $ Thus , similarity such that a part/Of-speech tagger developed for one corpus works well in the other , may differ from similarity for Machine Translation .
Abstract $ 0 $ 5 $ 1 $ We show that there is a strong relationship between a learning strategy , its formal learning framework and its logical representational theory .
Introduction $ 1 $ 10 $ 3 $ " 1 In Korean documents , compound nouns are represented in various forms ( shown in Table 1 ) , so there is a difficulty in indexing all types of compound nouns .
The_model $ 1 $ 38 $ 15 $ These morphemes were then linearized in the second network .
Given_an_input_space_X~*_of $ 3 $ 51 $ 20 $ The parameters e and S have the following meaning : e/TERM is the/DEF probability that the learner produces a generalization of the sample that does not coincide with the target concept ,/O while S/TERM is the/DEF probability , given D , that a particularly unrepresentative ( or noisy ) training sample is drawn ./O
The_TransType_model $ 2 $ 15 $ 0 $ 2 .1 User Viewpoint Our interactive translation system is illustrated in figure 1 for an English to French translation .
Summarization_Filters $ 4 $ 130 $ 37 $ For example , Figure 2 illustrates a complex filter created by using a GUI to compose together a named entity extractor , a date extractor , a component which discovers significant associations between the two and writes the result to a table , and a visualizer which plots the results as a graph .
Evaluation $ 3 $ 189 $ 0 $ In the evaluation , the training data is the PH corpus and the test data is the YZZK magazine articles ( 4+ Mbytes ) , downloaded from the Internet .
Building_Spoken_Dialo~te_Systems $ 4 $ 147 $ 42 $ The network/TERM name/TERM is the/DEF identifier of the language model for the speech recognition ./O
Mining_Discourse_Marker_Using $ 6 $ 188 $ 18 $ This information/TERM gain/TERM measures/DEF the expected reduction in entropy and defines one branch for the possible subset Si of the training examples ./O
Abstract $ 0 $ 18 $ 16 $ TIDES/TERM represents/DEF the pinnacle of information access and is a real challenge for MT ./O
Introduction $ 1 $ 25 $ 20 $ In other words , the corpora should have been built using the same stratified sampling method and with , if possible , randornised methods of sample selection.
OUT $ -1 $ 87 $ 45 $ Since a decision tree does not place all the samples with the same current label into a single equivalence class , it does not get stuck in the same situation as a rule list m in which no change in the current state of corpus can be made without incurring a net loss in performance .
Introduction $ 1 $ 9 $ 2 $ Although keywords may offer some indication of " meaning , " they alone cannot capture the richness and expressiveness of natural language .
The_steps_in_the_strategy_are_marked_with_the $ 7 $ 140 $ 18 $ ( Morik 1989 ) describes a system that uses a measure of evidence strength to tailor evaluations of hotel rooms to its users .
Experimental_Design $ 3 $ 180 $ 14 $ These measures may depend on none , one , or all of the collection of ground truth summaries {gj} .
Quality_of_the_first_order_weights $ 4 $ 101 $ 43 $ The disadvantage is a much more time-intensive hill-climbing procedure , but when developing an actual production model , the weights only have to be determined once and the results appear to be worth it most of the time .
Experimental_Setting $ 4 $ 127 $ 26 $ The whole manual set consists of about 2500 annotated nouns.
Experiments $ 6 $ 103 $ 8 $ For the RRE-based system , we mapped the +/5 word window of context into a string as follows ( where wi/TERM is a/DEF word and/O ti/TERM is a/DEF part of speech tag )/O : learned using the standard feature set .
Dialog_Management $ 3 $ 99 $ 38 $ The utility/TERM of/TERM a/TERM path/TERM in the graph is the/DEF summation of the reward/punishment ratio of all the nodes ( subgoals ) in that path ./O
The_WPDV_algorithm $ 1 $ 7 $ 0 $ Weighted/TERM Probability/TERM Distribution/TERM Voting/TERM ( WPDV/ACR ) is a/DEF supervised learning approach to classification ./O
Evaluation $ 4 $ 144 $ 29 $ Such SActually , cousin/TERM is one/DEF of the three relations which indicate the grouping of related senses of a word ./O
Speech_Sound_and_Transcription $ 2 $ 18 $ 0 $ The corpus consists of a collection of 14 taskoriented dialogues , each performed by two native speakers of Japanese.
_General_Outline_of_the_Method $ 2 $ 90 $ 74 $ The reason is that in the training data all word sequences whose previous word is "~" is a not a person name.
_Introduction $ 1 $ 14 $ 8 $ Section 3 describes the preliminary experimental results of our method.
The_hyperonym_problem $ 2 $ 26 $ 8 $ I will call it the hyperonym/TERM problem/TERM [ . . .] : When/DEF lemma A's meaning entails lemma B's meaning , B is a hyperonym of A ./O
The_machine_learning_method $ 3 $ 89 $ 30 $ Work has to be done to determine what is the most appropriate context for this task.
OUT $ -1 $ 0 $ 0 $ TransType/TERM : a/DEF Computer--Aided Translation Typing System Philippe/O Langlais and George Foster and Guy Lapalme RALI/DIRO -Universit@ de Montr@al C.P.
Introduction $ 1 $ 7 $ 3 $ In addition , there is a lack of publicly available Chinese corpus for evaluating Chinese text categorization systems .
Introduction $ 1 $ 12 $ 8 $ The PCFG/TERM obtained in this way consists/DEF of rules that include information about the context where the rule is applied ./O
Abstract $ 0 $ 4 $ 1 $ We tested the predictions made by these measures about corpus difficulty in two domains -news and molecular biology -using the result of two well-used paradigms for NE , decision trees and HMMs and found that gain ratio was the more reliable predictor.
Error_Analysis $ 6 $ 112 $ 11 $ There is a downside to this : sometimes the correct tag is featured in the ambitag , but the algorithm breaks free from the ambitag nevertheless.
OUT $ -1 $ 180 $ 109 $ Following Yarowsky ( 1993 ) , who explicitly addresses the use of collocations in the WSD work , we adopt his definition , adapted to our purpose : A collocation/TERM is a/DEF co/Occurrence of two words in a defined relation ./O
CoreLex $ 2 $ 25 $ 1 $ With this definition , we can construct classes of systematically polysemous words as shown in the CoreLex approach ( Buitelaar 1998a ) ( Buitelaar 1998b ) .
Information_Structures $ 3 $ 82 $ 5 $ Furthermore , the ' transport " event is a ' crime ' and the manner is ' secret ' .
Performance $ 4 $ 172 $ 9 $ Hits/TERM denotes how/DEF many of the files passed to IE actually had at least one template in them and Templates shows how many templates were extracted as a result of the query ./O
Determination_Schemes_of $ 4 $ 119 $ 4 $ Coverage/TERM is the/DEF ratio of the number of actually segmented sentences to the number of segmentation target sentences that are longer than ot words ,/O where o~/TERM is a/DEF fixed constant distinguishing long sentences from short ones ./O
_Decision_trees $ 4 $ 65 $ 0 $ For a set of annotated examples , we used decisiontree tools to construct the conditional probability of a specific grammatical relation , given other features in the domain , z The decision trees are constructed using a Bayesian learning approach that identifies tree structures with high posterior probability ( Chickefing et al. ).
The_Structural_Triggers_Learner $ 2 $ 36 $ 6 $ This is an important open research question which is the focus of a recent research endeavor here at CUNY .
Learning_Algorithms_Tested $ 2 $ 50 $ 9 $ For k ' s greater than 1 , the resulting sense is the weighted majority sense of the k nearest neighbours --where each example votes its sense with a strength proportional to its closeness to the test example .
Abstract $ 0 $ 6 $ 5 $ Such search iterations continue until the user' s ultimate information seeking goal is reached.
Introduction $ 1 $ 20 $ 9 $ Most context-free parsing algorithms have O ( n 3 ) parsing complexities in terms of time and space , where n/TERM is the/DEF length of a sentence (/O Tomita , 1986 ) .
Previous_Schemes_for_Grunt $ 2 $ 54 $ 26 $ Given a grunt , first he must examine the context to determine whether it is a back-channel or a filler , then determine whether it sounds affirmative or negative , and only then can he consider what the actual sound is , and his options are limited to picking one of the labels in the functional/semantic category.
Introduction $ 1 $ 23 $ 19 $ This paper presents WIT 1 , which is a toolkit IWIT/TERM is an/DEF acronym of Workable spoken dialogue lnter150 for building spoken dialogue systems that integrate speech recognition , language understanding and generation , and speech output ./O
Summary_and_future $ 6 $ 97 $ 0 $ developments To summarise , our achievements to date include a method for splitting a binary digit-stream into characters , by using entropy to diagnose byte-length ; a method for tokenising unknown character-streams into words of language ; - an approach to chunking words into phraselike sub-sequences , by assuming high-frequency function words act as phrase-delimiters ; - a visualisation tool for exploring word-combination patterns , where word-pairs need not be immediate neighbours but characteristically combine despite several intervening words .
Technique_description $ 2 $ 50 $ 21 $ ) is frequency , L/TERM is the/DEF set of left adjacent strings of X ,/O tz~L and ILl/TERM means the/DEF number of unique left adjacent strings ./O
Translations_selection_and_phrase $ 2 $ 50 $ 0 $ keeping It is a naive method to translate a Chinese query only by looking up each Chinese term to get its English senses in a Chinese-English dictionary .
Examples_of_YAG_in_use $ 3 $ 69 $ 4 $ In this representation , M2/TERM is the/DEF proposition that the discourse entity B2 is a member of class dog ./O
Word_Clustering $ 3 $ 52 $ 8 $ 2 For a fixed seed word s , we take a word w as a frequently/TERM co/Occurring/TERM word/TERM if/DEF the presence of s is a statistically significant indicator of the presence of w ./O Let a data sequence : ( sl , wl ) , ( s2 , w2 ) , .-. , ( Sin , Win ) be given where ( si , wi ) denotes the state of co/Occurrence of words s and w in the i-th text in the corpus data .
OUT $ -1 $ 145 $ 74 $ For example , ( A , Adv , D ) is a dependency type expressing the relationship between words in expression `` very large '' where `` very '' is a depending adverb and `` large '' is a head adjective .
Storing_the_corpus_in_a_database $ 4 $ 132 $ 11 $ The colnmn/TERM cl_id/TERM in the table node_pair_/ , for example , is a/DEF foreign key referring to the colnmn clad in the table pair_class ./O
Corpora $ 3 $ 60 $ 2 $ 3 .1 MUC-6 The/TERM corpus/TERM for/TERM MUC-6/TERM ( MUC , 1995 ) contains/DEF 60 articles , from the test corpus for the dry and formalruns ./O
Stochastic_Surface_Realization $ 2 $ 82 $ 24 $ ( n.1 ) , U ) where u/TERM is the/DEF utterance class ./O
Why_Reading_Comprehension $ 2 $ 67 $ 13 $ The compound noun maple sap is a semantically narrower term than the noun sap and encodes an implicit relation between the first element maple and the head noun sap.
Selection_of_candidate_strings $ 2 $ 42 $ 18 $ The IWP/TERM of/TERM a/TERM single/TERM character/TERM is the/DEF likelihood for this character to appear as an independent word in texts :/O N ( Word ( c ) ) IWP ( c ) = N ( c ) where N/TERM (/TERM Word/TERM (/TERM c/TERM )/TERM )/TERM is the/DEF number of occurrences of a character as an independent word in the sentences of a given text corpus and/O N/TERM (/TERM c/TERM )/TERM is the/DEF total number of occurrence of this character in the same corpus ./O
System_Overview $ 3 $ 54 $ 18 $ One conjoined constituent is marked as the foot node , and the other is expanded into a spine-etree whose head is the anchor of the whole tree .
Results_and_discussion $ 4 $ 184 $ 5 $ QS1/TERM is the/DEF subset of questions whose number of morphological derivations and synonyms is higher than three ;/O QS2/TERM is the/DEF subset whose number of lexical expansions is equal to two or three ;/O QS3/TERM is the/DEF subset whose number of lexical expansions is lower than two ./O
Focus_and_word_order $ 1 $ 11 $ 1 $ In many languages , intonation can reflect pragmatically motivated conceptual decisions .
The_lexicon_size_of_a_typical_large-vocabulary $ 9 $ 149 $ 70 $ We thus plan to report ranked retrieval measures of effectiveness such as average precision in addition to the detection/TERM statistics/TERM ( miss/DEF and false alarm )/O typically reported in TDT.
Statistical_Semantic_Parsing $ 4 $ 102 $ 3 $ S is the set of states reachable by the parser .
Sample_Selection $ 2 $ 44 $ 16 $ L/TERM is a/DEF set of labeled training examples ./O
Definitions $ 2 $ 35 $ 10 $ IqJ]l/TERM is the/DEF number of symbols in the jt~ string of the corpus ./O
OUT $ -1 $ 112 $ 70 $ Considering chunk tags within a contextual window of the target word raises a problem with C4 .5 .
_Annotation_Guidelines_I : $ 3 $ 52 $ 6 $ This set defines the domain of expressed syntactic information ( instead of projected or inherited information ) .
Dialogue_Management $ 2 $ 165 $ 121 $ The static/TERM part/TERM consists/DEF of preconditions , goal , content ( immediate act ) and consequences ./O
Parsing_with_fsgmatch $ 5 $ 64 $ 2 $ The rule consists of adding the attributes CAT=' AL' S='NP' ( left adjunct of a Noun Phrase ( NP ) ) to each word with the attribute C='VBG' which occurs after a word with the attribute C= ' DT' .
Joining_TM2_and_DTD $ 3 $ 84 $ 5 $ . Each record/TERM in the database consists of four/DEF fields : the segment string , a counter for the occurrences of that string in the corpus , the tag and the attributes ( type , id and corresp ) ./O
Introduction $ 1 $ 39 $ 33 $ The Bayesian Times reporting Mr Body took Mr Green ' s girlfriend implies Mr Green and Mr Body possibly were enemies , which implies Mr Green possibly had a motive to murder Mr Body .
Results $ 3 $ 82 $ 28 $ For VP , on the other hand , there is an accuracy increase , probably due to a corrected inclusion/exclusion of participles into/from NPs .
SYSTEM : _i_see_a_few_flights_from_new $ 5 $ 2 $ 0 $ york to washington national which depart about ten A M on january twenty seventh .
Implementation $ 6 $ 137 $ 7 $ The evaluation function of terminals is actually a constant function whose return value is the value to which the terminal has been bound.
Centroid-based_summarization $ 4 $ 78 $ 6 $ 101 The output consists of a sequence of In * r] sentences from the original documents in the same order as the input documents .
Complexity $ 3 $ 83 $ 0 $ This section puts forward some notes on the complexity of dialogue.
Statistical_Semantic_Parsing $ 4 $ 124 $ 25 $ Suppose q = an+l ( Sm ) , we have : P ( q 6 Q ( l ) ) ( 10 ) = P ( s~ • F~ ) ... = P ( s ,n • FS + l sm-1 •/St ,_a ) ... P ( s~ • OS~_ , I sj-1 • Is~_ , ) ... P ( s2 • Ob~ , Is1 • IS~ , ) P ( 'I • IS~ , ) where ak/TERM denotes the/DEF index of which action is applied at the kth step ./O
Task_description $ 2 $ 9 $ 0 $ Text/TERM chunking/TERM consists of dividing/DEF a text into phrases in such a way that syntactically related words become member of the same phrase ./O
_Preliminary_Evaluation $ 3 $ 108 $ 53 $ The senses with the highest confidence scores are the senses that contribute the most to the maximization function for the set .
Empirical_Evaluation $ 4 $ 126 $ 0 $ 4.3 Performance Measures Our experiments adopt the most commonly used performance measures , including the recall , precision , and F1 measures.
_Limitations $ 5 $ 34 $ 8 $ TRIPS-911 can currently interpret expressions with respect to the actual time .
System_Overview $ 3 $ 132 $ 96 $ 57 etrees.
Hyperonymy_in_lexical_semantics $ 5 $ 103 $ 6 $ And hence , there is a difference between fish ISA creature and fish ISA animal .
Abstract $ 0 $ 13 $ 11 $ Coverage/TERM was measured/DEF by having human IF specialists annotate unseen data ./O
Summary_and_Conclusions $ 6 $ 128 $ 2 $ The product consists of a tree of handlers , each handler/TERM encapsulates/DEF processing relevant to a particular schema ./O
OUT $ -1 $ 26 $ 23 $ A single scenario for the colour domain In order to learn a rule set for a concept , EVIUS uses the relational learning method explained in section 3 , and defines the learning space by means of a dynamic predicate model.
Tree_Generalization_using_Tree-cut $ 2 $ 61 $ 15 $ The MDL/TERM is a/DEF principle of data compression in Information Theory which states that , for a given dataset , the best model is the one which requires the minimum length ( often measured in bits ) to encode the model ( the model description length ) and the data ( the data description length ) ./O
Stochastic_Topic_Model $ 2 $ 40 $ 9 $ The STMs within a text are assumed to have the same set of topics , but have different parameter values .
Abstract $ 0 $ 3 $ 1 $ Preliminary results 1 across three language-specific FALCon 2 systems show that , with one exception , the derived measures consistently yield the same performance ranking : Haitian Creole at the low end , Arabic in the middle , and Spanish at the high end .
Optimizations $ 5 $ 88 $ 3 $ Define GoodPotential/TERM 0 to I ( S ) as the/DEF number of sentences s in the training corpus for which Guess[s]=0 , Truth[s]= 1 and 3k : ( s , k ) ~ corpus_position_set ( S ) ./O
The_Feasibility_of_the_STL $ 3 $ 54 $ 12 $ our purposes , the number of relevant parameters , r/TERM , is the/DEF total number of parameters that need to be set in order to license all and only the sentences of the target language ./O
Principles_and_Parameters $ 1 $ 28 $ 21 $ The motto of such a learner is : Don't learn . from ambiguous input and learning efficiency can be measured by the number of sentences the learner has to wait for usable , unambiguous inputs to occur in the input stream .
Evaluation $ 4 $ 81 $ 5 $ The test/TERM set/TERM here consists/DEF of the 3260 manually classified senses ./O
