Abstract $ 0 $ 130 $ 26 $ CRF $ More features are introduced in our method , such as the linear order of entity names , the word ( s ) between the entity names , the relative/TERM position/TERM of/TERM the/TERM entity/TERM names/TERM ( in/DEF one/DEF sentence/DEF or/DEF in/DEF neighboring/DEF sentences/DEF ) , etc . 
Utterance-Level_Robustness $ 4 $ 50 $ 7 $ PATTERN $ We collected four measures of performance : • Recognition time , measured , in multiples of CPU/DEF real/DEF time/DEF ( CPURT/TERM ) .
Whole_Sentence_Maximum $ 2 $ 22 $ 2 $ CRF $ The probability/TERM distribution/TERM is the distribution/DEF p/DEF that/DEF has/DEF the/DEF maximum/DEF entropy/DEF relative/DEF to/DEF a/DEF prior/DEF distribution/DEF P0/DEF (/O in other words : the distribution that minimize de divergence D ( pllpo ) ) ( Della Pietra et al. , 1995 ) . 
Approach $ 2 $ 31 $ 25 $ KP $ Like with the voting algorithms , we have tested these meta-classifiers with the output of the first classification stage.
Abstract $ 0 $ 21 $ 20 $ KP $ If each corpus generates a separate set of probabilities , which probabilities are the correct ones to use as a model of human language processing?
Description_of_the_algorithm $ 2 $ 16 $ 2 $ PATTERN $ We assume that in the spoken dialog a sentence consists of slightly related subphrases.
_Conclusion $ 4 $ 128 $ 5 $ PATTERN $ Although the five categories are defined by us , they can describe basic situations of Chinese.
Summary_and_future $ 6 $ 97 $ 0 $ CRF $ developments To summarise , our achievements to date include a method for splitting a binary digit-stream into characters , by using entropy to diagnose byte-length ; a method for tokenising unknown character-streams into words of language ; - an approach to chunking words into phraselike sub-sequences , by assuming high-frequency function words act as phrase-delimiters ; - a visualisation tool for exploring word-combination patterns , where word-pairs need not be immediate neighbours but characteristically combine despite several intervening words . 
Related_Work $ 4 $ 117 $ 26 $ PATTERN $ Among its effects , there is the fact that B comes to know A's choice ( refused and grounded attribute are set to "true" ) .
Introduction $ 1 $ 30 $ 23 $ PATTERN $ The Penn/TERM Treebank/TERM for example consists/DEF of trees with an additional coindexation relation ,/O Negra allows crossing branches and in Verbmobil , an element ( a tree-like structure ) in the corpus might contain completely disconnected nodes.
OUT $ -1 $ 59 $ 17 $ PATTERN $ Let R/TERM (/TERM z/TERM )/TERM to be the/DEF set of rules r that applies to the state el ( z ) ,/O R ( z ) = {ri ~ 7~Ir~ applies to si ( z ) } An equivalence/TERM class/TERM consists/DEF of all the samples z that have the same R ( z ) ./O
Abstract $ 0 $ 4 $ 1 $ PATTERN $ We tested the predictions made by these measures about corpus difficulty in two domains -news and molecular biology -using the result of two well-used paradigms for NE , decision trees and HMMs and found that gain ratio was the more reliable predictor.
OUT $ -1 $ 157 $ 157 $ PATTERN $ Defined as " an/DEF optimized body of coordinated on-line methods and resources that enable and maintain a person's or an organization's performance , "/O EPSS/TERM interventions range from simple help systems to intelligent wizard-types of support.
Tagged_Text $ 3 $ 114 $ 100 $ PATTERN $ If a noun is the object of a verb , then the subcat feature value of the verb can be used to disambiguate its word sense ( e.g. , take generally has the subcat of obj+time ) .
Abstract $ 0 $ 76 $ 15 $ PATTERN $ REINTERPRET_data.structures/TERM like ( 5 ) as compatible/DEF with descriptions of collections as well as singletons ./O
Parsing_with_fsgmatch $ 5 $ 64 $ 2 $ PATTERN $ The rule consists of adding the attributes CAT=' AL' S='NP' ( left adjunct of a Noun Phrase ( NP ) ) to each word with the attribute C='VBG' which occurs after a word with the attribute C= ' DT' .
The_steps_in_the_strategy_are_marked_with_the $ 7 $ 140 $ 18 $ PATTERN $ ( Morik 1989 ) describes a system that uses a measure of evidence strength to tailor evaluations of hotel rooms to its users.
Conclusions_and_Future_Work $ 5 $ 260 $ 4 $ PATTERN $ Content-based/TERM measures/TERM assign/DEF different rankings when ground truths do disagree in focus ./O
Results $ 6 $ 133 $ 6 $ CRF $ We deem one evaluation function more effective than another if the smallest set of sentences it selected can train a grammar that performs at least as well as the grammar trained under the other function and if the selected data contains considerably fewer brackets than that of the other function . 
Conclusion $ 5 $ 103 $ 2 $ PATTERN $ TRANSTYPE/TERM is a/DEF project funded by the Natural Sciences and Engineering Research Council of Canada ./O
Prosody_Prediction $ 4 $ 63 $ 0 $ KP $ The simple method that we have firstly used is the nearest/TERM neighbour/TERM algorithm/TERM : given/DEF a new sentence , the closest match among the corpus of sentences of known prosody is retrieved and used to infer the prosody of the new sentence ./O
Learning_Algorithms_Tested $ 2 $ 46 $ 26 $ CRF $ When testing , the decision list is checked in order and the feature with the highest weight that matches the test example is used to select the winning word sense . 
System_Overview $ 3 $ 101 $ 65 $ CRF $ Furthermore , Eset is the only tree set that satisfies all the following conditions : ( C1 ) Decomposition/TERM : The/DEF tree set is a decomposition of T* , that is , T* would be generated if the trees in the set were combined via the substitution and adjunction operations ./O 
_Summary_from_Common_Sections_and_Unique_Sec $ 6 $ 146 $ 59 $ CRF $ AP880823-0069 : 17 The ANC/TERM is the/DEF main guerrilla group fighting to overthrow the South African government and/O end apartheid/TERM , the/DEF system of racial segregation in which South Africa 's black majority has no vote in national affairs ./O 
Future_Research_Issues $ 5 $ 128 $ 6 $ CRF $ Inductive logic programming ( MDR94 ; Coh95 ) is a natural paradigm for this . 
Surface_realization_as_grammatical $ 5 $ 145 $ 3 $ PATTERN $ While tile/TERM realization/TERM of/TERM the/TERM focus/TERM domain/TERM is the/DEF task of converting the complete focus into one phrase ,/O word order will be determined by LP-rules that pick up the pragmaticall2 , ' motivated literals on topichood , identifial ) ility , and referential movement .
Introduction $ 1 $ 10 $ 3 $ PATTERN $ TIVIR/TERM captures/DEF the meanings of words in the text and represents them in a set of ontological concepts interconnected through ontological relations ./O
Selection_of_candidate_strings $ 2 $ 35 $ 11 $ PATTERN $ Here is an example : After dictionary look up , we get which is a sequence of 10 single characters.
_The_UNL_Project $ 2 $ 27 $ 1 $ KP $ Its main strength lies on the development of the UNL/TERM , as a/DEF unique semantic ( or meaning ) representation that can be interchanged with the various languages to be integrated in the KBMT system ./O
Introduction $ 1 $ 4 $ 0 $ PATTERN $ GTAG/TERM is a/DEF multilingual text generation formalism derived from the Tree Adjoining Grammar model ( ( Joshi/O and al. , 1975 ) , ( Shabes and Shieber , 1994 ) ) .
Selective_Sampling_Evaluation $ 4 $ 91 $ 15 $ PATTERN $ Entropy/TERM measures/DEF the uncertainty of assigning a value to a random variable over a distribution ./O
See_the_report_entitled_ $ 3 $ 24 $ 14 $ PATTERN $ related w~ tagged TL do domain wore Figure 1 Analysis Template Measures 2 An Embedded MT System Design 4 Our three systems process documents using a sequence of three software modules.
Abstract $ 0 $ 73 $ 42 $ CRF $ In the deeper linguistic analysis the/TERM two/TERM so/TERM 's/TERM may be related , for they refer to a/DEF situation involving excessive height with implied consequence which may or may not be stated ./O 
OUT $ -1 $ 67 $ 62 $ PATTERN $ One exception is the work of ( Radev et al. , 2000 ) at the TREC-8 QA track , which uses logistic regression to rank potential answers using a training set with seven features.
Information_extraction $ 3 $ 40 $ 5 $ PATTERN $ The latter situation is the most difficult to handle since time evolution needs to be considered.
OUT $ -1 $ 37 $ 37 $ KP $ The number of documents in a batch may vary from a few to hundreds.
Conclusion $ 7 $ 204 $ 1 $ CRF $ To our knowledge our system is the only one that uses semantic representation as basis for summarizing . 
_Introduction $ 1 $ 15 $ 8 $ CRF $ Most of the methods , to date , aim at solving the problem for one language , namely the language with the most available linguistic resources . 
Given_an_input_space_X~*_of $ 3 $ 75 $ 44 $ PATTERN $ Probabilistic/TERM learners/TERM usually/DEF associate to uncertain information a measure of the confidence the system has in that information ./O
Interclausal_Coherence $ 4 $ 124 $ 34 $ CRF $ In the simplest case , the relation/TERM is one/DEF of Elaboration ./O 
The_Complexity_of_Extracting_a $ 2 $ 68 $ 0 $ CRF $ As mentioned earlier , the level of a fact for a piece of text depends on the network constructed for the text . 
The_model $ 1 $ 29 $ 6 $ PATTERN $ The hotel/TERM Regina/TERM is a/DEF small hotel ./O
Example_Dialogue $ 3 $ 54 $ 31 $ KP $ cheerful ( U10 ) ) may be much more profound than a shorthand for December 25 -but , alas , conveying that is well beyond the simple grammar presented here.
Results $ 3 $ 56 $ 4 $ KP $ Base-NP/TERM chunking/TERM ( NPSM/TERM ) : the/DEF segmentation of sentences into non-recursive NPs ./O
Introduction $ 1 $ 13 $ 6 $ CRF $ where appropriate target language lexical items are chosen for each source language lexical item and ( b ) reordering phase where the chosen target language lexical items are reordered to produce a meaningful target language string . 
_Functional_tag_of_the_constituent $ 5 $ 56 $ 0 $ KP $ embedding/TERM the/TERM NP/TERM : If/DEF the category of the constituent embedding the NP is associated with one or more functional tags , they are used as features ./O
Introduction $ 1 $ 13 $ 7 $ PATTERN $ ( 1996 ) , who study the topic of author identification , apply statistical measures and methods on syntactic rewrite rules resulting by processing a given set of texts.
Related_Work $ 2 $ 30 $ 3 $ PATTERN $ Concept/TERM matching/TERM is a/DEF technique that has been used in limited domains ,/O like the legal field were conceptual indexing has been applied by ( Stein , 1997 ) .
OUT $ -1 $ 142 $ 71 $ PATTERN $ An SDR/TERM consists/DEF of two words and a dependency type ./O
Abstract $ 0 $ 16 $ 14 $ KP $ Strube and Hahn ( Strube , 1998; Strube and H~.hn , 1999 ) in particular , calculate prominence/DEF considering the information structure of the utterances (/O functional/TERM centering/TERM ) .
_Measurements $ 3 $ 85 $ 18 $ PATTERN $ The first collection to be seen moving up the headline at a remove of two nodes ( the main verb and the vp ) is the conjunction of companies.
_General_Outline_of_the_Method $ 2 $ 35 $ 19 $ PATTERN $ IG-Tree/TERM is a/DEF compressed representation of the training set that can be processed quickly in classification process ./O
Overview_of_the_Approach $ 2 $ 32 $ 12 $ CRF $ For example , largest/TERM (/TERM X/TERM ,/TERM Goal/TERM )/TERM states that the/DEF object X satisfies Goal and is the largest object that does so , using the appropriate measure of size for objects of its type (/O e.g . 
Introduction $ 1 $ 9 $ 0 $ PATTERN $ Word/TERM Sense/TERM Disambiguation/TERM ( WSD/ACR ) is the/DEF problem of assigning the appropriate meaning ( sense ) to a given word in a text or discourse ./O
Prosodic_Information_and $ 3 $ 30 $ 0 $ CRF $ 3.2 Part/Of-speech The part/Of-speech/TERM is another basic/DEF information for speech recognition , syntactic/semantic parsing , and dialogue processing as well as linguistic and psycholinguistic analysis of spoken discourse ./O 
Our_approach_to_Multilingual $ 2 $ 31 $ 4 $ PATTERN $ Thus , the author is terlingua/TERM ( specific/DEF to the class of documents being always overtly working in the language s/he nows ,/O modelled ) , and it is the responsibility of appropribut is implicitly building a language-independent ate "rendering" mechanisms to produce actual text representation of the document content.
OUT $ -1 $ 48 $ 6 $ PATTERN $ The final classification is the one attained when all rules have been applied.
Approach $ 2 $ 38 $ 11 $ KP $ Each node has a label/TERM , which offers/DEF a brief textual description of the node ./O
Results $ 5 $ 69 $ 24 $ KP $ tags chosen by linguists would score very badly against another without this implying any fault as there is no 'gold standard'.
Tree_Generalization_using_Tree-cut $ 2 $ 54 $ 8 $ CRF $ Thus , a treecut/TERM corresponds to one/DEF of the levels of abstraction in the tree ./O 
_Tagging_NuLL-marker_CDM_pairs_ ( via $ 8 $ 118 $ 48 $ PATTERN $ " denotes that no corresponding word is at the position ( beginning or end of sentence ) ; a , d , q , and u are part/Of-speech symbols in our segmentation dictionary , representing adjective , adverb , classifier , and auxiliary , respectively.
LTAGs_and_Extraction $ 3 $ 39 $ 12 $ CRF $ Figure 2 shows the etrees , the derived tree , and the derivation tree for the sentence underwriters still draft policies . 
BOV_Stem_NE_Defaults_Qspecific_41 $ 5 $ 64 $ 34 $ PATTERN $ Of course , our application is sentence retrieval , not document retrieval , so we define term/TERM frequency/TERM as the/DEF number of times the word appears in the candidate sentence ,/O and document/TERM frequency/TERM as the/DEF number of sentences in which this word appears ./O
_Conclusion $ 4 $ 112 $ 3 $ CRF $ It does not assume that the user receives any training in search and retrieval or has any prior experience in using Internet . 
Introduction $ 1 $ 78 $ 69 $ PATTERN $ The only consideration is the computation of the chain score.
Dialog_Management $ 3 $ 99 $ 38 $ CRF $ The utility/TERM of/TERM a/TERM path/TERM in the graph is the/DEF summation of the reward/punishment ratio of all the nodes ( subgoals ) in that path ./O 
Reinterpretation_of_CGS_in_RAGS $ 5 $ 109 $ 9 $ PATTERN $ Text Planner The input to the Longbow text planner discussed in section 4 above is a representation of a picture in SAGE/TERM format/TERM ( which has been annotated/DEF to indicate the types of complexity of each grapheme )/O together with a goal/TERM , which can/DEF typically be interpreted as "describe" ./O
Selective_Sampling_Evaluation $ 4 $ 101 $ 25 $ CRF $ To normalize for sentence length , we define an evaluation function that computes the similarity between the actual probability distribution and the uniform distribution for a sentence of that length . 
Experimental_Results $ 7 $ 246 $ 79 $ PATTERN $ Error/TERM probability/TERM is a/DEF metric for evaluating segmentation results proposed/O in ( Allan et ai. , 1998; Beeferman etal. , 1999 ) .
Results $ 6 $ 190 $ 12 $ PATTERN $ Since referent S was identifiable by /i definite description for the listener and is the topic , it remains identifiable by definite means , resulting in a definite NP.
Impact_of_Lexicon_Size $ 9 $ 191 $ 53 $ CRF $ • Rather than translation ambiguity , a more serious limitation to effective cross-lingual IR is incompleteness of the bilingual lexicon used for query translation . 
Applications $ 3 $ 62 $ 2 $ CRF $ This is a subcorpus of circa/TERM 4.5/TERM million/TERM words/TERM , in/DEF which speakers and respondents are identified by such factors as gender , age , social group and geographical region ./O 
Abstract $ 0 $ 146 $ 4 $ KP $ Le VIDAL ® includes a collection of notices , for .around• 5 5.00. dmgs..a~ailable .in France.
Abstract $ 0 $ 76 $ 15 $ CRF $ is a suggestion : REINTERPRET_data.structures/TERM like ( 5 ) as compatible/DEF with descriptions of collections as well as singletons ./O 
Comparative_analysis_~of_Japanese_and $ 3 $ 90 $ 2 $ PATTERN $ Chinese/TERM is a/DEF non-inflectional language and/O therefore morphological analysis is not essential.
Interlingua_system_ ( ISS ) $ 3 $ 69 $ 0 $ KP $ As said before , the Interlingua/TERM system/TERM takes/DEF the SS of the sentence after applying the anaphora resolution module as input ./O
Evaluation $ 4 $ 81 $ 5 $ PATTERN $ The test/TERM set/TERM here consists/DEF of the 3260 manually classified senses ./O
Informational_content_of_sentences $ 2 $ 34 $ 10 $ CRF $ 2 in the second article , while sentence ' 9 from the former article is later repeated in sentences 3 and 4 of the latter article . 
Conclusions_and_Future_Work $ 5 $ 261 $ 5 $ PATTERN $ In addition , these measures provide a finer grained score with which to compare summaries.
Prerequisites $ 2 $ 19 $ 0 $ PATTERN $ VERBMOBIL/TERM is a/DEF speech-to-speech translation project , which at present is approaching its end and in which over 100 researchers 1 at academic and industrial sites are developing a translation system for multilingual negotiation dialogues ( held face to face or via telephone ) using English , German , and Japanese ./O
The_Generation_System $ 3 $ 125 $ 83 $ PATTERN $ For the LCS-AMR in Figure 3 , the thematic/TERM hierarchy/TERM is what/DEF determined that the lunited statesl is the subject and Iquotal is the object of the verb Ireducel ./O
The_CGS_system $ 4 $ 97 $ 29 $ CRF $ ( 4 ) The color shows the neighbourhood and ( 5 ) shape shows the listing agency . 
Architecture_of_WIT-Based_Spoken $ 3 $ 95 $ 61 $ CRF $ If the system holds the initiative , the module executes the initial function of the phase . 
Comparison_experiment $ 3 $ 167 $ 69 $ CRF $ `` it is not sufficient that the string `` Robert Sheckley '' or `` Sheckley '' is in the text , but the document has to say that Robert Sheckley is the author of Options . 
How_to_generate_technical $ 2 $ 89 $ 38 $ CRF $ o The lexicalized/TERM grammar/TERM in G-TAG is compiled/DEF from the recta-grammar designed and implemented by M.H ./O 
Related_Work $ 6 $ 161 $ 21 $ KP $ Finite/TERM mixture/TERM models/TERM have been used/DEF in a variety of applications in text processing (/O e.g. , ( Li and Yamanishi , 1997; Nigam et al. , 2000; Hofmann , 1999 ) ) , indicating that they are essential to text processing.
Models_and_Modifications $ 2 $ 62 $ 49 $ PATTERN $ Xia ( 1999 ) describes a similar process , and in fact our rules for the Xinhua corpus are based on hers.
_Background $ 2 $ 29 $ 13 $ PATTERN $ Chinese/TERM is a/DEF syllable-based language , where each syllable carries a lexical tone ./O
Introduction $ 1 $ 11 $ 4 $ CRF $ In particular we are interested in one-pass decoding and translation of speech as opposed to the more prevalent approach of translation of speech lattices . 
Introduction $ 1 $ 14 $ 8 $ PATTERN $ They report that the accuracy thus obtained is higher than when applying the same statistical measures to the original text.
OUT $ -1 $ 134 $ 97 $ PATTERN $ Resuming the example shown in Figure 1 , this is the case of the UW "on" in ( lb ) : the preposition 'on' fills in the position feature of the verb 'sit' and , thus , is represented in UNL correspondingly as the second term of the binary relation 'plc' and the first term of 'obj'.
Technique_description $ 2 $ 50 $ 21 $ CRF $ ) is frequency , L/TERM is the/DEF set of left adjacent strings of X ,/O tz~L and ILl/TERM means the/DEF number of unique left adjacent strings ./O 
_Generation_of_the_surface_phrase_from_the $ 4 $ 172 $ 135 $ CRF $ The time/TERM for/TERM Question-a/TERM is a/DEF sum of the times for Questions al and a2 ./O 
EXOT $ 9 $ 108 $ 36 $ KP $ SEMCAT/TERM weights/TERM are calculated/DEF based on the following equations ./O
Closing_thoughts $ 5 $ 203 $ 11 $ CRF $ I have shown that using covers to abstract collective and distributive readings -and using sets of assignments to represent plural references -yields a search space for this problem which largely mirrors that for singulars , and which avoids computation and search over sets of collections . 
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 112 $ 77 $ PATTERN $ In a similar way , Wpit/TERM denotes TF*IDF/DEF of the term t in the i-th paragraph ./O
MALIN $ 4 $ 125 $ 19 $ PATTERN $ For simple information requests we have identified two important concepts , termed Objects and Properties ( JSnsson , 1997 ) where Objects/TERM models the/DEF set of objects in the database and/O Properties/TERM denotes a/DEF complex predicate ascribed to this set ./O
Sentence_planning_in_FOGS $ 4 $ 90 $ 3 $ PATTERN $ Our discourse/TERM model/TERM is a/DEF knowledge store consisting of two major registers ./O
Background $ 2 $ 22 $ 2 $ CRF $ Third , we present some of our current primitives , and finally , we describe the dialogue/TERM engine/TERM and how it uses/DEF the application description and other sources to calculate dialogue primitives ./O 
Explaining_Probabilistic_Methods $ 3 $ 73 $ 25 $ PATTERN $ Consequently , the/TERM Bayes/TERM optimal/TERM prediction/TERM is given by : h/DEF ( x ) = argmaxteLH~n=l Pr ( xill ) Pr ( 1 ) , where Pr ( 1 ) denotes the prior probability of l ( the fraction of examples labeled l ) and Pr ( xill ) are the conditional feature probabilities ( the fraction of the examples labeled l in which the ith feature has value xi ) ./O
The_Generation_System $ 3 $ 56 $ 14 $ CRF $ Thus , for example , the following structure ( with some aspects elided for brevity ) represents a node that could be one of three possibilities . 
Experiments $ 5 $ 87 $ 4 $ KP $ The NB/TERM ( naive/DEF Bayes/DEF ) and SNoW classifiers use the same feature set , conjunctions of size 3 of POS tags ( + words ) in a window of size 6 around the target word.
Reference_Resolution $ 4 $ 126 $ 27 $ PATTERN $ The same procedure can be used to establishing that the reference of Doctor Andreu is the same as that of Docteur Andreu : establish the semantic class of Doctor Andreu , inspect each existing referent of that class to 5 see whether or not a plausible connection can be established.
_IBM_expected_[SBAa_each_employee_to $ 5 $ 46 $ 34 $ PATTERN $ Here are the first few elements generated by the model for the tree of Figure 1 : 1.
Bridging_Natural_Language_and $ 4 $ 106 $ 27 $ PATTERN $ Finally , ternary expressions are highly amenable to rapid large-scale indexing , which is a necessary prerequisite of information retrieval systems.
Proper_name_recognition $ 4 $ 85 $ 11 $ CRF $ Besides part of speech , the only other information used by the recognizer is the lexical status of words , i.e . 
Definitions $ 2 $ 25 $ 0 $ PATTERN $ Below we provide the standard definition for regular expressions , and then define a/DEF less expressive language formafism ,/O which we will refer to as reduced/TERM regular/TERM expressions/TERM .
The_TABULATE_ILP_Method $ 3 $ 95 $ 29 $ PATTERN $ The metric/TERM M/TERM (/TERM H/TERM )/TERM used as the search heuristic is defined as : M/DEF ( H ) = accuracy ( H ) + C log 2 size ( H ) ( 4 ) where C is a constant used to control the relative weight of accuracy vs. complexity ./O
OUT $ -1 $ 146 $ 95 $ PATTERN $ What we need is a combination that is coherent enough for people to understand.
Introduction $ 1 $ 12 $ 6 $ PATTERN $ To illustrate the operation of BIAS and its rebuttal capability , consider the exchange/TERM in Figure 1 , which consists/DEF of a preamble that contains background information~ followed by an argument generated by BIAS , a user's rejoinder and BIAS' rebuttal ./O
OUT $ -1 $ 68 $ 15 $ KP $ Continuous/TERM Speech/TERM Recognition/TERM ( CSR/TERM ) 9 for audio indexing , followed by text retrieval techniques.
Feature_Selection_and_Extraction $ 2 $ 25 $ 3 $ PATTERN $ As a consequence , word segmentation is a major issue in Chinese document processing.
Implementation $ 5 $ 137 $ 3 $ PATTERN $ The generation/TERM component/TERM consists/DEF of the following subcomponents : Decomposition and lexlcal selection First , primitive LCSes for words in the target language are matched against CLCSes , and tree structures of covering words are selected ./O
OUT $ -1 $ 116 $ 79 $ KP $ The NL-UNL/DEF encoding tool ,/O or UNL/TERM Encoder/TERM , is generic enough to handle all the 29 languages included in the Project.
_Introduction $ 1 $ 18 $ 9 $ CRF $ What makes the interlingua/TERM UNL/TERM special is its intended use : as an/DEF electronic language for networks ,/O it has to allow for high quality 2 conversation systems involving many languages . 
_KNOWLEDGE_EXTRACTION $ 3 $ 89 $ 31 $ PATTERN $ The samples of organizations from the CKIP dictionary As we observed , the morphological structure of an organization name usually is a compounding of a proper name and a organization type.
The_query_language $ 3 $ 94 $ 37 $ PATTERN $ Lt/TERM ~/TERM C/TERM is a/DEF total function ./O
OUT $ -1 $ 268 $ 78 $ PATTERN $ An example is the annotation framework recently proposed by Bird and Liberrnan ( 1999 ) which is based on annotation graphs.
Degree_of_Polysemy_in_Mandarin_Chinese $ 2 $ 28 $ 0 $ PATTERN $ The degree/TERM of/TERM polysemy/TERM is defined as the/DEF average number of senses of words ./O
Comparison_experiment $ 3 $ 174 $ 76 $ PATTERN $ where p/TERM (/TERM i/TERM )/TERM is the/DEF position of the web document in the ordered list ./O
_The_session_was_also_attended_by_observers_from_the_following_international_organizations : $ 7 $ 164 $ 113 $ CRF $ In this context , there is no clue for deciding whether Ashley Judd is a star or an athlete . 
Recognition $ 9 $ 130 $ 5 $ PATTERN $ In Figure 6 , acceptable/TERM is the/DEF sum of perfect and ok scores ,/O s Figure 6 shows the results of the intra-site and inter-site evaluations.
Background $ 2 $ 78 $ 58 $ KP $ withdraw/TERM (/TERM p/TERM )/TERM : system/DEF withdraws from dialogue for reason p ./O
The_TransType_model $ 2 $ 77 $ 62 $ PATTERN $ The passive/TERM vocabulary/TERM is a/DEF large dictionary containing over 380 ,000 word forms ./O
Markov_Modeling $ 2 $ 23 $ 0 $ CRF $ HMM/TERM is a/DEF probabilistic finite state automaton used to model the probabilistic generation of sequential processes ./O 
Introduction $ 1 $ 16 $ 14 $ KP $ From the requester's point of view , it results in 84 the production of complex linguistic forms aimed at reducing the potential offence intrinsic to a demand to act ( conversationally or behaviorally ) ; from the requestee's point of view , while acceptance normally addresses the requester's potential offence by a displaying of good-tempered feelings , any refusal at the conversational or behavioral level constitutes in turn a potential offence to the requestee's face , and sets up the social need for the refusing agent to act in order to nullify this potential offence ( Goffman , 1981 ) .
The_MATE_Markup_Framework $ 3 $ 167 $ 79 $ PATTERN $ An example is the following of coding elements which may refer to utterances in a transcription but which depend on the technical programming choice of the underlying , non-user related representation : ModuleRefs CDATA 'href : transcription#u' See Figure 2 for a concrete example from the MATE Workbench.
Complexity_Formulas $ 4 $ 71 $ 15 $ PATTERN $ = B ( O ) + max ( OCob# ) obj~class where O/TERM is the/DEF specification of an object in class ./O
Related_work_in_content_aggregation $ 3 $ 158 $ 4 $ CRF $ The definition of aggregation/TERM that we gave at the beginning of previous section is similar/DEF to those provided by Dalianis and Huang , although it focuses on common feature factorization to insure aggregation remains a proper subset of sentence planning ./O 
OUT $ -1 $ 75 $ 29 $ PATTERN $ The basic count measures A. through M. are preliminary and will require refinement as more data sets are tested.
Introduction $ 1 $ 15 $ 4 $ KP $ A textual/TERM IR/TERM system/TERM stores/DEF a collection of documents and special data structures for effective searching ./O
Extending_Domain_Semantics_for $ 5 $ 175 $ 18 $ PATTERN $ He is the Head of Division and is a professor.
Comparing_Taggers $ 1 $ 10 $ 3 $ CRF $ POS/TERM tagging/TERM is a/DEF useful first step in text analysis , but also a prototypical benchmark task for the type of disambiguation problems which is paramount in natural language processing : assigning one of a set of possible labels to a linguistic object given different information sources derived from the linguistic context ./O 
Discussion $ 6 $ 205 $ 27 $ PATTERN $ What is the net worth of Bill Gates?
Clustering_Systematic_Polysemy $ 3 $ 89 $ 9 $ PATTERN $ In the calculation of the data description length in equation ( 6 ) , each word in a cluster , observed or unobserved , is assigned an estimated/TERM probability/TERM , which is a/DEF uniform fraction of the probability of the cluster ./O
Overview_of_the_Approach $ 2 $ 25 $ 5 $ PATTERN $ We explain the features of the Geoquery representation language through a sample query : Input : "W'hat is the largest city in Texas?
Abstract $ 0 $ 21 $ 18 $ PATTERN $ The second part briefly describes neural networks.
Experimental_Results $ 7 $ 169 $ 2 $ PATTERN $ We thus utilized Reuters/TERM news/TERM articles/TERM referred to as 'Reuters-21578/TERM ,' which has/DEF been widely used in text classification v./O We used a prepared SAn exception is the method proposed in ( McCallure and Nigam , 1999 ) , which , instead of labeled texts , uses unlabeled texts , pre-determined categories , and keywords defined by humans for each category.
Abstract $ 0 $ 9 $ 8 $ PATTERN $ We can confirm that a term is good/TERM to/TERM discriminate/TERM subject/TERM concepts/TERM if/DEF relevant documents contain such terms and non-relevant documents do not contain them and/O that a term is noisy/TERM if/DEF the situation is the opposite ./O
Abstract $ 0 $ 2 $ 1 $ CRF $ We present a tool for the acquisition and the typing of NEs from the Web that associates a harvester and three parallel shallow parsers dedicated to specific structures ( lists , enumerations , and anchors ) . 
Models $ 2 $ 30 $ 10 $ KP $ To create a MEMD/TERM analog/TERM to/TERM IBM/TERM model/TERM 1/TERM (/O MEMD1/TERM ) , I used boolean features corresponding to bilingual word pairs : 1 , sEsandt----w fst ( W ,S ) = 0 , else where ( s , t ) is a ( source ,target ) word pair.
_Conclusion $ 4 $ 119 $ 10 $ CRF $ The preliminary results have demonstrated that the integration of a concept network , a query reformulator , a standard search algorithm , an auto summarizer , and an optional TTS engine indeed suits the current information seeking behavior and make search activities in websites more intuitive , as well as productive . 
Introduction $ 1 $ 10 $ 4 $ PATTERN $ One of the main advantages of probabilistic/TERM methods/TERM , on the other hand , is that they include/DEF a measure of uncertainty in their output ./O
Integration_Process $ 4 $ 92 $ 4 $ PATTERN $ SURGE/TERM ( Elhadad and Robin , 1996 ) is a/DEF comprehensive English Grammar written in FUF ./O
Comparison_experiment $ 3 $ 125 $ 27 $ PATTERN $ ( Which is the longest river of the worM?
Abstract $ 0 $ 25 $ 23 $ PATTERN $ 102 One of the most well-known models of this type is the BDI model , see Allen ( 1994 ) .
Introduction $ 1 $ 20 $ 15 $ PATTERN $ The widely available corpus is Academic/TERM Sinica/TERM Balanced/TERM Corpus/TERM abbreviated as ASBC/TERM hereafter ( I-Iuang and Chen , 1995 ) , which is a/DEF POS-tagged corpus ./O
Evaluation $ 3 $ 114 $ 23 $ KP $ For both BHT and LLR there was an increase in FNs at high frequencies , and an increase in FPs at medium and low frequencies , when compared to MLE.
Experimental_results $ 3 $ 56 $ 2 $ PATTERN $ Corpus/TERM A/TERM consists/DEF of local news with more than 325 million characters ./O
Segmentation $ 5 $ 189 $ 39 $ PATTERN $ ( 15 ) this , however , is a political science course.
Maximum_entropy-based_parse $ 2 $ 30 $ 1 $ KP $ In the present approach , parses are ranked according to their goodness by a statistical model built using the maximum/TERM entropy/TERM technique/TERM , which involves/DEF building a distribution over events which is the most uniform possible , given constraints derived from training data ./O
_Limitations $ 5 $ 33 $ 7 $ PATTERN $ • Time in the 911 domain there are at least two temporal contexts that can be "used" by the conversants : there is the actual time ( i.e. , when they are talking ) , but there also is the time relative to a point of focus in a plan , or even simply talking about the past or the future.
_Introduction $ 1 $ 4 $ 0 $ CRF $ Natural/TERM language/TERM generation/TERM involves/DEF a number of processes ranging from planning the content to be expressed through making encoding decisions involving syntax , the lexicon and morphology ./O 
Conclusion_and_future_work $ 6 $ 229 $ 0 $ CRF $ In this paper , I have presented a query tool for syntactically/TERM annotated/TERM corpora/TERM that is developed/DEF for the German Verbmobil treebank annotated at the University of Tiibingen ./O 
Introduction $ 1 $ 8 $ 1 $ CRF $ There are many definitions for the compound noun which cause ambiguities as to whether a given continuous noun sequence is a compound noun or not . 
Background $ 2 $ 71 $ 51 $ KP $ inform/TERM (/TERM aTask=n/TERM )/TERM : system/DEF presents the n'th answer to the query t ./O 
OUT $ -1 $ 88 $ 87 $ PATTERN $ In these strategies , the compellingness/TERM of/TERM an/TERM objective/TERM measures/DEF the objective's strength in determining the overall value difference between the two alternatives , other things being equal ./O
Introduction $ 1 $ 21 $ 15 $ PATTERN $ The LCS/TERM represents predicate/DEF argument structure abstracted away from languagespecific properties of semantics and syntax ./O
_NTCIR_Data_Analysis $ 2 $ 121 $ 51 $ PATTERN $ If the phrasal term AB has a high MI ( AB ,rel ) value in contrast with MI ( A , rel ) and MI ( B ,rel ) , this is the ease where phrasal terms are effective.
Abstract $ 0 $ 7 $ 5 $ PATTERN $ Introduction With the rapid growth of electronic documents and the great development of network in China , there are more and more people touching the Internet , on which , however , English/TERM is the/DEF most popular language being used ./O
Learning_Phrase-based_Variable $ 4 $ 84 $ 15 $ PATTERN $ The c/TERM transition/TERM from state 4 to state 6 is a/DEF back/Off transition to a lower order n-gram probability ./O
Abstract $ 0 $ 13 $ 11 $ PATTERN $ Coverage/TERM was measured/DEF by having human IF specialists annotate unseen data ./O
_IBM_expected_[SBAa_each_employee_to $ 5 $ 32 $ 20 $ CRF $ Inspired by the PP-attachment work of ( Stetina and Nagao , 1997 ) , we use WordNet vl.6 ( Miller et al. , 1990 ) as our semantic dictionary , where the hypernym structure provides the basis for semantically-motivated soft clusters . 
OUT $ -1 $ 126 $ 0 $ PATTERN $ 4.2 Active Learning To demonstrate the usefulness of obtaining probabilities from a transformation rule list , this section describes an application which utilizes these probabilities , and compare the resulting performance of the system with that achieved by C4.5.
October_2000 $ 8 $ 14 $ 13 $ CRF $ The remaining papers combine NL techniques from related areas summarization , information extraction to extend search capabilities and presentation of results , including summarization of search engine hit lists and summarization for text categorization , and a search interface that accepts template-like general constraints and is able to return specific information items such as locations , `` ~ple or companies that satisfy user 's constraints . 
Abstract $ 0 $ 144 $ 49 $ CRF $ Table 3 provides a comparison of the accuracy of decision trees applied across domains compared to those constructed and evaluated within a given domain . 
OUT $ -1 $ 64 $ 39 $ CRF $ `` +1/TERM '' means that the/DEF algorithm needs one bit to indicate whether the collocational relationship between the two clusters exists ./O 
OUT $ -1 $ 73 $ 72 $ CRF $ Guideline ( a ) Given the reader 's AMVF , it is straightforward to establish what represent supporting or opposing evidence for an argument with a given argumentative intent and a given subject . 
Abstract $ 0 $ 154 $ 59 $ CRF $ The addition of the heterogeneous Wall Street Journal articles , however , dilutes the focus of the model constructed for Encarta . 
Methodology $ 2 $ 53 $ 10 $ CRF $ This gives the effect of placing the largest LL value at the top of the list representing the word which has the most significant relative frequency difference between the two corpora . 
See_the_report_entitled_ $ 3 $ 33 $ 23 $ PATTERN $ 6 As we sought to measure the performance of each component in the systems , it quickly became apparent that not all available measures may be equally applicable for our filtering task.
The_Classifiers $ 3 $ 67 $ 0 $ PATTERN $ Adaptive/TERM Resonance/TERM Associative/TERM Map/TERM ( ARAM/TERM ) is a/DEF class of predictive serforganizing neural networks that performs incremental supervised learning of recognition categories ( pattern classes ) and multidimensional maps of patterns ./O
Introduction $ 1 $ 35 $ 28 $ KP $ Our key interest in this work was to provide a system which allowed users to get answers : not just documents or sub-documents.
_Generation_of_the_surface_phrase_from_the $ 4 $ 163 $ 126 $ PATTERN $ The F-measure/TERM is the/DEF balanced score of precision and recall , calculated as follows : 2 * precision * recall "F-measure = precision + recall Figures/O 4 and 5 show that the phraserepresented summary ( C ) presents the highest performance.
Abstract $ 0 $ 2 $ 1 $ CRF $ Our agent plans each utterance so that multiple communicative goals may be realized opportunistically by a composite action including not only speech but also coverbal gesture that fits the context and the ongoing speech in ways representative of natural human conversation . 
_INTRODUCTION $ 1 $ 12 $ 6 $ PATTERN $ New words are easily constructed by combining morphemes and their meanings/TERM are the/DEF semantic composition of morpheme components ./O
Experimental_Results $ 5 $ 94 $ 20 $ PATTERN $ Precision/TERM is the/DEF ratio between the number of correct parses produced by the specialized grammar and the total number of parses produced by the same grammar ./O
Approach $ 2 $ 17 $ 11 $ KP $ In memory-based/TERM learning/TERM the/DEF training data is stored and a new item is classified by the most frequent classification among training items which are closest to this new item ./O 
Architecture_and_Principles $ 3 $ 36 $ 13 $ CRF $ The second collection is a list organized into two sublists . 
Introduction $ 1 $ 26 $ 18 $ PATTERN $ 9 Are the scientific statements expressed in this sentence attributed to the authors , the general field , or specific other n work / Other Work Does this sentence contain material that describes the specific aim of the paper?
Reasons_for_cross-corpor_a_degradation $ 9 $ 230 $ 4 $ PATTERN $ Section 8 shows that sharing genre/topic is a key factor; as the WSJ corpus attains better results on the press : reportage category than the rest of 213 the categories on the BC itself.
_General_Outline_of_the_Method $ 2 $ 21 $ 5 $ PATTERN $ The second stage is the extracting process in which Chinese entity names and their relations are extracted using the classifiers learned.
The_steps_in_the_strategy_are_marked_with_the $ 7 $ 137 $ 15 $ PATTERN $ His approach to content selection and structuring does not provide a measure of evidence strength , which is necessary to implement several of the guidelines from argumentation literature we have examined.
Text_Summarization $ 1 $ 20 $ 0 $ KP $ The task of summarization/TERM is to/DEF identify informative evidence from a given document , which are most relevant to its content and create a shorter version of smnmary of the document from this information ./O
Previous_Schemes_for_Grunt $ 2 $ 30 $ 2 $ PATTERN $ The central inspiration here is the fact that grunts/TERM are unlike words , in that they contain/DEF sounds which are never seen in the lexical items of the language ./O
Discussion $ 5 $ 201 $ 25 $ PATTERN $ Thus , as long as the goal of the realizer/TERM is to/DEF enmlate as closely as possible a given corpus (/O rather than provide a maximal range of paraphrastic capability ) , then our approach can be used for evaluation , r As in the case of machine translation , evaluation in generation is a complex issue.
Experimental_Design $ 3 $ 181 $ 15 $ PATTERN $ Measures which do not depend on ground truth compute the summarydocument similarity sire ( s , d ) .
Comparing_the_five_approaches $ 4 $ 89 $ 19 $ PATTERN $ ~The Kappa/TERM statistic/TERM ( Cohen , 1960 ) is a/DEF better measure of inter-annotator agreement which reduces the effect of chance agreement ./O
The_Structure_of_a_Relational $ 3 $ 98 $ 24 $ PATTERN $ Each link file also defines a predicate.
_Relations_that_are_relevant_and_correct_in $ 3 $ 37 $ 3 $ PATTERN $ We define a synonymy/TERM relation/TERM as a/DEF binary relation between two synonym terms (/O with respect to • a particular sense ) .
EXOT $ 9 $ 81 $ 9 $ CRF $ One document `` Barbie/TERM '' in the Jang ( 1997 ) collection has a/DEF total of 1 ,468 words comprised of 755 content words and 713 function words ./O 
Introduction $ 1 $ 24 $ 19 $ PATTERN $ For example , a CORELEX/TERM class/TERM AQU/TERM ( which represents/DEF a relation between ARTIFACT and QUANTITY )/O contains words such as "bottle" , "bucket" and "spoon".
Experiments $ 3 $ 93 $ 40 $ PATTERN $ The other experimental variable is the number of chosen features.
Experimental_Results $ 5 $ 154 $ 4 $ PATTERN $ The second application is the restaurant query system illustrated in Figure 1.
_Introduction $ 1 $ 6 $ 0 $ KP $ The Penn Treebank ( Marcus et al.
OUT $ -1 $ 26 $ 23 $ PATTERN $ A single scenario for the colour domain In order to learn a rule set for a concept , EVIUS uses the relational learning method explained in section 3 , and defines the learning space by means of a dynamic predicate model.
ADAM : _Architectural_Principles $ 3 $ 88 $ 65 $ CRF $ ATLAS/TERM offers/DEF a threelayers solution to the problem of integrating different data storage formats by providing a logical level which consists of the language formalism and the API ./O 
The_Classifiers $ 3 $ 106 $ 69 $ PATTERN $ Based on the keyword feature table , the second phase of rule insertion translates each rule into a M-dimensional vector a and a N-dimensional vector b , where M/TERM is the/DEF total number of features in the keyword feature table and/O N/TERM is the/DEF number of categories ./O
Introduction $ 1 $ 31 $ 27 $ PATTERN $ However , in case k = 3 , the expansion/TERM probabilities/TERM depend/DEF on the states that are defined by the node label , the number of descendents the node and the sequence of labels in the descendents (/O if any ) .
The_formula_is_valid_when_J_>_R_ ( that_is ,_the_judges $ 7 $ 130 $ 44 $ CRF $ For example , if a sentence mentioning a new entity is included in a summary , one might also want to include a sentence that puts the entity in the context of the re§t of the article or cluster . 
Abstract $ 0 $ 2 $ 0 $ KP $ In the context of language learning , we address a logical approach to information extraction.
The_Diversity_of_Semantic $ 2 $ 24 $ 0 $ PATTERN $ Relations between "noun -tNO" and their Head Nouns Among Japanese adnominal constituents , " noun + NO" represents a wider range of semantic relations than other adnominal constituents.
Implementation $ 3 $ 131 $ 13 $ PATTERN $ In table 2 are some example inputs and outputs , a 1/TERM represents activation/DEF on an input or output node ./O
Generation_of_Crisp_Descriptions $ 3 $ 71 $ 17 $ PATTERN $ 'Success'/TERM means that the/DEF properties in L are sufficient to characterize S ./O 
Introduction $ 1 $ 13 $ 5 $ KP $ form WSD.
Abstract $ 0 $ 3 $ 2 $ PATTERN $ We compare performance using different measures of association , and find that Yule's coefficient of colligation Y gives somewhat better results over other measures.
Generation_of_Multiple_Quantifiers $ 5 $ 192 $ 7 $ CRF $ In `` Each patient is given a high severity rating '' , performing universal quantification on the patients ( ARG3/TERM ) is a/DEF separate decision from the existential quantification of the severity ratings ( ARG2 ) ./O 
OUT $ -1 $ 152 $ 101 $ CRF $ So it is an excellent framework for experimenting with the interaction between aggregation and text planning . 
The_hyperonym_problem $ 2 $ 28 $ 10 $ PATTERN $ Hence , if A is the correct lemma , B will ( also ) be retrieved.
_Rare_w~_contains_a_hyphen $ 9 $ 78 $ 23 $ CRF $ Model Overall Unknown Word Accuracy Accuracy Baseline , 96.72 % 84.5 % J Ratnaparkhi 96.63 % 85.56 % ( 1996 ) Table 3 Baseline model performance This table also shows the results reported in Ratnaparkhi ( 1996 : 142 ) for COnvenience . 
_Can_a_satisfactory_e~perimental_technique_be_devel $ 9 $ 293 $ 3 $ CRF $ Three measures are used in the evaluation of the system performance : ( 1 ) precision/TERM , dEfined as the/DEF number of relevant documents retrieved over the total number of documents retrieved ;/O ( 2 ) recalL/TERM , defined as the/DEF number of relevant documents retrieved over the total number of relevant documents found in the collection and/O ( 3 ) F-measure/TERM , which combines/DEF both the precision and recall into a single formula : Fmeasure/TERM = 2*R*P/(R+P)/DEF where P is the precision , R is the recall and is the relative importance given to recall over precision ./O 
Adequacy $ 4 $ 115 $ 15 $ PATTERN $ Regarding unambignity , the scheme is an improvement but has one failing : repetition of a letter represents either extended duration or the presence of multiple syllables.
October_2000 $ 7 $ 9 $ 8 $ KP $ Allied to corpus-similarity is corpus-homogeneity .
Introduction $ 1 $ 23 $ 16 $ KP $ ( 1995 ) and Beale ( 1997 ) .
PAR_as_anIL $ 4 $ 30 $ 19 $ PATTERN $ 5 Independent of which is the source language , the PAR schema selected is motion , the activity/TERM  field/TERM , which determines/DEF how the action is performed (/O in this case , by floating ) , is filled by float/TERM ( the/DEF main verb in English , or the adjunct in Spanish )/O .
Focusing_on_Definitory_Contexts $ 2 $ 15 $ 0 $ CRF $ Two issues are addressed in this paper : be accessed directly and entirely through large-scale filters such as shallow parsers , access to Web pages is restricted to the narrow and specialized medium of a search engine . 
OUT $ -1 $ 29 $ 29 $ KP $ Our users rely on machine generated summaries ( single document , either generic or query-based , with user adjustment of compression rates ) to judge relevance of full documents to their information need.
Introduction $ 1 $ 56 $ 48 $ PATTERN $ " TEXTUAL "Section describes three unification-based/DEF parsers which/O are... " OWN/TERM "We also compare with the English language and draw some conclusions on the benefits of our approach.
_Annotation_Guidelines_I : $ 3 $ 60 $ 14 $ CRF $ 2.VP : A VP/TERM is a/DEF phrase headed by a predicate ./O 
The_WPDV_algorithm $ 1 $ 7 $ 0 $ KP $ Weighted/TERM Probability/TERM Distribution/TERM Voting/TERM ( WPDV/ACR ) is a/DEF supervised learning approach to classification ./O
Middle $ 6 $ 92 $ 55 $ CRF $ A sense/TERM tag/TERM Ctag/TERM is in/DEF terms of a vector ( wl , w2 , ... , wn ) ,/O where n/TERM is the/DEF vocabulary size and/O wi/TERM is a/DEF weight of word cw ./O 
Abstract $ 0 $ 3 $ 0 $ CRF $ A computational/TERM framework/TERM is presented which is used/DEF to model the process by which human language learners acquire the syntactic component of their native language ./O 
Explaining_Probabilistic_Methods $ 3 $ 70 $ 22 $ PATTERN $ Clearly , the LSQ/TERM is a/DEF linear discriminator over the feature space A' ,/O with coefficients f that are computed given ( potentially all ) the values ^D P[x ,t]" The definition generalizes naturally to non-binary classifiers; in this case , the discriminator between predicting l and other values is linear.
Abstract $ 0 $ 7 $ 3 $ PATTERN $ The paper also describes the interactions between the dialogue component and the other servers of the system , mediated via a central hub.
Related_work $ 6 $ 164 $ 15 $ PATTERN $ In our case the meta-interpreter/TERM is the/DEF chart parser augmented with the generation of needs and/O the partial proof is represented by the chart augmented with the needs.
Complexity $ 3 $ 110 $ 27 $ PATTERN $ Figure 3 shows how different parameter setting affects the cardinality of utterances for different values of M. The ( logarithmic/TERM ) yaxis/TERM represents the/DEF cardinality of utterances ,/O and the ( linear/TERM ) x-axis/TERM the/DEF maximal number of semantic items in one utterance ./O
_Annotation_Guidelines_II : $ 4 $ 121 $ 2 $ PATTERN $ In other words , the only empirical evidence for the existence of a thematic relation is a realized argument.
_Amanda_Huggenkiss ,_she_proposes_to_meet_you $ 2 $ 77 $ 70 $ CRF $ Topic , the role of aboutness is attributed to a discourse referent that is identifiable and more or less active . 
ALLiS $ 2 $ 22 $ 17 $ KP $ LT TTT : the last tool tried , LT/TERM TTT/TERM ( Grover et al. , 1999 ) , is a/DEF text tokenisation system and toolset ./O
The_Experiment $ 5 $ 201 $ 27 $ PATTERN $ Some details on measures based on subjects' self-reports can be examined in Figure 4 , which shows an excerpt from the final questionnaire that subjects are asked to fill out at the end of the interaction.
Implementation $ 4 $ 260 $ 89 $ KP $ The briefing can then be presented by the author if desired , or else directly by the computer ( particularly useful if the briefing is being sent to someone else ) .
Abstract $ 0 $ 6 $ 2 $ PATTERN $ Content-based/TERM measures/TERM increase/DEF the correlation of rankings induced by synonymous ground truths , and exhibit other desirable properties ./O
Segmentation $ 5 $ 189 $ 39 $ CRF $ ( 15 ) this , however , is a political science course . 
Introduction $ 1 $ 25 $ 20 $ KP $ In other words , the corpora should have been built using the same stratified sampling method and with , if possible , randornised methods of sample selection.
Interlingual_representation_issues $ 5 $ 159 $ 1 $ PATTERN $ A 57 case in point is the representation of numbers.
Previous_work $ 2 $ 38 $ 14 $ CRF $ He also builds a probabilistic model which indicates that the probability of two words being morphological variants is based upon the probability of their respective changes in orthography and morphosyntactics . 
The_computation_of_the_velocity_is_easily ,_done_from $ 4 $ 171 $ 18 $ PATTERN $ MOVE/TERM is a/DEF label for complex events that consists of maximally three sub-events , namely START , CHPOS ( CHANGE OF POSITION ) , and STOP , where the first and the last sub-event are optional and the middle event can be any kind of movement along a trajectory ./O
Stochastic_Surface_Realization $ 2 $ 71 $ 13 $ PATTERN $ In other words , W* = arg max P ( WIA ) = arg max P ( AI W ) Pr ( W ) where W/TERM is the/DEF string of words , wl , ... , wn ,/O and A/TERM is the/DEF acoustic evidence (/O Jelinek 1998 ) .
Abstract $ 0 $ 38 $ 36 $ PATTERN $ P ( tl ) = n ,__~_ ( 2 ) N P ( t2 ) = n ,2 ( 3 ) N P ( tl , t2 ) = n , , , ,~ ( 4 ) N Where nt~/TERM , nt2/TERM is the/DEF individual term frequency of term t I and t 2 respectively if either of them occur in a sentence of the collection ,/O ntt/TERM is the/DEF co-occurrence frequency of term t I and t 2 if they are all in a sentence of the collection ./O
Introduction $ 1 $ 32 $ 25 $ CRF $ Plain text documents are provided to a lexical analyzer and a noun-recognizer ( XEROX MULTEXT ) , whose output is the document text tagged with parts of speech to be fed to the parser . 
Discussion $ 6 $ 114 $ 0 $ CRF $ A knowledge-based/TERM machine/TERM translation/TERM can be viewed as extracting/DEF and representing the meaning of a text and generating a text in target language based on the meaning presented ./O 
Interlingua_system_ ( ISS ) $ 3 $ 70 $ 1 $ KP $ This system , named lnterlingua/TERM Slot/TERM Structure/TERM ( 1SS/TERM ) , generates/DEF an interlingua representation from the SS of the sentence ./O
Introduction $ 1 $ 24 $ 15 $ PATTERN $ Section 2 describes parse selection and discusses the "compositional" statistical features employed in a maximum entropy approach to the task.
_Conclusion $ 4 $ 162 $ 1 $ CRF $ The method provides a unified framework based on MBL . 
Applications_of_LexTract $ 4 $ 157 $ 0 $ CRF $ 4.1 Evaluating the coverage of hand-crafted grammars The XTAG/TERM grammar/TERM ( XTAG-Group , 1998 ) is a/DEF hand-crafted large-scale grammar for English , which has been developed at University of Pennsylvania in the last decade ./O 
Example_Dialogue $ 3 $ 65 $ 42 $ PATTERN $ That is the effect of adding ( M1 ) to the Grammar : Since the Parsebank and the Prediction Models are updated on-line , the presence of the word retrieve in subsequent utterances becomes a strong indicator of LIST and , associatively , of [listMa±l].
The_machine_learning_method $ 3 $ 89 $ 30 $ PATTERN $ Work has to be done to determine what is the most appropriate context for this task.
Principles_and_Parameters $ 1 $ 13 $ 6 $ PATTERN $ grammars/TERM is the/DEF set of all possible combinations of parameter values (/O and lexicon ) .
Background : _The_STOP_System $ 2 $ 68 $ 51 $ PATTERN $ Again this is a standard expert-system technique for KA.
Introduction $ 1 $ 12 $ 6 $ CRF $ To illustrate the operation of BIAS and its rebuttal capability , consider the exchange in Figure 1 , which consists of a preamble that contains background information~ followed by an argument generated by BIAS , a user 's rejoinder and BIAS ' rebuttal . 
Dialogue_Management_Strategies $ 3 $ 131 $ 43 $ CRF $ This reduction is based on a hierarchy of quality measures for each flight , beginning with any stated or inferred preference ( e.g. , a particular airport in the case of cities with multiple airports , or a particular airline in the case of a multi-leg booking where one leg has already been established ) and including number of stops and length of flight . 
The_Generation_of_the_Initial $ 4 $ 39 $ 1 $ PATTERN $ The result of its operation is a set of rules which assign a default category to each tag.
OUT $ -1 $ 22 $ 18 $ KP $ Technical writers find it difficult to comply with the writing rules of a CL which are often hard to justify ( CLA.
Corpus_comparison_based_on $ 6 $ 109 $ 0 $ PATTERN $ information theoretical measures In this section we attempt to present measures that overcome some of the limitations of the classtoken method.
Experimental_Results $ 5 $ 156 $ 6 $ PATTERN $ The third domain consists of a set of 300 computer-related jobs automatically extracted from postings to the USENET newsgroup austin.jobs.
MALIN $ 4 $ 130 $ 24 $ CRF $ For complex requests the Dialogue Manager needs an information structure that holds the parameters needed before successful access of the background system can be performed . 
Global_View_on_the_DE $ 2 $ 22 $ 5 $ CRF $ The central module of the DE iS a compiler that maps a structure specified at one of tile five first of the above strata on a structure at the adjacent stratum . 
Introduction $ 1 $ 8 $ 5 $ KP $ The Deep Read group provided us with an on-line version of the Remedia material along with several marked up versions of * This research was supported in part by NSF grant LIS SBR 9720368.
Dialog_Management $ 3 $ 65 $ 4 $ PATTERN $ If a user is using the system for the first time , a good indication of the initial user expertise level is the level of detail and technical complexity of the initial query.
Summary_and_Future_Work $ 4 $ 213 $ 9 $ PATTERN $ Although the precision ( so far ) is not high ( 60% 80% ) , it is not the most important result because ( 1 ) this only represents a minor waste of checking effort , compared with scanning the entire text , and ( 2 ) the identified errors will be checked further or corrected either manually or automatically.
Abstract $ 0 $ 5 $ 2 $ KP $ The RIPPER rule induction algorithm is adopted for the selection of the underlying rules.
Introduction $ 1 $ 13 $ 4 $ KP $ UCE/TERM filtering/TERM is a/DEF text categorization task ./O
_KNOWLEDGE_EXTRACTION $ 3 $ 111 $ 53 $ PATTERN $ The X/TERM is the/DEF initial two-characters of the keyword and/O Y/TERM is the/DEF remained characters ./O
Introduction $ 1 $ 11 $ 3 $ KP $ Authoring/TERM is seen as a/DEF top-down interactive process of step-wise refinement of the root nonterminal ( corresponding to the whole document ) where the author iteratively selects a rule for expanding a lBut see ( Wood , 1995 : Prescod , 1998 ) for discussions of the differences ./O
Introduction $ 1 $ 65 $ 57 $ CRF $ It is a part of the class 'computer' . 
Rule_set_learning $ 3 $ 64 $ 26 $ CRF $ , word._X B~ , lemma-X sl , ... , lemma_X B~ , sem-X B1 , ... , sem_X B~ , context ) where B1/TERM ,/TERM .../TERM ,/TERM Bn/TERM are the/DEF unrepeated terminal nodes from A1 , ... , An ,/O context/TERM is the/DEF set of all predicates subsumed by the syntactico-semantic structure between the nearest positive example on the left and the nearest one on the right ,/O and sem_XB/TERM is the/DEF list of isa_X and has_hypernym_X predicates for Bi ./O 
Technique_description $ 2 $ 50 $ 21 $ PATTERN $ ) is frequency , L/TERM is the/DEF set of left adjacent strings of X ,/O tz~L/TERM and/TERM ILl/TERM means the/DEF number of unique left adjacent strings ./O
Centroid-based_summarization $ 4 $ 82 $ 10 $ CRF $ For example , the sentence `` President Clinton met with Vernon Jordon in January '' gets a score/TERM of 243.34 which is the/DEF sum of the individual eentroid values of the words (/O clinton = 36.39 ; vernon = 47.54 ; jordan = 75.81 ; january = 83.60 ) . 
Eckert_and_Strube's_Algorithm $ 3 $ 55 $ 6 $ PATTERN $ ES99 define the following *I predicates ( Eckert and Strube , 1999b ) [p. 40] : Equating/TERM constructions/TERM where a/DEF pronominal referent is equated with an abstract object ,/O e.g. , x is making it easy , x is a suggestion.
OUT $ -1 $ 125 $ 54 $ PATTERN $ The constructions of coordination and apposition are represented by a special/TERM node/TERM ( usually the node of the coordinating conjunction or other expression ) that is the/DEF governor of the coordinated subtrees and their common complementation in the ATS ./O
Background $ 2 $ 115 $ 95 $ KP $ Speech acts have no propositional content , thus in the context of the current dialogue history and the state of the application description , they are translated into dialogue primitives , which have content , for example , the name of a parameter and a potential value for this parameter.
LTAGs_and_Extraction $ 3 $ 34 $ 7 $ PATTERN $ An auxiliary/TERM tree/TERM represents a/DEF recursive structure and has a unique leaf node ,/O called the foot/TERM node/TERM , which has/DEF the same syntactic category as the root node ./O
The_results_reported_in_this_paper_are_part_of_a_more $ 1 $ 39 $ 28 $ CRF $ Section 3 reports the experimental setting for the comparative evaluation of the three search modalities . 
Models_and_Modifications $ 2 $ 48 $ 35 $ CRF $ Pi/TERM (/TERM c~/TERM )/TERM is the/DEF probability of beginning a derivation with c~ ;/O Ps/TERM (/TERM o/TERM I/TERM 77/TERM )/TERM is the/DEF probability of substituting o~ at 7 ;/O finally , Pa/TERM (/TERM NONE/TERM I/TERM 7/TERM )/TERM is the/DEF probability of nothing adjoining at ~/ ./O 
Dialog_Management $ 3 $ 120 $ 59 $ KP $ User expertise level and corresponding dialog strategies 98 3.3 Algorithm The proposed algorithm for action planning , content selection and content realization is given in Figure 3.
OUT $ -1 $ 111 $ 106 $ CRF $ Intuitively , a feature value of 0 is the best , indicating that for that question-sentence pair q and s , they have the most number of matching words in the story , when comparing q with all sentences sz in the same story . 
OUT $ -1 $ 238 $ 48 $ PATTERN $ The user just selects the utterance to nark up and then clicks on the violation type palette , or , in case it is a new type , clicks on the violated cooperafivity guideline which means that a new violation type is added and text can be entered to describe it , el.
T $ -1 $ 0 $ 0 $ KP $ An Extended Architecture for Robust Generation* Tilman Becker , Anne Kilger , Patrice Lopez , Peter Poller DFKI GmbH Stuhlsatzenhausweg 3 D-66123 Saarbriicken , Germany {becker , kilger , lopez , poller}@dfki , de
Evaluation_Measures $ 2 $ 89 $ 64 $ PATTERN $ As such , these measures produce different scores , but the same ranking of all the K-sentence extracts from the document.
Learning_validation_and_results $ 4 $ 174 $ 36 $ CRF $ A Perl program presents to one expert all the N-V pairs that appear in one sentence in a part of the corpus and include one of the studied nouns . 
Learning_Verb_Rules $ 3 $ 63 $ 14 $ PATTERN $ In our example it is the verb zdSastnit that is removed.
Conclusion $ 5 $ 69 $ 0 $ CRF $ It is found that the performance with the help of error-driven learning is improved by 2.20% and integration of memory-based learning further improves the performance by 0.35 % to 92.12% . 
Abstract $ 0 $ 4 $ 2 $ KP $ Models using a lexicon alone are generally held to be incapable of explaining these data.
Bridging_Natural_Language_and $ 4 $ 110 $ 31 $ PATTERN $ Furthermore , matching trees and sub-trees is a computationally intensive task , especially since full linguistic parse trees may be relatively deep.
Implementing_Embedded_MT $ 2 $ 116 $ 59 $ PATTERN $ The first of these is the problem of API's from COTS systems and GOTS systems.
From_Prepositional_Phrases_to $ 3 $ 83 $ 26 $ CRF $ But actually the situation/TERM HAS-PART-STATE/TERM is a/DEF state in which only one is present ,/O which is obviously `` little '' . 
Only_the_relevant_attributes_of_each_semantic_role $ 5 $ 194 $ 103 $ CRF $ For the correct generation into Spanish the following morphological rule is constructed : pronoun + third..person + plural + antecedent ( ~olice ' ) ~ ~sta ( pronoun , third person , feminine and singular ) The left-hand side of the morphological rule contains the interlingua representation of the pronoun and the right-hand side contains the pronoun in the target language . 
Unfolding_and_Specialization $ 2 $ 28 $ 1 $ KP $ This is in itself a specialization of the grammar which was used to parse the treebank , since some rules may not show up in any correct parse in the training set; experimental results for this first/Order specialization are reported in ( Cancedda and Samuelsson , 2000 ) .
Experiments $ 5 $ 101 $ 7 $ CRF $ `` Precision/TERM '' is the/DEF percentage of correct answers among the answers proposed by the system ./O 
Abstract $ 0 $ 57 $ 26 $ CRF $ Although the effect of discourse markers in other languages might not be too prominent , there is a great necessity to study discourse markers in Chinese in order to capture the major associated rhetorical patterns in Chinese texts . 
Abstract $ 0 $ 3 $ 1 $ KP $ Conventional/DEF parsing techniques based on Machine Learning framework ,/O such as Decision/TERM Trees/TERM and Maximum/TERM Entropy/TERM Models/TERM , have difficulty in selecting useful features as well as finding appropriate combination of selected features.
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 254 $ 219 $ CRF $ ttpj = ttpn { / ( t , pj ) ift , pj is atoplc of Stp 0 otherwise f ( w ) denotes term frequency of word w. term vectors 35 Let $1 , - , S , , , be all the other training documents ( where m/TERM is the/DEF number of training documents which does not belong to the target event )/O and Sx/TERM be a/DEF test document which should be classified as to whether or not it discusses the target event ./O 
Introduction $ 1 $ 10 $ 3 $ PATTERN $ " 1 In Korean documents , compound nouns are represented in various forms ( shown in Table 1 ) , so there is a difficulty in indexing all types of compound nouns.
Related_Work $ 6 $ 173 $ 5 $ KP $ As prepositions ( and cue-phrases in general ) can signal different coherence relations , the presented computational approach couples a cue-phrase approach like ( Marcu , 1998 ) with inferences using the computed semantic representation.
Approach $ 2 $ 46 $ 40 $ CRF $ In case a word is classified as belonging to more than one chunk type , preference will be given to the chunk type that occurs most often in the training data . 
_Our_Classification_Algorithm_for $ 2 $ 103 $ 39 $ PATTERN $ So , at the beginning of the algorithm , we judge if the verb is a "linking verlS' ( ~ ( be~' , ~ ( equal~' ,etc ) or a "possessive verlS' ( '~-q~J' ( have ) ) .
_KNOWLEDGE_EXTRACTION $ 3 $ 110 $ 52 $ CRF $ For the keywords of length 3 ,4 , and 5 , each keyword is divided into two parts X and Y. X/TERM is a/DEF candidate of proper name and/O 17 Y/TERM is a/DEF candidate of organization type ./O 
PAC_Boolean_concept_learning $ 3 $ 51 $ 5 $ PATTERN $ An example is a vector denoting the truth ( presence ,l ) or falsehood ( absence ,0 ) of propositional variables.
Query_compositions $ 2 $ 76 $ 1 $ CRF $ In particular we experimented the `` Boolean/TERM phrase/TERM '' modality/TERM , which allows/DEF the user to submit queries with keywords composed by means of logical operators ./O 
Results $ 6 $ 210 $ 32 $ PATTERN $ What ..differs are the word order and/Or the position of the nucleus accent.
Pre-processing_design $ 2 $ 35 $ 0 $ KP $ Input pre-processing is essential in an embedded real time system , in order to simplify the core processing and make it both timeand memoryeffective.
The_MATE_Markup_Framework $ 3 $ 167 $ 79 $ CRF $ An example is the following of coding elements which may refer to utterances in a transcription but which depend on the technical programming choice of the underlying , non-user related representation : ModuleRefs CDATA 'href : transcription # u ' See Figure 2 for a concrete example from the MATE Workbench . 
Abstract $ 0 $ 5 $ 1 $ KP $ Mercury provides telephone access to an on-line flight database , and allows users to plan and price itineraries between major airports worldwide.
Experimental_Results $ 5 $ 151 $ 1 $ PATTERN $ The first is the U.S. Geography domain.
Grammar_Induction $ 3 $ 62 $ 1 $ PATTERN $ When the training corpus consists of a large reservoir of fully annotated parse trees , it is possible to directly extract a grammar based on these parse trees.
Dialog_Management $ 3 $ 85 $ 24 $ PATTERN $ Response/TERM Complexity/TERM : There/DEF is a reward and a punishment associated with each system response that reflects the complexity of the content and realization of the system responses ./O
Introduction $ 1 $ 18 $ 11 $ KP $ The ILEX project was supported by EPSRC grant GR/K53321.
Introduction $ 1 $ 16 $ 0 $ PATTERN $ A major advantage of inductive logic programming is the ability to incorporate domain knowledge ( background knowledge ) into the inductive process.
Memory-based_learning $ 3 $ 75 $ 18 $ PATTERN $ ( 7 ) cEClass/TERM H/TERM (/TERM C[F=v]/TERM )/TERM is the/DEF class entropy computed over the subset of instances that have v as value for Fi ./O
Concepts $ 2 $ 35 $ 10 $ KP $ The coUocational/TERM degree/TERM is defined as the/DEF ratio of the existing collocation instances between the cluster and its distribution envffonment to all possible collocations generated by them ./O
Empirical_Evaluation $ 4 $ 149 $ 38 $ PATTERN $ kNN/TERM is a/DEF lazy learning method in the sense that it does not carry out any off-line learning to generate a particular category knowledge representation ./O
Generation_of_Multiple_Quantifiers $ 5 $ 218 $ 33 $ CRF $ What happens if a sentence contains an existential quantifier which has a wider scope than a universal quantifier ? 
_Invert_the_lexicon_to_make_it_an_English $ 5 $ 65 $ 9 $ PATTERN $ One problem is the segmentation of Chinese text , since Chinese has no spaces between words.
Abstract $ 0 $ 4 $ 2 $ KP $ The reasoning/TERM model/TERM is interacting/DEF with the model of communication process ./O
The_link_between_~i~l~ ( /guniang/ ,_girl ) _and $ 7 $ 116 $ 10 $ PATTERN $ 35 ) /shi/ /sheng/ teacher student teacher and student Intuitively , there is a relationship , i.e. , 9-f ( /bing/ , and ) , between the two concepts denoted by the two words.
Indexing_and_Retrieval $ 5 $ 283 $ 50 $ PATTERN $ What is the combined effect of surface heat and mass transfer on hypersonic flow?
Complexity $ 3 $ 94 $ 0 $ PATTERN $ The basic/TERM entity/TERM is a/DEF semantic object ( S ) which is an atomic item treated by the DM ./O
Introduction_1 $ 1 $ 14 $ 9 $ CRF $ Text either from the recognizer or directly input by IThis paper also appears in the proceedings of the Sixth International Conference on Applied Natural Language Processing~ Seattle , WA , April 2000. the user is then converted into some kind of logical formula , which abstractly represents the user 's intended command ; this formula is.then fed into a command interpreter , which execdtes the command . 
Abstract $ 0 $ 2 $ 1 $ KP $ Quarc/TERM uses/DEF heuristic rules that look for lexical and semantic clues in the question and the story ./O
OUT $ -1 $ 114 $ 72 $ PATTERN $ In our case , the samples are dependent; the classification of sample i is a feature for sample i + 1 , which means that changing the classification for sample i affects the context of sample i + 1.
How_to_generate_technical $ 2 $ 95 $ 44 $ CRF $ G-TAG/TERM thus seems a/DEF good candidate for producing technical documentation complying with the constraints of an ( EM ) CL ./O 
_General_Outline_of_the_Method $ 2 $ 90 $ 74 $ PATTERN $ The reason is that in the training data all word sequences whose previous word is "~" is a not a person name.
Results $ 3 $ 62 $ 8 $ KP $ For NP , the basic and reverse model produce accuracies which can compete with the highest published non-combination accuracies so far.
Introduction $ 1 $ 65 $ 56 $ KP $ Barzilay and Elhadad use the notion of strong/TERM chains/TERM ( i.e. , chains/DEF whose scores are in excess of two standard deviations above the mean of all scores )/O to determine which chains to include in a summary.
Introduction $ 1 $ 18 $ 14 $ CRF $ ( In many other approaches , speed is a function of the grammar size , because it is searched during realization ( Elhadad , 1992 ; Elhadad , 1993 ; Mann , 1983 ; McKeown , 1982 ; McKeown , 1985 ) . 
OUT $ -1 $ 230 $ 40 $ CRF $ 25 The attribute vtype is mandatory , vtype/TERM is a/DEF reference to a description of a guideline violation in a file which contains the different kinds of violations of the individual guidelines ./O 
Hyperonyms_in_NLG_systems $ 4 $ 75 $ 16 $ PATTERN $ Thus , on the taxonomic path Tweety ( instance/Of ) Robin Bird Vertebrate Animal Object , the concept Bird is a basic-level one , which leads to a preference for using the corresponding lexical item when referring to some kind of bird ( i.e. , some concept or instance subsumed by it ) .
Introduction $ 1 $ 24 $ 16 $ PATTERN $ One thing , however , is constant across all articles : the argumentative aim of every single article is to show that the given work is a contribution to science ( Swales , 1990; Myers , 1992; Hyland , 1998 ) .
Setting $ 3 $ 87 $ 8 $ KP $ From these corpora , a group of 21 words which frequently appear in the WSD literature has been selected to perform the comparative experiments ( each word is treated as a different classification problem ) .
Abstract $ 0 $ 112 $ 10 $ PATTERN $ We are investigating whether this is a result of the BNC discussing weather more often , or a result of which particular grammatical structures are used to describe the weather floods in British and American English.
OUT $ -1 $ 93 $ 51 $ CRF $ This alleviates the problem of overpartitioning/TERM of/TERM data/TERM , which is a/DEF widely-recognized concern during decision tree growth ./O 
Selection_of_candidate_strings $ 2 $ 42 $ 18 $ PATTERN $ The IWP/TERM of/TERM a/TERM single/TERM character/TERM is the/DEF likelihood for this character to appear as an independent word in texts :/O N ( Word ( c ) ) IWP ( c ) = N ( c ) where N/TERM (/TERM Word/TERM (/TERM c/TERM )/TERM )/TERM is the/DEF number of occurrences of a character as an independent word in the sentences of a given text corpus and/O N/TERM (/TERM c/TERM )/TERM is the/DEF total number of occurrence of this character in the same corpus ./O
Experimental_Setting $ 4 $ 127 $ 26 $ PATTERN $ The whole manual set consists of about 2500 annotated nouns.
Related_Research_and_Motivation $ 1 $ 57 $ 37 $ CRF $ One of the authors of this paper is not a native speaker of Chinese or Japanese but has the intermediate level proficiency in both languages now . 
The_Generation_System $ 3 $ 56 $ 14 $ PATTERN $ Thus , for example , the following structure ( with some aspects elided for brevity ) represents a node that could be one of three possibilities.
Generation_and_linguistic_representation $ 4 $ 86 $ 14 $ CRF $ The generation task in REA thus involves selecting a number of such lexicalized descriptors and organizing them into a grammatical whole that manifests the right semantic and pragmatic coordination between speech and gesture . 
The_link_between_~i~l~ ( /guniang/ ,_girl ) _and $ 7 $ 106 $ 0 $ PATTERN $ ~b~/TERM ( /waimao/ , appearance ) denotes a/DEF relation between concepts and relationships ./O
Topic_Analysis $ 4 $ 109 $ 0 $ CRF $ In segmentation , we first identify candidates for points of segmentation within the given text . 
Introduction $ 1 $ 9 $ 4 $ KP $ A dialogue/TERM manager/TERM facilitates/DEF the negotiation of parameter values between a user and an SDS ./O
Using_CST_for_information_fusion $ 5 $ 152 $ 15 $ PATTERN $ A graph-based/TERM operator/TERM defines/DEF a transformation on a multi-document graph ( MDG ) G which preserves some of its properties while reducing the number of nodes ./O
_Relations_that_are_relevant_and_correct_in $ 3 $ 198 $ 164 $ CRF $ A second test was designed to check whether there is a correlation between the levels of confidence of automatic and manual pruning . 
Information_extraction $ 3 $ 65 $ 30 $ PATTERN $ For instance , the third clause means : "Text A deals with imports if it contains a sentence with a subject composed by a NP containing a persona giuridica , the verb of the main sentence is interessare ( to interest ) in finite affirmative mood , and the indirect object consists of a PP containing the word importazione".
Context-dependent_Lexicons $ 3 $ 83 $ 0 $ KP $ In the last section , we only use current part-of-speech as a lexical entry.
_Preliminary_Evaluation $ 3 $ 106 $ 51 $ CRF $ The algorithm disambiguate_class/TERM , which is implemented by Resnik and described in detail in [ Resnik , 1999] , calculates/DEF the similarity between all the words' senses of words in a set ./O 
The_MATE_Approach $ 2 $ 43 $ 7 $ PATTERN $ The resulting report ( Klein et al. , 1998 ) describes more than 60 coding schemes , giving details per scheme on its coding book , the number of annotators who have worked with it , the number of annotated dialogues/segments/ utterances , evaluation results , the underlying task , a list of annotated phenomena , and the markup language used.
OUT $ -1 $ 146 $ 141 $ CRF $ For each question type , we find one word that has the highest positive C value for that question type . 
The_Refinement $ 5 $ 87 $ 31 $ KP $ Tag Word left right VBG operating NN VBG operating IN NN VBG operating VBD NN Table 4 : Superfluous rules.
The_interplay_of_focus_and_word $ 2 $ 50 $ 26 $ PATTERN $ Extrapositions/TERM are the/DEF linguistic means in German to separate sense units ./O
Robust_Name_Finding $ 3 $ 73 $ 14 $ PATTERN $ However , the second version in Figure 2 shows that if we know that "OUR STRAWS" is a location phrase and that "YEAR BEHIND IT" is a person phrase ( albeit incorrectly transcribed ) , we could at least know where in the passage to find the answer to the Who?
The_Interactional_Framework $ 2 $ 31 $ 11 $ PATTERN $ If this is the case , it is possible that agent B comes to choose a plan to satisfy this goal , even if it does not yield any direct utility to him.
_Introduction $ 1 $ 14 $ 8 $ PATTERN $ Section 3 describes the preliminary experimental results of our method.
Abstract $ 0 $ 2 $ 1 $ CRF $ We discuss these concepts and the way they are implemented in the architectural framework of the ADAM/TERM corpus/TERM , which is a/DEF corpus of 450 Italian spontaneous dialogues ./O 
Abstract $ 0 $ 162 $ 6 $ CRF $ We believe that this is a good example of a case where developing a system with the RAGS data structures in mind simplifies the task . 
_Background $ 2 $ 47 $ 31 $ PATTERN $ Aside from homographs and homophones , another source of ambiguity in the Chinese language is the definition of a Chinese word.
Introduction $ 1 $ 17 $ 8 $ CRF $ This approach reports experimental results , using the SRI Core Language Engine , ( Alshawi , 1992 ) , in the ATIS domain , of more than a 3-fold speedup at a cost of 5 % in grammatical coverage , the latter which is compensated by an increase in parsing accuracy . 
Rule_set_learning $ 3 $ 59 $ 21 $ KP $ In ( Turmo et al. , 1999 ) , the concept of S-set/TERM has been presented as a/DEF syntactic relation generalization ,/O and a distance measure has been based on this concept.
_The_co-reference_problem_in_summarization $ 4 $ 55 $ 31 $ PATTERN $ Clustering/TERM : The/DEF ability to cluster similar documents and passages to find related information ./O
Abstract $ 0 $ 88 $ 86 $ KP $ An end-to-end evaluation includes an analyzer/TERM , which maps/DEF the source language input into IF and/O a generator/TERM , which maps/DEF IF into target language sentences ./O
Phrase-Representation_Summarization $ 1 $ 25 $ 3 $ PATTERN $ To avoid the burden of reading such long and complex sentences , we have developed the phrase-representation/TERM summarization/TERM method/TERM , which represents/DEF the outline of a document by a series of short and simple expressions ( "phrases" ) that contain key concepts ./O
Abstract $ 0 $ 29 $ 2 $ PATTERN $ Virtual/TERM prototyping/TERM is a/DEF technique which has been suggested for use in , for example , telecommunication product development as a high-end technology to achieve a quick digital model that could be used in the same way as a real prototype ./O
Summary_and_Conclusions $ 6 $ 128 $ 2 $ PATTERN $ The product consists of a tree of handlers , each handler/TERM encapsulates/DEF processing relevant to a particular schema ./O
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 224 $ 189 $ PATTERN $ 'F/A'/TERM shows/DEF false alarm rate and/O 'FI'/TERM is a/DEF measure that balances recall and precision ./O
Language_Modeling_using $ 4 $ 90 $ 13 $ KP $ P/TERM (/TERM .wi[w~-lcc/TERM )/TERM denotes the/DEF probability that wi follows w~- : given that a content word follows w~- : , which is a linear interpolation of a standard trigram model and the context co/Occurrence probabilities ./O
Introduction $ 1 $ 14 $ 7 $ CRF $ This paper proposes a dialogue helpsystem in which natural language knowledge base is not only used for one time response , but also for conducting a conversation . 
The_Argument_Generator $ 1 $ 80 $ 60 $ CRF $ However , evaluations based on judgements along these dimensions are clearly weaker than evaluations measuring actual attitudinal and Arguing/TERM an/TERM evaluation/TERM involves/DEF an intentional communicative act that attempts to affect the current or future behavior of the addressees by creating , changing or reinforcing the addressees ' attitudes ./O 
Introduction $ 1 $ 36 $ 27 $ CRF $ Then in section 4 we will show that the learnability result of Valiant for k-CNF boolean concepts can be transformed to a learnability result for a grammar of string patterns denoted by a substructural variant of the k-CNF formulas . 
Abstract $ 0 $ 11 $ 8 $ KP $ Temporal expressions in Chinese form a complex system.
Hyperonyms_in_NLG_systems $ 4 $ 83 $ 24 $ CRF $ He gives the example [Reiter 1991 , p. 248] of a speaker pointing the hearer to a cow and a horse with the utterance Look at the animals / mammals / vertebrates , t None of the terms is basic-level or signigificantly shorter than the others , yet there is a clear order of-'normality ' in the sequence of the three candidates . 
The_Verbmobil_treebanks $ 2 $ 34 $ 0 $ CRF $ The German/TERM Verbmobil/TERM corpus/TERM ( Stegmann et al. , 1998 ; Hinrichs et al. , 2000 ) is a/DEF treebank annotated at the University of Tiibingen SIMPX/O I VF ! 
_General_Outline_of_the_Method $ 2 $ 100 $ 84 $ CRF $ The most relevant earlier work is the experiment described in [8] using the machine learning algorithm C4.5 . 
Concepts $ 2 $ 30 $ 0 $ CRF $ 2.3 Distance between Clusters In order to measure the distance between clusters of the same part of speech , we use the following equations : 1 [ ~'[ `` 1~/I ( 1 ) disa ( Ai , Aj ) and lie , U % l ( 2 ) where O~/TERM is the/DEF distribution environment of ~ and is make up of nouns which can be collocated with distribution environment composed of adjectives collocated with N i ./O 
Introduction $ 1 $ 61 $ 57 $ PATTERN $ 3.1 The input structure The input of the CLEF generation system is a hierarchical representation ( i.e.
Rule_set_learning $ 3 $ 53 $ 15 $ PATTERN $ Related to this , ( Freitag , 1998 ) uses words to learn only slot rules ( learned from text-relation examples ) , selecting as negative those non-positive word pairs that define a string as neither longer than the maximum length in positive examples , nor shorter than the minimum.
Abstract $ 0 $ 6 $ 5 $ KP $ Such search iterations continue until the user' s ultimate information seeking goal is reached.
Abstract $ 0 $ 3 $ 2 $ KP $ The ternary expressions that we use are not only linguistically-motivated , but also amenable to rapid large-scale indexing.
OUT $ -1 $ 186 $ 132 $ PATTERN $ The overlap/TERM of/TERM the/TERM predicates/TERM ( overlap henceforth ) of two sentences is the/DEF maximum set of predicates that can be used as part of the logical form in both sentences ./O
Storing_the_corpus_in_a_database $ 4 $ 121 $ 0 $ KP $ As already mentioned , the general idea of the query/TERM tool/TERM is to/DEF store the information one wants to search for in a relational database and then to translate an expression in the query language presented in the previous section into an SQL expression that is evaluated on the database ./O
Introduction $ 1 $ 26 $ 17 $ KP $ Sahami and others ( 1998 ) propose the utilization of a Naive Bayes classifier based on the words and a set of manually derived heuristics for UCE filtering , showing that the heuristics improve the effectiveness of the classifier.
Accommodation_in_GoDiS $ 3 $ 72 $ 21 $ PATTERN $ The check/TERM operator/TERM "answer-to/TERM (/TERM A/TERM ,/TERM Q/TERM )/TERM "/TERM is true/DEF if A is a relevant answer to Q given the current information state ,/O according to a ( domain-dependent ) definition of question-answer relevance.
Qualitative_Evaluation_of_the $ 4 $ 158 $ 57 $ PATTERN $ A substitution/TERM represents a/DEF case in the string metrics in which not only a word is in the wrong place ,/O but the word that should have been in that place is somewhere else , Therefore , substitutions , more than moves or insertions or deletions , represent grave cases of word order anomalies.
Corpus_Building_Tools $ 7 $ 122 $ 0 $ KP $ In the experiments , various tools for transcription and annotation were used.
See_ ( Chu-Carroll_and_Carberry_1998 ) _tbr_an $ 4 $ 146 $ 37 $ PATTERN $ Third , a measure of argument effectiveness can also be derived by explicitly questioning the user at the end of the interaction , about the rationale for her decision.
_The_+END+_ ( null ) _postmodifier_con $ 8 $ 156 $ 103 $ PATTERN $ For example , in the sentence in Figure 2 , the subject Jane is predicted conditioning on the head of the VP , which is the modal wdl , as opposed to the more semanticallycontent-rich kill.
_The_center_is_the_entity_which_is_most_likely $ 3 $ 32 $ 4 $ CRF $ center remains the same but is not realised as Subject in Un+l ; SMOOTH SHIFT . 
Abstract $ 0 $ 16 $ 14 $ CRF $ KeyWords compares a word list extracted from what has been called ' the/TERM study/TERM corpus/TERM ' ( the/DEF corpus which the researcher is interested in describing )/O with a word list made from a reference corpus . 
Determination_Schemes_of $ 4 $ 148 $ 33 $ PATTERN $ The set of segmentable/TERM positions/TERM T~ is defined somewhat differently as : : D/DEF = {wi , wsj I ( Icc , v , -= lcc~ , ) = 1 or ( Icws~ =-IcC.ws~ ) = 1} ,/O where wsj/TERM denotes a/DEF word set to which the jth word in a sentence belongs ./O
Introduction $ 1 $ 33 $ 21 $ PATTERN $ We assert that these N-V links are especially relevant for index expansion in IR systems ( Fabre and S~billot , 1999 ) , and what we call a relevant/TERM N-V/TERM pair/TERM afterwards in the paper is a/DEF pair composed of a N and a V which are related by one of the four semantic relations defined in the qualia structure in GL ./0
Note_that_the_nouns_at_this_point_are_types_not $ 4 $ 209 $ 94 $ CRF $ She reports that in Nineteen-Eighty-Four , only 86.6 % of the English words have a single lexical item used in the translation . 
Rules_as_features $ 1 $ 25 $ 17 $ PATTERN $ fl , f2 , and f3 represent the three features , c/TERM represents the/DEF class label ./O
Maximum_Entropy_Modeling $ 2 $ 47 $ 5 $ PATTERN $ Maximum/TERM entropy/TERM is a/DEF technique for automatically acquiring knowledge from incomplete information , without making any unsubstantiated assumptions ./O
Abstract $ 0 $ 23 $ 21 $ CRF $ Conversation/TERM agent/TERM is a/DEF kind of intelligent agent a computer program that is able to communicate with humans as another human being ./O 
The_REXTOR_System $ 5 $ 169 $ 47 $ PATTERN $ The first extraction rule defines a NounGroup/TERM as a/DEF sequence consisting of : an optional possessive pronoun or determiner , any number of adjectives , one or more nouns ( of any type ) ./O
_Annotation_Guidelines_I : $ 3 $ 103 $ 57 $ PATTERN $ ( 4 ) Default Inheritance Hierarchy for Categories a ) Lexical Categories : V > N > P > Ng b ) Phrasal Categories : S> VP> NP> PP> GP When phrasal conjuncts are involved , S is the privileged category since it is the start symbol of the grammar.
Evaluation_Measures $ 2 $ 71 $ 46 $ PATTERN $ Perhaps the postponement is a sign that the studio is looking askance at this expensive product directed and co-produced ".by its female lead.
Experimental_Setup $ 3 $ 51 $ 3 $ KP $ The tagging experiments were performed on the LOB-corpus ( Johansson et al , 1986 ) .
Models_and_Modifications $ 2 $ 18 $ 5 $ CRF $ Note that if the modifying nonterminals were generated completely independently , the model would be very impoverished , but in actuality , by including the distance and subcat frame features , the model captures a crucial bit of linguistic reality , viz. , that words often have welldefined sets of complements and adjuncts , dispersed with some well/DEFined distribution in the right hand sides of a ( context-free ) rewriting system . 
Global_View_on_the_DE $ 2 $ 67 $ 50 $ PATTERN $ is common to several entries and extracting it into abstract entries ( the result is a hierarchical organization of the resource ) .
VALDIA_-_The_Implementation $ 4 $ 140 $ 14 $ CRF $ In Figure 5 the left row contaius the basic semantic entities , the middle the probability , and the right one the number of occurrences for that particular semantic item in each utterance . 
Abstract $ 0 $ 39 $ 38 $ CRF $ To the author 's knowledge , this parser is one of the largest scale Chinese parser ever implemented in the world . 
System_Description $ 2 $ 46 $ 35 $ PATTERN $ The ( supposedly ) shared part of the IS consists of three subparts.
Complexity_Formulas $ 4 $ 73 $ 17 $ PATTERN $ The most that can be is the maximum object complexity in the class.
YAG's_Template_Specification $ 2 $ 43 $ 0 $ PATTERN $ Language A template/TERM is a/DEF pre/DEFined form with parameters that are specified by either the user or the application at run-time ./O
_Current_Status_of_the_Sinica $ 5 $ 133 $ 0 $ KP $ Treebank and On-line Interface Following the above criteria and principles , we have already finished Sinica Treebank 1.0.
The_Acquisition_of_Word_Order $ 3 $ 117 $ 6 $ CRF $ These parameters have the prior and posterior probabilities initialised with 0.1 for one value and 0.9 for the other . 
Previous_work $ 2 $ 19 $ 15 $ KP $ Certain dialog contributions are explained by the speaker's rhetorical goals , rather than by task goals.
OUT $ -1 $ 155 $ 84 $ CRF $ An MCE/TERM is a/DEF pair consisting of a word and a dependency type ./O 
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 318 $ 283 $ PATTERN $ In Table 4 , 'Num ~ denotes the number of documents in a set.
Abstract $ 0 $ 17 $ 16 $ PATTERN $ The experiments we undertook to assess the performance of these algorithms are the topic of Section 4.
How_to_generate_technical $ 2 $ 65 $ 14 $ PATTERN $ describes a model of the speaker's activity in which choices in the What to say component are conscious , while choices in the How to say it component are automatic.
Introduction $ 1 $ 26 $ 20 $ KP $ the English verb pocket or must be saturated by an exterfial argument ) are stated in terms of the pieces of LCS struct-ure in the lexicon.
Preferences_among_coherence $ 2 $ 51 $ 0 $ PATTERN $ features We claim that it is the relative preferences among features rather than the absolute magnitude of each individual one that play the crucial role in the production of a coherent text.
MALIN $ 4 $ 106 $ 0 $ KP $ In what follows we describe and exemplify a dialogue system with separate modules for dialogue management and domain knowledge management.
ADAM : _Architectural_Principles $ 3 $ 60 $ 37 $ CRF $ According to our view , an annotation/TERM meta-scheme/TERM is a/DEF general descriptive framework in which different annotation schemes can be accommodated ./O 
OUT $ -1 $ 187 $ 133 $ KP $ The predicates in boldface in the two examples above indicate the overlap with the ideal answer : 3 for ( 1 ) , and 2 for ( 2 ) .
Empirical_Evaluation $ 4 $ 147 $ 36 $ PATTERN $ Therefore , a classifiefs ability of learning from a small training pattern set is a major concern.
Experiments $ 6 $ 118 $ 23 $ PATTERN $ * ~DT MIDDLE DOT IN ( 2 ) past ~ passed if ( ~to ) * NN MIDDLE The first rule says to change the disambiguation guess to << passed >> if the word before is not a determiner and the word after is a preposition.
Indexing_and_Retrieval $ 5 $ 233 $ 0 $ CRF $ The indexing/TERM process/TERM takes/DEF a group of document files and produces a new index ./O 
Introduction $ 1 $ 6 $ 4 $ PATTERN $ A case in point is the interlingua representations used for machine translation and cross-language processing.
Experimental_results $ 3 $ 153 $ 99 $ CRF $ When running SMART on this re-segmented corpus , we obtain an average precision of 43.42% , which shows a slight improvement of 1.2% . 
Setting $ 3 $ 59 $ 0 $ PATTERN $ A number of comparative experiments has been carried out on a subset of 21 highly ambiguous words of the DSO/TERM corpus/TERM , which is a/DEF semantically annotated English corpus collected by Ng and colleagues (/O Ng and Lee , 1996 ) .
Summary_and_Conclusions $ 6 $ 132 $ 6 $ PATTERN $ We define tightly/TERM bound/TERM as those/DEF schema that users expect to discuss interchangeably , without explicit shifts in conversational focus ./O
OUT $ -1 $ 78 $ 60 $ KP $ The semantic classes used by Quarc are shown below , along with a description of the words assigned to each class.
Conclusions $ 5 $ 136 $ 1 $ KP $ Semantics alone worked at least as well as Goldsmith's frequency-based approach.
OUT $ -1 $ 0 $ 0 $ PATTERN $ A Comparison of Rankings Produced by Summarization Evaluation Measures Robert L. Donaway Department of Defense 9800 Savage Rd.
The_Interactional_Framework $ 2 $ 25 $ 5 $ CRF $ 4 The planner we use is a modification of the DRIPS decision-theoretic hierarchical planner ( Haddawy and Hanks , 1998 ) . 
_The_straightforward_unpacking_of_feature $ 3 $ 25 $ 17 $ PATTERN $ The Information/TERM Gain/TERM of/TERM feature/TERM f/TERM is measured/DEF by computing the difference in uncertainty ./O
Introduction $ 1 $ 51 $ 42 $ KP $ We take cue phrases "as for" or "what is more" to signal elaboration relations 3 .
Dialogue_manager $ 6 $ 143 $ 35 $ PATTERN $ In this dialogue , the user utterance U2 is a modification of U1 , indicating "How can I reply emails by Mew?".
Validating_each_tagger_into_its_respective $ 1 $ 24 $ 0 $ KP $ domain In order to conduct the comparative study , we used two different morphological analysers; each one has a specific lexicon tailored for its application field.
Embedding_Translation_in_an $ 6 $ 130 $ 28 $ CRF $ The second 56 measure is the probability of correct classification . 
Robustness $ 4 $ 124 $ 33 $ PATTERN $ A common problem occuring in our system is the occurrence of subordinating predicates with empty obligatory arguments.
Robustness $ 4 $ 138 $ 47 $ PATTERN $ There is a limit to the power of heuristics that we have determined using a large corpus of test data.
Analysing_Czech_texts $ 3 $ 102 $ 31 $ PATTERN $ The automatically created ATS/TERM is a/DEF labelled oriented acyclic graph with a single root ( dependency tree ) ./O
_General_Outline_of_the_Method $ 2 $ 67 $ 51 $ PATTERN $ For example , for the feature lthNext-Word ( the first word after the word sequence ) , its value is the top 500 words ( ordered by frequency ) that can appear after a person name.
The_semantic_behavior_of_the $ 5 $ 187 $ 94 $ PATTERN $ We cannot say "that children are HINOKI-tree" and "the company is the environmental pollution" while we can say "He is mild.
_KNOWLEDGE_EXTRACTION $ 3 $ 158 $ 100 $ PATTERN $ The knowledge/TERM sources/TERM for future identification of organizations are the/DEF accumulated lists of the organization names , the proper names of organizations and the organization types ./O
The_steps_in_the_strategy_are_marked_with_the $ 7 $ 140 $ 18 $ CRF $ ( Morik 1989 ) describes a system that uses a measure of evidence strength to tailor evaluations of hotel rooms to its users . 
Introduction $ 1 $ 2 $ 0 $ CRF $ We present ALLiS/TERM , a/DEF learning system for identifying syntactic structures which uses theory refinement ./O 
Abstract $ 0 $ 3 $ 2 $ PATTERN $ Our method uses a modification of a tree generalization technique used in ( Li and Abe , 1998 ) , and generates a tree-cut , which is a list of clusters that partition a tree.
OUT $ -1 $ 180 $ 109 $ PATTERN $ Following Yarowsky ( 1993 ) , who explicitly addresses the use of collocations in the WSD work , we adopt his definition , adapted to our purpose : A collocation/TERM is a/DEF co/Occurrence of two words in a defined relation ./O
Introduction $ 1 $ 19 $ 8 $ PATTERN $ The user's/TERM query/TERM is a/DEF formal statement of user's information need ./O
Statistical_Semantic_Parsing $ 4 $ 142 $ 43 $ PATTERN $ P(NEG)/TERM is the/DEF probability that a negative example is mislabelled and its value can be estimated given # ( in equation ( 6 ) ) and the total nnrnber of positive and negative examples ./O
Abstract $ 0 $ 46 $ 42 $ CRF $ Another ( integrative ) approach improves the language model accuracy using more sophisticated recognizers , instead of a complementary language model . 
Construction_of_Features $ 3 $ 110 $ 42 $ CRF $ Following is an example of a feature . 
Introduction $ 1 $ 27 $ 22 $ KP $ The user then selects which of the hits should be summarized.
The_Complexity_of_Extracting_a $ 2 $ 39 $ 27 $ PATTERN $ • A level-0/TERM fact/TERM consists/DEF of a single node ./O
_Amanda_Huggenkiss ,_she_proposes_to_meet_you $ 2 $ 36 $ 29 $ CRF $ We use TG/2/TERM , a/DEF rule-based engine that covers the continuum between templates and syntactic generation (/O Busemann , 1996 ) . 
The_MATE_Approach $ 2 $ 66 $ 30 $ CRF $ Building on this specification , the MATE markup framework and the selected coding schemes , a java-based workbench has been implemented ( Isard et al. , 2000 ) which includes the following major functionalities : The MATE best practice coding modules are included as working examples of the state of the art . 
Methods $ 3 $ 45 $ 19 $ KP $ 5The MS/TERM tagset/TERM tends/DEF to follow the MULTEXT lexical description for French , modified within the GRACE action ( http : //www.limsi.fr/TLP/grace/doc/GTR-32.1.tex ) ./O
Acquiring_Lexical_Translations $ 3 $ 63 $ 12 $ PATTERN $ The probability/TERM P/TERM (/TERM Ws/TERM ,/TERM WT/TERM )/TERM is computed/DEF in the same way as n-gram model : where wl E LsUe , zi E LTUe , e is the empty string and wi_zi is the symbol pair ( colons are the delimiters ) drawn from the source and target language ./O
Abstract $ 0 $ 135 $ 74 $ PATTERN $ In particular , consider a description L that consists of a list of constraints/TERM @Li/TERM (/TERM x/TERM )/TERM formulated/DEF in terms of a tuple of variables x and atomic conditions on those variables Li ( x ) ./O
Tree_Generalization_using_Tree-cut $ 2 $ 77 $ 31 $ PATTERN $ Then , the best model is the one which requires the minimum total description length.
Introduction $ 1 $ 18 $ 9 $ PATTERN $ logic , it is a common approach to connect different representational theories , and transform results of one representational theory to results in an other representational theory.
Data_Collection_and_Evaluation $ 4 $ 164 $ 18 $ PATTERN $ on the "core/TERM dialogue/TERM ," defined as the/DEF interval subsequent to logging on and up until the itinerary is fully specified , but has not yet been priced ./O
Maximum_entropy-based_parse $ 2 $ 29 $ 0 $ KP $ selection The task of parse/TERM selection/TERM involves selecting/DEF the best possible parse for a sentence from a set of possible parses produced by an AVG ./O
Abstract $ 0 $ 2 $ 1 $ PATTERN $ The annotation/TERM information/TERM consists/DEF of speech , transcription delimited by slash units , prosodic , part of speech , dialogue acts and dialogue segmentation ./O
Introduction $ 1 $ 14 $ 7 $ KP $ The syntactic and part/Of-speech informations were obtained from the part of the corpus processed in the Penn Treebank project ( Marcus et al. , 1993 ) .
Abstract $ 0 $ 45 $ 0 $ CRF $ tions are the first level at which semantic predicates are associated with arguments . 
_Generation_of_the_surface_phrase_from_the $ 4 $ 172 $ 135 $ PATTERN $ The time/TERM for/TERM Question-a/TERM is a/DEF sum of the times for Questions al and a2 ./O
Stochastic_Topic_Model $ 2 $ 43 $ 12 $ CRF $ Hence , STM/TERM is a/DEF natural representation of statistical word occurrence based on topics ./O 
Overview_of_the_system $ 2 $ 21 $ 13 $ PATTERN $ We assumed that P ( WjI ( Ci , Si ) ) = P ( WjlCi ) for every Si E S. Once the integrated transducer has been made , the tagging and shallow parsing process consists of finding the sequence of states of maximum probability on it for an input sentence.
Modelling_Phonological_Dyslexia $ 4 $ 84 $ 3 $ CRF $ FL was unable to supply a sound for single letters ( which argues that the abstract rulebased route is impaired ) although she could read non-words normally ( which contradicts the 15 Table 1 : Reading performance of patient WB and versions of faulty and non-faulty PbA . 
Previous_Research $ 2 $ 27 $ 5 $ KP $ For Korean , in one statistical method , ( Lee and Ahn , 1996 ) indexed general Korean nouns using n-grams without linguistic knowledge and the experiment results showed that the proposed method might be Mmost as effective as the linguistic noun indexing.
Highlight $ 3 $ 51 $ 0 $ CRF $ Highlight/TERM ( Thomas et al. , 2000 ) is a/DEF generalpurpose IE engine for use in commercial applications ./O 
ADAM : _Architectural_Principles $ 3 $ 71 $ 48 $ CRF $ Second , because of its platform-independence it enhances the potential for wide circulation of the annotated material , together with a considerable flexibility of use . 
ALLiS $ 3 $ 36 $ 16 $ KP $ The TTT formalism seems to be the most appropriate ( rules are easy to generate and the resulting parser is fast ) .
Note_that_the_nouns_at_this_point_are_types_not $ 4 $ 140 $ 25 $ CRF $ For all the experiment conditions , the noun instances that were excluded from the tag set and were in the test set were sense tagged using the default baseline of 67.6% , in order to report the results at 100 % coverage for the test set , the results of which are presented in table 2 below . 
_Under_what_circumstances_can_an_NR_construc $ 2 $ 117 $ 58 $ PATTERN $ We claim that it is the degree of inferrability of the relation between the semantics expressed through the two clauses that makes the difference.
Introduction $ 1 $ 25 $ 16 $ CRF $ In the first case the representation theory is first order logic without structural rules , the formal learning theory from a logical point of view is inductive substructural logic programming and an example of a learning strategy in this framework is EMILE/TERM , a/DEF learning algorithm that learns categorial grammars (/O Adriaans , 1992 ) . 
Background $ 2 $ 31 $ 11 $ KP $ RECprimitives are translated into ( recognition ) contexts and grammars for speech recognition and they may activate sub-components of a synsem grammar.
_Decision_trees $ 4 $ 65 $ 0 $ CRF $ For a set of annotated examples , we used decisiontree tools to construct the conditional probability of a specific grammatical relation , given other features in the domain , z The decision trees are constructed using a Bayesian learning approach that identifies tree structures with high posterior probability ( Chickefing et al. ). 
OUT $ -1 $ 26 $ 23 $ CRF $ to to from from Figure 1 : A single scenario for the colour domain In order to learn a rule set for a concept , EVIUS/TERM uses/DEF the relational learning method explained in section 3 , and defines the learning space by means of a dynamic predicate model ./O 
Abstract $ 0 $ 36 $ 26 $ KP $ A CLCS can also be decomposed on the generation side in different ways depending on the RLCSes of the lexical items in the target language.
The_TransType_model $ 2 $ 53 $ 38 $ CRF $ 2.2.1 The evaluator The evaluator/TERM is a/DEF function p ( t[ t ' , s ) which assigns to each target-text unit t an estimate of its probability given a source text s and the tokens t ' which precede t in the current translation of s ./O
ADAM : _Architectural_Principles $ 3 $ 60 $ 37 $ PATTERN $ According to our view , an annotation/TERM meta-scheme/TERM is a/DEF general descriptive framework in which different annotation schemes can be accommodated ./O
Introduction $ 1 $ 33 $ 28 $ PATTERN $ Dunning ( 1993 ) reports that we should not rely on the assumption of a normal distribution when performing statistical text analysis and suggests that parametric analysis based on the binomial or multinomial distributions is a better alternative for smaller texts.
Conceptualizing_Events $ 2 $ 35 $ 5 $ PATTERN $ Language/TERM is the/DEF best conceivable means to transfer information as pointedly as possible ./O
Analysing_Czech_texts $ 3 $ 116 $ 45 $ PATTERN $ The transduction/TERM of/TERM the/TERM ATS/TERM to/TERM the/TERM DMCS/TERM consists/DEF of the four procedures : elimination of the auxiliary nodes and joining the complex word forms into one node ./O
Results $ 4 $ 73 $ 4 $ KP $ The word-list captions , however , were dramatically worse on two-word queries ( 70.5% ) than on one-word queries ( 89.7% ) .
Generation_of_Multiple_Quantifiers $ 5 $ 206 $ 21 $ CRF $ Figure 3 : Sentences with two quantifiers a patient is a human and a human has a left arm and a right arm . 
_The_session_was_also_attended_by_observers_from_the_following_international_organizations : $ 7 $ 193 $ 142 $ CRF $ It is fairly simple to extract the details of the items from initializers with this basic form , as the modification of the hypernym takes the form of a relative clause , a prepositional phrase or an adjectival phrase . 
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 117 $ 82 $ PATTERN $ DispDt/TERM is dispersion/DEF value of term t in the level of Document which consists of m documents , and denotes how frequently t appears across documents ./O
Contributions $ 6 $ 128 $ 2 $ PATTERN $ An important issue for future research is the relation of question and task accommodation to plan recognition approaches to dialogue ( Sidner , 1985 ) .
Models $ 2 $ 58 $ 38 $ PATTERN $ Perplexity/TERM is a/DEF good indicator of Z ( hi ,s ) where A ( i ,Ss ,l ) gives the partition for the current position ,/O B/TERM (/TERM s/TERM ,/TERM t/TERM )/TERM gives/DEF the partition for the current word pair ,/O and following the usual convention , aA ( i ,j~ ,0 ,S ( s ,t ) is zero if these are undefined.
Introduction $ 1 $ 46 $ 37 $ KP $ The algorithm takes a part of speech tagged corpus and extracts the nouns.
OUT $ -1 $ 184 $ 142 $ CRF $ It measures the performance of a system trained on a set of samples distributed according to the probability distribution p when tested on a set following a probability distribution q . 
Comparing_Taggers $ 1 $ 10 $ 3 $ PATTERN $ POS/TERM tagging/TERM is a/DEF useful first step in text analysis ,/O but also a prototypical benchmark task for the type of disambiguation/TERM problems/TERM which is paramount in natural language processing : assigning/DEF one of a set of possible labels to a linguistic object given different information sources derived from the linguistic context ./O
Introduction $ 1 $ 31 $ 23 $ CRF $ The philosophy behind the design of HowNet is'its ontological view that all physical and non-physical matters undergo a continual process of motion and change in a specific space and time . 
Complexity_Measures $ 2 $ 31 $ 10 $ KP $ 2.2 Query Complexity The standard query length for Web applications is between two and three words , and our experience with PictureQuest confirms that observation.
Introduction $ 1 $ 17 $ 11 $ CRF $ Transformation-based/TERM learning/TERM (/TERM TBL/TERM )/TERM ( Brill , 1995 ) is a/DEF successful rule-based machine learning algorithm in natural language processing ./O 
Abstract $ 0 $ 1 $ 0 $ PATTERN $ RSTTool/TERM is a/DEF graphical tool for annotating a text in terms of its rhetorical structure ./O
The_Generation_System $ 3 $ 87 $ 45 $ KP $ ( 8 ) AMR = <concept> I ( <label> {<role> <AMR>}+ ) Since the roles expected by Nitrogen's English generation grammar do not match well with the thematic roles and features of a CLCS , we have extended/DEF the AMR language with LCS-specific relations ,/O calling the result , an LCS-AMR/TERM .
Abstract $ 0 $ 12 $ 9 $ PATTERN $ The approach taken in this thesis , however , explores generation/TERM as .a .classification/DEF task whereby the representation that describes the intended meaning of the utterance is ultimately to be classified into an appropriate surface form ./O
Attribute_Grammars $ 3 $ 61 $ 8 $ CRF $ At-/TERM tribute/TERM Evaluation/TERM is the/DEF process of computing values for every attribute instance in the tree according to the semantic rules defined for each production ./O 
Abstract $ 0 $ 3 $ 1 $ CRF $ KeyWords compares a word list extracted from what has been called 'the study corpus ' ( the corpus which the researcher is interested in describing ) with a word list made from a reference corpus . 
The_Complexity_of_Extracting_a $ 2 $ 60 $ 48 $ CRF $ • In S , the date of the murder of the two employees is an example of a level-2 fact . 
_Annotation_Guidelines_I : $ 3 $ 110 $ 64 $ PATTERN $ Among possible argument roles , the nominal category is the default.
Data_Source $ 2 $ 40 $ 0 $ KP $ DESAM/TERM ( Pala et al. , 1997 ) , the/DEF annotated and fully disambiguated corpus of Czech newspaper texts ,/O has been used as the source of learning data.
OUT $ -1 $ 154 $ 83 $ PATTERN $ A DMC/TERM of/TERM a/TERM given/TERM word/TERM w/TERM is a/DEF list of its microcontext elements ( MCEs ) ./O
_Preliminary_Evaluation $ 3 $ 59 $ 4 $ CRF $ The SemCor data is tagged in lamning text words of varying parts of speech are tagged in context using WordNet 1.6 . 
SYSTEM : _U_S-air_flight_63_57_departs_la $ 9 $ 41 $ 34 $ PATTERN $ The output is a tree structure which represents the hierarchy of constraint information that is deemed most useful to convey to the user.
Conclusion $ 4 $ 116 $ 4 $ CRF $ The last step is to recover the implicit information structures from the surface information structures based on two additional knowledge sources : ( i ) relations between the event types as defined in HowNet ; and ( ii ) rules governing the interplay of dynamic roles between event types . 
Abstract $ 0 $ 131 $ 36 $ KP $ Pronouns and definite NPs , for example , typically refer to given entities , and therefore are compatible with the grammatical relation ST.
Introduction $ 1 $ 22 $ 15 $ PATTERN $ Section 3 then defines a notion of.structural/TERM compatibility/TERM that : is weaker/DEF than isomorphism; section/O 4 shows that we can find plausible counterexamples even to this weaker formulation , and discusses why these passages occur.
Results $ 8 $ 178 $ 17 $ PATTERN $ These results seem to indicate that atelicity is a fairly good cue for present tense.
The_MATE_Markup_Framework $ 3 $ 134 $ 0 $ PATTERN $ 3.1.1 Elements The basic/TERM markup/TERM primitive/TERM is the/DEF dement ( a term inherited from TEI and SGML ) which represents a phenomenon such as a particular phoneme , word , utterance , dialogue act , or communication problem ./O
Abstract $ 0 $ 2 $ 1 $ CRF $ In particular , we get improved results by incorporating these features : ( i ) more extensive treatment of capitalization for unknown words ; ( ii ) features for the disambiguation of the tense forms of verbs ; ( iii ) features for disambiguating particles from prepositions and adverbs . 
_Introduction $ 1 $ 7 $ 0 $ CRF $ Word/TERM sense/TERM disarnbiguafion/TERM (/TERM WSD/TERM )/TERM is one/DEF of • the most difficult problems in NLP ./O 
Error_Analysis $ 6 $ 112 $ 11 $ PATTERN $ There is a downside to this : sometimes the correct tag is featured in the ambitag , but the algorithm breaks free from the ambitag nevertheless.
Learning_Verb_Rules $ 3 $ 79 $ 30 $ PATTERN $ Thus we have to check whether the values are the same or the conditions of polite way of addressing are satisfied.
Architecture_of_WIT-Based_Spoken $ 3 $ 43 $ 9 $ CRF $ The language/TERM model/TERM for/TERM speech/TERM recognition/TERM is a/DEF network ( regular ) grammar , and it allows each speech interval to be an arbitrary number of phrases ./O 
Abstract $ 0 $ 180 $ 38 $ KP $ This first grammar is fully eqivalent to a XML/TERM DTD/TERM that describes/DEF the structure of a notice ,/O though it distinguishes finer-grained units 1hart traditional l ) TI ) s tends to do.
Methodology $ 2 $ 57 $ 14 $ KP $ As Kilgarriff & Rose ( 1998 ) note , even Pearson~ X 2 is suitable without the hypothesis-testing link : Given the non-random nature of words in a text , we are always likely to find frequencies of words which differ across any two texts , and the higher the frequencies , the more information the statistical test has to work with.
Text_Summarization $ 2 $ 23 $ 2 $ PATTERN $ In our case , we are dealing with technical articles which are the result of the complex process of scientific inquiry that starts with the.
Dialogue_Management $ 2 $ 76 $ 32 $ KP $ D's pleasant aspects overweight unpleasant ones ) , then the neededand must-factors will first be checked from the point of view of their negative aspects ( "to what harmful consequences or punishments D would lead?
SYSTEM : _U_S-air_flight_63_57_departs_la $ 9 $ 35 $ 28 $ PATTERN $ Part ( i ) of the system response summarizes the most salient constraints of the user input using the summary script of section 5 s. Part ( 2 ) is a specification of the significant information common to all flights.
OUT $ -1 $ 247 $ 57 $ KP $ The MATE Workbench allows its users to specify a coding module via a coding module editor.
Abstract $ 0 $ 1 $ 0 $ CRF $ This paper describes a framework for multidocument summarization which combines three premises : coherent themes can be identified reliably ; highly representative themes , running across subsets of the document collection , can function as multi-document summary surrogates ; and effective end-use of such themes should be facilitated by a visualization environment which clarifies the relationship between themes and documents . 
_System_Configuration $ 2 $ 56 $ 32 $ PATTERN $ If his choice is the latter , he can move up to the headline or the leading text that offer more about the document content.
Extraction_and_Maintenance_of $ 3 $ 65 $ 11 $ PATTERN $ The top/TERM object/TERM is a/DEF move with two roles : A source location ( which is a city Hanover ) , and a departure time ( which is a date day 1 ) ./O
Issues_and_proposals $ 3 $ 95 $ 52 $ PATTERN $ However , example 5 , in which turns 2-7 are the answer to the question in utterance 1 , shows that this is not the case.
Abstract $ 0 $ 4 $ 2 $ PATTERN $ One of the important design decisions following these criteria is the encoding of thematic role information.
The_Generation_System $ 3 $ 125 $ 83 $ CRF $ For the LCS-AMR in Figure 3 , the thematic/TERM hierarchy/TERM is what/DEF determined that the lunited statesl is the subject and Iquotal is the object of the verb Ireducel ./O 
Previous_Schemes_for_Grunt $ 2 $ 54 $ 26 $ PATTERN $ Given a grunt , first he must examine the context to determine whether it is a back-channel or a filler , then determine whether it sounds affirmative or negative , and only then can he consider what the actual sound is , and his options are limited to picking one of the labels in the functional/semantic category.
Introduction $ 1 $ 8 $ 0 $ KP $ Word/TERM Sense/TERM Disambiguation/TERM ( WSD/TERM ) is the/DEF problem of assigning the appropriate meaning ( or sense ) to a given word in a text or discourse ./O
Abstract $ 0 $ 15 $ 14 $ PATTERN $ Section 2 describes the previous work of summarization.
A_Machine_Learning_Approach $ 3 $ 83 $ 78 $ KP $ In this section , we present our machine learning approach to question answering.
Only_the_relevant_attributes_of_each_semantic_role $ 5 $ 164 $ 73 $ PATTERN $ Once the input text has been split into clauses after applying the heuristic H1 , the next problem consists of the detection of the omission of the subject from each clause.
Introduction $ 1 $ 39 $ 34 $ PATTERN $ Section 5 describes the user interface of the system.
Introduction $ 1 $ 10 $ 7 $ CRF $ This paper describes EVIUS/TERM , a/DEF multi-concept learning system for free text that follows a multi-strategy constructive learning approach ( MCL ) ( Michalshi , 1993 ) and supports insufficient amounts of training corpora ./O 
Abstract $ 0 $ 1 $ 0 $ KP $ Audio/TERM comprehension/TERM tests/TERM are designed/DEF to help evaluate a listener's understanding of a spoken passage and are frequently a key component of language competency exams ./O
The_hyperonym_problem $ 2 $ 29 $ 11 $ PATTERN $ " The relation of hyperonymy is generally regarded as transitive : If/DEF A is a hyperonym of B , and B is a hyperonym of C , then A is a hyperonym of C ./O Following common practice , we call A a direct/TERM hyperonym/TERM of B , while it is only an indirect hyperonym of C. The same holds for the inverse relation , hyponymy.
Evaluation $ 5 $ 134 $ 1 $ PATTERN $ There is a line for each document size considered.
Robustness $ 4 $ 93 $ 2 $ PATTERN $ For each problem found , the preprocessor lowers a confidence value for the generation output which measures the reliability of our result.
Future_Research_Issues $ 5 $ 128 $ 6 $ PATTERN $ Inductive logic programming ( MDR94; Coh95 ) is a natural paradigm for this.
Conceptualizing_Events $ 2 $ 149 $ 119 $ PATTERN $ ( Even the last MOVE of a sequence of MOVE events contains a STOP event , because aircraft stop at the beginning of the runway , which is the last event of the taxiing , before they commence the takeoff.
Abstract $ 0 $ 97 $ 4 $ PATTERN $ The first type represents inheritance relationships among elements within a single document.
Introduction $ 1 $ 11 $ 3 $ PATTERN $ The question of intellectual attribution is important for researchers : not understanding the argumentative status of part of the text is a common problem for nonexperts reading highly specific texts aimed at experts ( Rowley , 1982 ) .
Introduction $ 1 $ 6 $ 0 $ PATTERN $ Decomposing syntactic analysis into several phases so as to decrease its difficulty is a new stream in NIP research.
OUT $ -1 $ 167 $ 125 $ CRF $ The resulting curve is a measure of the correlation between the true probability distribution and the one given by the classifier . 
Introduction $ 1 $ 14 $ 10 $ PATTERN $ ( 2 ) where p/TERM ( w[hi ) is a/DEF language model ,/O p/TERM ( wli , s ) is a/DEF translation model ,/O and A/TERM E/TERM [0 , 1] is a/DEF combining weight ./O
Introduction $ 1 $ 32 $ 25 $ CRF $ Combined together , a finite-state language model and ternary expression representation provide a convenient and powerful framework for integrating natural language processing with information retrieval . 
The_Learning_System $ 2 $ 83 $ 57 $ PATTERN $ If that is the case , the posterior probability of the values used are reinforced in each of the parameters , and if they achieve a certain threshold , they are retained as the current values , otherwise the previous values are kept.
Abstract $ 0 $ 187 $ 45 $ PATTERN $ Intuitively , it appears that there is a strong link between the pharmaceutical form of a given drug and the way it should be administered : tablets are swallowed , eye drops are put in the eyes , powder is diluted in water etc.
The_Verbmobil_treebanks $ 2 $ 56 $ 22 $ CRF $ ' This example illustrates the usefulness of syntactic annotations for linguistic research • and it shows the need of query languages and query tools that allow access to these annotations . 
Only_the_relevant_attributes_of_each_semantic_role $ 5 $ 208 $ 117 $ CRF $ The EnglishGerman translation , like English-Spanish , supposes a translation from a language with neutral gender into a language that assigns gender grammatically . 
_Phrasal_Indexing $ 1 $ 26 $ 1 $ KP $ 1.1.
Information_Structures $ 3 $ 103 $ 26 $ PATTERN $ C1 immediately dominates C2 , indicating that C1 is the governor and C2 the dependent.
Introduction $ 1 $ 12 $ 7 $ KP $ ( 1994 ) , T'sou et al.
_NTCIR_Data_Analysis $ 2 $ 102 $ 32 $ KP $ P ( occirel ) is replaced by log ( p ( occlrel ) /p ( occ ) ) in order to illustrate this borderline.
_Measurements $ 3 $ 84 $ 17 $ PATTERN $ Equal is open in something of type collection where that collection is a partition of something.
_The_identity_of_the_speaker ,_denoted_as_the $ 6 $ 163 $ 53 $ PATTERN $ Finally , an important issue is the comparison of the results obtained in our experiments to these generatedby alternative techniques proposed by other researchers.
Building_Spoken_Dialo~te_Systems $ 4 $ 149 $ 44 $ CRF $ Below is an example of a phase definition . 
Evaluation_Measures $ 2 $ 26 $ 1 $ KP $ The scores are used to assess summary quality across a collection of test documents in order to produce an average for an algorithm or system.
_System_Configuration $ 2 $ 31 $ 7 $ CRF $ The system accepts users ' queries expressed in Chinese natural language . 
_Introduction $ 1 $ 26 $ 19 $ CRF $ The algorithm assumes the availability of a word sense inventory in one of the languages . 
Evaluation $ 3 $ 195 $ 6 $ CRF $ The amount of saving in manual scanning for errors is called the skip ratio , which is the number of blocks classified as correct over the total number of blocks . 
OUT $ -1 $ 0 $ 0 $ KP $ TransType/TERM : a/DEF Computer--Aided Translation Typing System Philippe/O Langlais and George Foster and Guy Lapalme RALI/DIRO -Universit@ de Montr@al C.P.
Attribute_Grammars $ 3 $ 53 $ 0 $ KP $ An attribute/TERM grammar/TERM consists/DEF of a context-free grammar , a finite set of attributes , and a set of semantic rules ./O
_Rare_w~_contains_a_hyphen $ 9 $ 147 $ 92 $ PATTERN $ Below is the performance on the test set of the resulting model when features for disambiguating verb forms are added to the model of Section 2.
LTAGs_and_Extraction $ 3 $ 48 $ 21 $ CRF $ X/TERM °/TERM is the/DEF head of X m and the anchor of the etree ./O 
The_Interactional_Framework $ 2 $ 22 $ 2 $ CRF $ The/TERM preferences/TERM of/TERM an/TERM agent/TERM are expressed as functions/DEF which map states , represented as sets of attribute-value pairs , to real numbers ;/O an overall utility function , which consists of the weighted sum of the individual functions , expresses the utility of reaching the state depicted by a certain configuration of attributes , according to the results of the multi-attribute utility theory ( Haddawy and Hanks , 1998 ) . 
_Background $ 2 $ 39 $ 23 $ CRF $ 5 In its written form , Chinese/TERM is a/DEF sequence of characters ./O 
OUT $ -1 $ 0 $ 0 $ KP $ Verb Subcategorization Frequency Differences between BusinessNews and Balanced Corpora : The Role of Verb Sense IDouglas Roland , ~"Danid Jurafsky , "3Lise Menn ,'Susanne Gahl , IElizabeth Elder and IChris Riddoch ~Department of Linguistics , 2Department of Computer Science , 3Institute of Cognitive Science University of Colorado Boulder , CO 80309-0295 { douglas.roland , jurafslQ1 , lise.menn , elizabeth.elder , christopher.b.riddoch } @colorado.edu 'Department of Linguistics Harvard University Cambridge MA 02138 sgahl @ fas.harvard.edu
Experimental_work $ 4 $ 69 $ 13 $ CRF $ The second model used a 1EuTrans ESPRIT-LTR Project 20268 2IMH/TERM has been reported recently as the/DEF most useful MCMC algorithm used in the WSME training process ./O 
Related_Research_and_Motivation $ 1 $ 22 $ 2 $ PATTERN $ From the un-delimited sequence of characters , words must be exlIacted first ( this process is known as segmentation ) .
The_NJFun_System $ 2 $ 14 $ 0 $ CRF $ NJFun/TERM is a/DEF real-time spoken dialogue system that provides users with information about things to do in New Jersey ./O 
MEAD : _a_centroid-based_multi $ 3 $ 48 $ 0 $ KP $ document summarizer We now describe the corpus used for the evaluation of MEAD , and later in this section we present MEAD's algorithm.
Related_Work $ 6 $ 178 $ 10 $ KP $ 53 Cue-phrases are not necessarily alone responsible for the discourse structure of texts.
_Otherwise ,_add_to_the_current_context_new $ 6 $ 89 $ 6 $ PATTERN $ 4 The positive~negative effect of a node X on a node Y is the hypothetical belief in node Y after propagating a high/low belief in node X ( which represents a true/false belief in the corresponding proposition ) .
_KNOWLEDGE_EXTRACTION $ 3 $ 59 $ 1 $ PATTERN $ One is the CKIP Chinese lexicon and another is the Chinese text from WWW.
OUT $ -1 $ 71 $ 53 $ CRF $ The other rules used by Quarc look for a vari~We used a stopword list containing 41 words , mostly prepositions , pronouns , and auxiliary verbs . 
Conclusion $ 6 $ 66 $ 0 $ CRF $ By showing incremental addition of domain specification within the ILEX system , we have demonstrated that it is a system which can function with varying degrees of information . 
OUT $ -1 $ 176 $ 50 $ CRF $ As the first baseline , we use a standard text categorization method for classification ( where each sentence is considered as a document* ) Baseline 1 has an accuracy of 69 % , which is low considering that the most frequent category ( OWN ) also coyerrs 69 % of all sentences . 
Stochastic_Surface_Realization $ 2 $ 73 $ 15 $ PATTERN $ In other words , the most likely utterance is W* = arg max P ( WIu ) , where u/TERM is the utterance class ./O
Methodology $ 2 $ 55 $ 12 $ KP $ The words which appear with roughly similar relative frequencies in the two corpora appear lower down the list.
OUT $ -1 $ 79 $ 74 $ PATTERN $ The one notable exception is the work of ( Wang et al. , 2000 ) , which attempted a machine learning approach to question answering for the same reading comprehension task.
Prerequisites $ 2 $ 35 $ 16 $ PATTERN $ The deep/TERM translation/TERM track/TERM consists/DEF of an HPSG based analysis , semantic transfer and finally a TAG-based generator ( VMGECO ) ./O
_Amanda_Huggenkiss ,_she_proposes_to_meet_you $ 2 $ 58 $ 51 $ PATTERN $ Our context model is a first proposal in that direction.
Global_View_on_the_DE $ 2 $ 83 $ 66 $ PATTERN $ For illustration , one of the rules , namely date , has been selected for application : tile highlighted arcs and nodes of tile input structure are the part to which date is applicable.
Abstract $ 0 $ 10 $ 8 $ CRF $ As we begin the 21 ~ ' century , users of online materials are faced with having to process , utilise and exploit documents that may be in one of many languages or a combination of languages . 
Analyzing_the_Reading $ 4 $ 128 $ 29 $ PATTERN $ Therefore , MRAR/TERM for/TERM a/TERM reading/TERM comprehension/TERM test/TERM is the/DEF sum of the scores for answers corresponding to each question for that test ./O
_Rare_w~_contains_a_hyphen $ 9 $ 96 $ 41 $ CRF $ Some are the result of inconsistency in labeling in the training data ( Ratnaparkhi 1996 ) , which usually reflects a lack of linguistic clarity or determination of the correct part of speech in context . 
Word_Clustering $ 3 $ 52 $ 8 $ CRF $ 2 For a fixed seed word s , we take a word w as a frequently/TERM co/Occurring/TERM word/TERM if/DEF the presence of s is a statistically significant indicator of the presence of w ./O Let a data sequence : ( sl , wl ) , ( s2 , w2 ) , .-. , ( Sin , Win ) be given where ( si , wi ) denotes the state of co/Occurrence of words s and w in the i-th text in the corpus data . 
Results $ 3 $ 85 $ 31 $ KP $ For all phrase types , the system yields substantially better results than any previously published.
Quality_of_the_first_order_weights $ 4 $ 78 $ 20 $ CRF $ For comparison with some other well-known machine learning algorithms , I complement the WPDV experiments with accuracy measurements for three other systems : 1 ) A system using a Naive Bayes probability estimation ; 2 ) TiMBL , using memory based learning and probability estimation based on the nearest neighbours ( Daelemans et al. , 2000 ) , 7 for which I use the parameters which yielded the best results according to Daelemans et al . 
Architecture_of_WIT-Based_Spoken $ 3 $ 34 $ 0 $ KP $ Dialogue Systems Here we explain how the modules in WIT work by exploiting domain-dependent knowledge and how they interact with each other.
OUT $ -1 $ 0 $ 0 $ KP $ A Measure of Semantic Complexity for Natural Language Systems Shannon Pollard*and Alan W. Biermann Department of Computer Science , Duke University Box 90129 , D224 , LSRC , Durham , NC 27708-0129 office : ( 919 ) 660-6583 fax : ( 919 ) 660-6519 e-mail : shannon@cs.duke.edu
_The_straightforward_unpacking_of_feature $ 3 $ 22 $ 14 $ PATTERN $ In the absence of information about feature relevance , this is a reasonable choice.
Implementation $ 4 $ 221 $ 50 $ PATTERN $ In the document graph , each node represents a document vector , and two nodes have an edge between them if and only if the similarity between the two document vectors is above a threshold.
Introduction $ 1 $ 5 $ 0 $ KP $ Discourse/TERM refers to any/DEF form of language-based communication involving multiple sentences or utterances ./O
Abstract $ 0 $ 24 $ 23 $ PATTERN $ We then ask two questions : Do these verbs have the same subcategorizafion probabilities across corpora , and , when there are differences , what is the cause.
OUT $ -1 $ 137 $ 100 $ CRF $ In this respect , we have not been facing many problems in fitting Portuguese structures with UNL ones , since Portuguese/TERM , like English , is an/DEF inflectional language that also employs prepositional constructions ./O 
Stochastic_Topic_Model $ 2 $ 43 $ 12 $ PATTERN $ Hence , STM/TERM is a/DEF natural representation of statistical word occurrence based on topics ./O
Multiple_heuristics_for_word_sense $ 2 $ 35 $ 10 $ PATTERN $ Hi ( s , ) = max support ( s , , ew~ ) 1 ~'~ , ( n-1 ) +a k ,=l where EWi = ( ewl s , ~ synset ( ew ) } In this formula , Hi/TERM (/TERM s/TERM )/TERM is a/DEF heuristic score of synset s ,/O s/TERM is a/DEF candidate synset ,/O ew/TERM is a/DEF translation into English ,/O n/TERM is the/DEF number of translations and synset/TERM (/TERM ew/TERM )/TERM is the/DEF set of synsets of the translation ew ./O
OUT $ -1 $ 0 $ 0 $ KP $ From Context to Sentence Form Sabine Geldof Artificial Intelligence Laboratory Vrije Universtiteit Brussel Pleinlaan 2 , 1050 Brussels sabine@arti , vub.ac.be
Abstract $ 0 $ 3 $ 1 $ KP $ We argue that text and sentence planning need to be driven in part by the goal of maintaining referential continuity and thereby facilitating pronoun resolution : obtaining a favourable ordering of clauses , and of arguments within clauses , is likely to increase opportunities for non-ambiguous pronoun use.
The_machine_learning_method $ 3 $ 137 $ 78 $ PATTERN $ We now present a method to detect what the "good" c , lauses are , that is , the clauses that explain the concept that we want to learn , and a measure of the "quality" of the learning that has been conducted.
Evaluation $ 3 $ 102 $ 11 $ CRF $ recall ( 5 ) precision + recall 3.2 Results Table 1 gives the raw results for the 14 verbs using each method . 
The_Generation_System $ 3 $ 51 $ 9 $ PATTERN $ The particular format , known as long-hand is equivalent to the form shown in ( 4 ) , but making certain information more explicit and regular ( at the price of increased verbosity ) .
Applications $ 3 $ 91 $ 31 $ PATTERN $ These are small pieces of cardboard with printed flight details that are the most fundamental artefact used by the air traffic controllers to manage their air space.
Discussion $ 5 $ 244 $ 57 $ PATTERN $ Also Kanji/TERM is a/DEF kind of ideogram and each character has its own meaning ./O
Introduction $ 1 $ 16 $ 12 $ CRF $ The main contributions of this paper are : the development of a centroid-based multi-document summarizer , the use of cluster-based/TERM sentence/TERM utility/TERM ( CBSU/ACR ) and cross-sentence/TERM informational/TERM subsumption/TERM ( CSIS/ACR ) for evaluation of single and multi-document summaries , two user studies that support our findings , and an evaluation of MEAD . 
Abstract $ 0 $ 37 $ 1 $ PATTERN $ In terms of this distinction , it is the AbsDocRep that is specified during text planning; graphical markup can be deferred to a later formatting stage.
Generating_from_Bare_Data $ 2 $ 12 $ 0 $ PATTERN $ We start initially with a relational/TERM database/TERM , as defined/DEF by a set of tab-delimited database files , plus some minimal semantics ./O
The_Classifiers $ 3 $ 107 $ 70 $ CRF $ Given a rule of the following format , IF Xl , X2 , • • , Xm THEN Yl , Y2 , .-. , Yn where xt , ... , xm are antecedents and Yt , ... , Yn are consequences , the algorithm derives a pair of vectors a and b such that • for each index i = 1 , ... , M , 1 ifwi = xj for some j 6 { 1 , ... , m } ai = 0 otherwise ( 19 ) where wi is the i th entry in the keyword feature table ; and for each index i = 1 , ... , N , 1 ifwi = yj for some j E { 1 , ... , n } bi = 0 otherwise ( 20 ) where wi is the class label of the category i . 
Abstract $ 0 $ 41 $ 37 $ CRF $ For example , the bigram probabilities of the language model may be estimated and updated with the corrected data . 
Introduction $ 1 $ 29 $ 17 $ KP $ In GL formalism , lexical/TERM entries/TERM consist/DEF in structured sets of predicates that define a word ./O
_Introduction $ 1 $ 14 $ 8 $ CRF $ First , it is one of the first structurally annotated corpora in Mandarin Chinese . 
Building_Spoken_Dialo~te_Systems $ 4 $ 125 $ 20 $ CRF $ Each definition/TERM is a/DEF pair comprising a network name and a set of phrase category names ./O 
Grammar_Induction $ 3 $ 74 $ 13 $ PATTERN $ The learning algorithm we use is a variant of the Inside/Outside/TERM algorithm/TERM that induces/DEF grammars expressed in the Probabilistic Lexicalized Tree Insertion Grammar representation (/O Schabes and Waters , 1993; Hwa , 1998 ) .
_Otherwise ,_add_to_the_current_context_new $ 6 $ 95 $ 12 $ PATTERN $ Inference to the best explanation The assertion of the goal G supports a proposition Q which is firmly believed ( i.e. , P ( Q ) = High , where Q/TERM is a/DEF premise or inferred from premises )/O , but which would be unexplained ( improbable ) without supposing the truth of the goal.
Principles_and_Parameters $ 1 $ 21 $ 14 $ CRF $ Ambiguity/TERM is a/DEF natural enemy of efficient language acquisition ./O 
Conclusion $ 6 $ 134 $ 9 $ CRF $ Extra- position provides a useful and sometimes important means of rearranging complex material in an abstract discourse representation in order to satisfy the constraints posed by linearisation into text . 
OUT $ -1 $ 157 $ 86 $ PATTERN $ if there is an SDR [w , DT , wd or [wl , DT' , w] , then w~ and the dependency type DT or DT' , respectively , constitute a mierocontext element [DT , wd or [wl , DT'] , respectively , of the word w. The first case implies that w is a head word in the SDR and in the second case the word w is a dependant.
Abstract $ 0 $ 49 $ 22 $ CRF $ In this paper we have presented a more efficient distributed algorithm which construct a breadth-first search tree in an asynchronous communication network P/T -/Title Lst/Presents a model and gives an First we present a model and give overview of lst/Overview of related research , related research . 
Abstract $ 0 $ 18 $ 16 $ CRF $ effectiveness/TERM by this method are ambiguities/DEF caused by more than one translation of a query term and failures to translate phrases during query translation ./O 
Conclusions_and_loose_ends $ 5 $ 126 $ 2 $ PATTERN $ The numerical data that are the input to our algorithm , for example , take a very different form in the descriptions generated , and yet there is , in an interesting sense , no loss of information : a description has the same reference , whether it uses • ... : ,..exaet~.anforroataon : ( ~he : 3c~zz.mouse.
Learning_Algorithms_Tested $ 2 $ 38 $ 18 $ PATTERN $ A key point that allows a fast learning is that the winnow nodes are not connected to all features but only to those that 1The Winnow algorithm ( Littlestone , 1988 ) consists of a linear threshold algorithm with multiplicative weight updating for 2-class problems.
Empirical_Evaluation $ 4 $ 126 $ 0 $ PATTERN $ 4.3 Performance Measures Our experiments adopt the most commonly used performance measures , including the recall , precision , and F1 measures.
Introduction $ 1 $ 23 $ 12 $ KP $ Traditional IR systems treat the query/TERM as a/DEF pattern of words to be matched by documents ./O
Experiments $ 4 $ 124 $ 3 $ KP $ verbs , and overall , are the best results for each case are printed in boldface.
The_link_between_~i~l~ ( /guniang/ ,_girl ) _and $ 7 $ 107 $ 1 $ KP $ 106 between them sometimes.
Abstract $ 0 $ 4 $ 2 $ KP $ The adapted algorithm is tested on four Danish dialogues from two dialogue collections and the results obtained are evaluated.
Introduction $ 1 $ 31 $ 22 $ CRF $ 2This corpus is collected and annotated for the GNOME/TERM project/TERM ( Poesio , 2000 ) , which aims/DEF at developing general algorithms for generating nominal expressions ./O 
Findings $ 6 $ 135 $ 26 $ KP $ ~UPenn CTB at 80 ,000 tokens , 146 New token+types observed : AD~ , hD~ig , AD~ , ADrift , AD~ , ADI]~ , AD~ , CD 1 " 2 ~ , CD 1 7 1 6~ , CD2 0 0 , CD2 0 0~ , CD2 1 0 0~ , CD2 8 0 0~' , CD4 00~j' , CD 5 5 , jj~..~d~ : , JJ~.~E , JJ : : ~ ,~ , JJ~-- , JJ~J~ , LC~ , LC~ , LC~ , LC~ , M~ , M~ , NN~ , NN~ , NN~ , NN~ , NN "~ , NN~ , NN~ , NN~-~ , NN~ , NN~ , NN~ NN~J~ , NN~ , NN~]~ , NN~J NN~ , NN~ , NN~ , NNJ~ , NN~ NN~ NN~; , NN~ , NN~J~ , NN~J~ NN~ , NN~]~ NN~ , NN~ NN~ , NN~ , NN~ , NN~ NN~ , NN~ , NN~ , NN~ NN~ NN/J~ , NN]~I~ , NN~ , NN~ , NN~ , NN ~ , NN~ NN~ , NN~t , NN~ , NN~ , NN~ , NN~ , NR~ , NR~ , NRP~~ , NR~ , NR~~ , NR~ , NR~ , NR~ , NR~-~ , NR~X~ , NR~'~-~ " ~ , NR~~ , NR~ , NR~ " ~~ , NR~J~ , NR~ , NRJ~ , NR-~ , NR~ , NR~ , NR~ , NR~ , NR~ , NR~ , NTI 1 ~ , NTI 8 ~ 3~E , NTI 8~ , NT1 9 ~ 9~ E , NTI ~ 8 2~E , NTI ~ 8 6~E , NTI ~ 8 9~E , NTIS , PUt'i , P~ , e~ , VA~-~ , VV~ ,. , vvI~K vv~ , vv3~ , vv~lK vvt/~lii , vv : ~& vv-~l~ , vv-~ , vv'~'~- , vv~-.k vv : ~' , vv~ , vv~ , vv~t , vv~a , vv~J~ , vv~~l , vv~~ , vv~~J , vv~l~A~ , wE , vv~ , vv~ , vv~#L.t~ , vv~ffff~. , , vv--~'~J= , vv~~! , vv~l; Tota tags : 12 , rota new items : 146.
_Annotation_Guidelines_I : $ 3 $ 103 $ 57 $ CRF $ ( 4 ) Default Inheritance Hierarchy for Categories a ) Lexical Categories : V > N > P > Ng b ) Phrasal Categories : S > VP > NP > PP > GP When phrasal conjuncts are involved , S/TERM is the privileged category since it is the start symbol of the grammar . 
Perfect_Sampling $ 3 $ 47 $ 3 $ CRF $ In PS , we obtain a sample from the limit distribution of an ergodic Markov Chain X = { Xn ; n _ > 0 } , taking values in the state space S ( in the WSME case , the state/TERM space/TERM is the/DEF set of possible sentences )/O . 
Abstract $ 0 $ 2 $ 1 $ PATTERN $ The system consists of four components , a concept network , a query reformulation model , a standard search engine , and an automatic summarizer.
Introduction $ 1 $ 9 $ 1 $ PATTERN $ Resolving/TERM the/TERM ambiguity/TERM of/TERM words/TERM is a/DEF central problem for large scale language understanding applications and their associate tasks (/O Ide and V4ronis , 1998 ) .
Resolution_Procedures $ 2 $ 39 $ 8 $ PATTERN $ For instance , the Spanish verb comprar ( to buy ) might be associated with the ontological concept named PURCHASE/TERM which is a/DEF generic frame structure corresponding to purchasing events ./O
Introduction $ 1 $ 21 $ 15 $ KP $ The LCS/TERM represents predicate/DEF argument structure abstracted away from languagespecific properties of semantics and syntax ./O
Implementing_Embedded_MT $ 2 $ 64 $ 0 $ PATTERN $ 2.1 Language / Code Set Identification Knowing the language and encoding~ or code set , of a document is a necessary first step in utilizing on-line text.
Collaborative_Agents $ 1 $ 101 $ 97 $ PATTERN $ " "What is the eemail address?
Maximum_Entropy_Modeling $ 2 $ 61 $ 19 $ PATTERN $ In building a model , we consider the linear exponential family Q given as 1 Q ( f ) = {p ( ylx ) = ~exp ( E ~ifi ( x ,y ) ) } , ( 2 ) 165 where Ai are real-valued parameters and ZA ( x ) is a normalizing constant : = exp ( y ) ) .
Analysing_Czech_texts $ 3 $ 71 $ 0 $ PATTERN $ Linguistic analysis of an input Czech text consists of a sequence of procedures depicted in Figure 1.
Prosody_Prediction $ 4 $ 70 $ 8 $ PATTERN $ The previous mapping between the two structures defines a tree transformation.
Abstract $ 0 $ 3 $ 1 $ KP $ The system INTHELEX , used to carry out this task , requires a logic representation of sentences to run the learning algorithm.
Tree_Generalization_using_Tree-cut $ 2 $ 61 $ 15 $ PATTERN $ The MDL/TERM is a/DEF principle of data compression in Information Theory which states that , for a given dataset , the best model is the one which requires the minimum length ( often measured in bits ) to encode the model ( the model description length ) and the data ( the data description length ) ./O
Applications_of_LexTract $ 4 $ 219 $ 73 $ PATTERN $ For Srinivas' and our grammars , the first line is the results tested on Section 23 , and the second line is the one for Section 22.
Danish_Data $ 4 $ 74 $ 0 $ KP $ In this section I shortly describe Danish third person personal and possessive pronouns and demonstrative pronouns.
Construction_of_Features $ 3 $ 91 $ 23 $ CRF $ An example of a training data and a resulting lexical context is shown in Figure 3 . 
Introduction $ 1 $ 20 $ 9 $ PATTERN $ Most context-free parsing algorithms have O ( n 3 ) parsing complexities in terms of time and space , where n/TERM is the/DEF length of a sentence (/O Tomita , 1986 ) .
Abstract $ 0 $ 36 $ 34 $ PATTERN $ P ( t l ) and P/TERM ( t 2 ) are the/DEF occurrence probabilities of term t I and t 2 in a sentence ./O
OUT $ -1 $ 154 $ 103 $ KP $ Given two sequences , crossover inserts a random segment from one sequence in a random position in the other to produce two new sequences.
Why_Reading_Comprehension $ 2 $ 67 $ 13 $ PATTERN $ The compound noun maple sap is a semantically narrower term than the noun sap and encodes an implicit relation between the first element maple and the head noun sap.
The_semantic_behavior_of_the $ 5 $ 95 $ 2 $ PATTERN $ MIKETTEI NO JOUTAI ( indecision ) ( of ) ( situation ) a situation of indecision In this case , the "MIKETTEI NO ( of indecision ) " also represents the state concretely.
OUT $ -1 $ 60 $ 18 $ KP $ Class/TERM probability/TERM assignments/TERM are then estimated/DEF using statistics computed on the equivalence classes ./O
Knowledge_base $ 5 $ 103 $ 17 $ CRF $ For each phrase in the user question , the most similar phrase in the KU case part is looked for based on the following criteria : • Matching of content words : 3 points • The second or more matching of content words ( when the phrase contains two or more content words ) : 1 point 144 3+0+1+1= -5 3+0+1+1=5 The user question ( The maximum matching score : 15 ) A knowledge uait ( The maximum matching score : 20 ) The c~tainty score ( 5+5+5 : 15 x 20 x 100 = 75 ( % ) Figure 2 : Matching of the user question and a knowledge unit . 
OUT $ -1 $ 189 $ 63 $ PATTERN $ Agent is a strong feature beating both baselines.
Experimental_work $ 4 $ 71 $ 15 $ PATTERN $ The base-line model is a trigram model ( Trigram ) PS algorithm and features of n-grams and distance 2 n-grams.
Introduction $ 1 $ 14 $ 5 $ KP $ The earliest work ( Rayner , 1988; Samuelsson and Rayner , 1991 ) builds a specialized grammar by chunking together grammar rule combinations while parsing training examples.
Experiments $ 4 $ 81 $ 2 $ PATTERN $ DIMIN is a Dutch diminutive formation task derived from the Celex lexical database for Dutch ( Baayen et al. , 1993 ) .
Related_Work $ 2 $ 37 $ 10 $ CRF $ He defines also a new measure , called success/TERM rate/TERM which indicates/DEF if a question has an answer in the top ten documents returned by a retrieval system ./O 
OUT $ -1 $ 6 $ 6 $ KP $ In this way , a PAR schema for the action enter may actually translate into an animation PAR for walking into a certain area.
Abstract $ 0 $ 42 $ 41 $ PATTERN $ Section 2 describes in details the block-based dependency parsing approach.
Using_CST_for_information_fusion $ 5 $ 142 $ 5 $ CRF $ : \ Figure 6 : Processing stages G ~ / / / \ \ DOC2 _J Figure 7 : Summarization using graph cover operators The third stage is the automatic creation and typing of links among textual spans across documents . 
Evaluation_Measures $ 2 $ 98 $ 73 $ PATTERN $ Thus , recall-based measures are likely to violate both properties ( i ) and ( ii ) , discussed at the beginning of Section 2.
Psycholinguistic_production $ 3 $ 49 $ 14 $ PATTERN $ The model consists of three layers of nodes : A layer of concept nodes with labelled concept links , a layer of lemma nodes , and a layer of word form nodes that include morpho94 logical information.
Discussion_and_Conclusions $ 6 $ 115 $ 15 $ CRF $ Stolcke and Segal ( 1994 ) describe a method for combining a context-free grammar with an n-gram model generated from a small corpus of a few hundred utterances to create a more accurate n-gram model . 
ALLiS $ 3 $ 34 $ 14 $ KP $ In order to parse a text , a module automatically converts this formalism into appropriate formalisms which can be used by existing symbolic parsers.
Abstract $ 0 $ 17 $ 10 $ KP $ who developed topic?
Examples_of_YAG_in_use $ 3 $ 69 $ 4 $ PATTERN $ In this representation , M2/TERM is the/DEF proposition that the discourse entity B2 is a member of class "dog" ./O
Statistical_Semantic_Parsing $ 4 $ 99 $ 0 $ CRF $ 4.1 The Parsing ModeL A parser/TERM is a/DEF relation Parser C_ Sentences x Queries where Sentences and/O Queries/TERM are the/DEF sets of natural language sentences and database queries respectively ./O 
Discussion $ 4 $ 76 $ 3 $ CRF $ 13 Another advantage is the ease with which natural language interfaces can be constructed for new domains : Since all the task and linguistic knowledge is extracted from the grammar , 14 one need only develop a Kernel Grammar that models the domain at ( extracted from the final interpretation of ( U1 ) ) would have been learned too , but its subsumption by existing rule ( R1 ) was automatically detected . 
Introduction $ 1 $ 33 $ 26 $ KP $ Since Italian grammar is very different from the English one , some terms do not have an English equivalent and , hence , cannot be translated.
Word_Co/Occurrence_Vector $ 2 $ 48 $ 16 $ PATTERN $ In this matrix : the rows and colunms correspond to words and the ith diagonal/TERM element/TERM denotes the/DEF number of documents in which the word wl appears , F ( wi ) ./O
OUT $ -1 $ 17 $ 14 $ KP $ Starting with C , EVIUS/TERM reduces/DEF set b/Of unlearned concepts iteratively by selecting subset P C/g formed by the primitive concepts in/.4 and learning a rule set for each c E P 4/O For instance , the single colour scenario 5 in fig3With EuroWordNet ( http : //www.hum.uva.nl/-ewn/ ) synsets.
Narrative_Summarization $ 5 $ 378 $ 7 $ CRF $ 2 ) 2.2.2 text = `` Latest document summary '' audio = text = ere ate ( `` summarize -gen eric -compression .1/peru/p32 '' ) 2.3 Biographies 2.3.1 audio = `` A profile of @ 2 . 
Abstract $ 0 $ 135 $ 40 $ KP $ The accuracy of the decision trees was 80.45% for Encarta and 78.36% for the Wall Street Journal.
Introduction $ 1 $ 11 $ 7 $ KP $ Reductio ad absurdum assumes the negation of the goal , leading to an argulnent which results in a contradiction with a believed premise and requires the assertion of the Premise to goal : Corrective lenses are required.
The_Argument_Generator $ 1 $ 22 $ 2 $ PATTERN $ An evaluation , which is a number in the interval [0 ,1] where , depending on the subject , 0 means "terrible" or "much worse" and 1 means "excellent" or "much better" ) .
EXOT $ 9 $ 166 $ 94 $ PATTERN $ In order to facilitate the use of long runs as predictors , we modified the traditional measures of Boyd et al.
Qualitative_Evaluation_of_the $ 4 $ 171 $ 70 $ CRF $ We therefore divide by this score to assure that a perfect sentence gets a score of 1 . 
Middle $ 6 $ 38 $ 1 $ KP $ Of these , 5 ,922 words are polysemous/TERM , i.e. , they/DEF have more than one sense ./O
Statistical_Semantic_Parsing $ 4 $ 124 $ 25 $ PATTERN $ Suppose q = an+l ( Sm ) , we have : P ( q 6 Q ( l ) ) ( 10 ) = P ( s~ • F~ ) ... = P ( s ,n • FS + l sm-1 •/St ,_a ) ... P ( s~ • OS~_ , I sj-1 • Is~_ , ) ... P ( s2 • Ob~ , Is1 • IS~ , ) P ( 'I • IS~ , ) where ak/TERM denotes the/DEF index of which action is applied at the kth step ./O
Abstract $ 0 $ 2 $ 0 $ CRF $ This paper describes a first attempt at a statistical model for simultaneous syntactic parsing and generalized word-sense disambignation . 
_The_+END+_ ( null ) _postmodifier_con $ 8 $ 130 $ 77 $ PATTERN $ Even though the model is a top-down , generative one , parsing proceeds bottom-up.
Future_Research $ 4 $ 134 $ 3 $ CRF $ The result is a set of tasks for which the system 's output appears to be suitable . 
Abstract $ 0 $ 3 $ 2 $ PATTERN $ The integration is a multi-step unification process.
Comparison_experiment $ 3 $ 115 $ 17 $ PATTERN $ ) Chi ~ il regista di "I Mostri" ( Who is the director of "I Mostri' ) Quale attore ha recitato con Benigni nel film "I1 piccolo Diavolo"?
Introduction $ 1 $ 11 $ 4 $ PATTERN $ While extrinsic evaluation measures such as these are often very Concrete , the act of designing the task and scoring the results of the task introduces bias and subject-based variability.
Interclausal_Coherence $ 4 $ 103 $ 13 $ PATTERN $ The cue "however" alone does not give enough information to decide whether Sentence ( 3-c ) should connect to ( 3-b ) or to ( 3-a ) : further information is needed , like that there is a referential relation between the old MessagePad 120 and the MessagePad family.
MALIN $ 4 $ 209 $ 103 $ CRF $ 127 Status : Stops : Su~es8 Name : Id : Name : Id : Name : Id : Cen~.~rum ] `` Snickareg . 
_Pronominalise_the_Cb_only_after_a_Continue $ 4 $ 90 $ 51 $ CRF $ concession approve ( fda , elixir-plus ) cause NUCL~ S~LITE ban ( fda , elixir ) contain ( elixir , gestodene ) Figure 2 : Rhetorical structure The text planner has been developed within ICONOCLAST/TERM , a/DEF project which investigates applications of constraint-based reasoning in Natural Language Generation using as subjectmatter the domain of medical information leaflets ./O 
Results $ 3 $ 87 $ 33 $ PATTERN $ after applying corrective measures to base chunker combination.
OUT $ -1 $ 0 $ 0 $ KP $ Task-based dialog management using an agenda Wei Xu and Alexander I. Rudnicky School of Computer Science Carnegie Mellon University 5000 Forbes Ave Pittsburgh , PA 15213 {xw , air] @cs.
Content_Planning $ 1 $ 48 $ 23 $ CRF $ The model/TERM is the/DEF probability distribution P ( nk ) = P ( nklck ) ,/O where nk/TERM is the/DEF number of attributes and/O Ck/TERM is the/DEF utterance class for system utte~anee k ./O 1.2.2 The bigram model of the attributes This model will predict which attributes to use in a system utterance . 
Architecture $ 3 $ 55 $ 4 $ PATTERN $ The RAGS/TERM architecture/TERM ( Cahill et al. , 1999 ) is a/DEF reference architecture for natural language generation systems ./O
Introduction $ 1 $ 9 $ 1 $ CRF $ While speed is one factor associated with the construction of such a device , another factor is the language type and format . 
_Amanda_Huggenkiss ,_she_proposes_to_meet_you $ 2 $ 12 $ 5 $ CRF $ Our practical goal is to enhance the effectiveness of a wearable device that provides spoken advice to a user operating in a real-world physical environment . 
OUT $ -1 $ 169 $ 118 $ KP $ This sequence is then evaluated and ordered in the populat ion.
Results_and_discussion $ 4 $ 184 $ 5 $ CRF $ QS1/TERM is the/DEF subset of questions whose number of morphological derivations and synonyms is higher than three ;/O QS2/TERM is the/DEF subset whose number of lexical expansions is equal to two or three ;/O QS3/TERM is the/DEF subset whose number of lexical expansions is lower than two ./O 
Generation_of_Vague_Descriptions $ 4 $ 108 $ 27 $ PATTERN $ The result is a list that may be written as L = {chihuahua , largesh } , which can be employed to generate the description 'the largest chihuahua'.
_User : _Ok $ 9 $ 211 $ 25 $ CRF $ Furthermore , the system allows automated sub-goal utility adjusmaent based on history of interactions with groups of users . 
Experiments $ 6 $ 114 $ 19 $ PATTERN $ Both for execution time and space considerations for the learner and for fear of overtraining , we put a bound on the length of the RRE that could be learned , s We define an atomic/TERM RRE/TERM as any/DEF RRE derived without any concatenation operations ./O
Discussion $ 8 $ 175 $ 5 $ PATTERN $ The argument here is simply that these two closure measures are used to spot when a sublanguage corpus approaches closure--that is , when the curve of new types• and new combinations of type with token begins to flatten at a rate below ten percent.
Reading_Comprehension_Tests $ 2 $ 26 $ 8 $ PATTERN $ 3 A Rule-based System for Question Answering Quarc/TERM (/TERM QUestion/TERM Answering/TERM for/TERM Reading/TERM Comprehension/TERM )/TERM is a/DEF rule-based system that uses lexical and semantic heuristics to look for evidence that a sentence contains the answer to a question ./O
_The_Problem $ 1 $ 38 $ 32 $ PATTERN $ the word capitalize in the sense used here 3 denotes a function of two arguments , where J is restricted to ( can only be bound to ) objects of type joint venture and to objects of type amount of money.
Highlight $ 3 $ 74 $ 23 $ KP $ Deriving semantic relationships from this is not particularly reliable , so it is preferable to use search terms which rely more on positional clues.
Abstract $ 0 $ 2 $ 1 $ KP $ The MT Proficiency Scale project has developed a means of baselining the inherent "tolerance" that a text-handling task has for raw MT output , and thus how good the output must be in order to be of use to that task.
OUT $ -1 $ 146 $ 139 $ KP $ l ) 3 Amanda Huggenkiss.
Middle $ 6 $ 129 $ 92 $ PATTERN $ ( 1 ) If the English translation corresponds to only one symet , this symet is the solution.
Chunk_Types $ 3 $ 28 $ 11 $ KP $ " ( VP loves ( NP Mary ) ) " above , or ADJPs and PPs below.
OUT $ -1 $ 151 $ 133 $ CRF $ We also found that that the words `` this '' and `` story '' were strong indicators that the dateline is the best answer ( rules # 3 and # 4 ) . 
Task_Structure_and_Scripts $ 3 $ 94 $ 49 $ CRF $ AI : U2 : A3 : U4 : A5 : A6 : U7 : A8 : U9 : A10 : Ul1 : A12 : U13 : A14 : A15 : U16 : A17 : U18 : AI9 : Hello . 
Dialog_Management $ 3 $ 61 $ 0 $ KP $ The Dialog/TERM Manager/TERM can be broadly classified into two main modules : Content/DEF Selection and Content Realization ./O
System_Overview $ 3 $ 101 $ 65 $ PATTERN $ Furthermore , Eset/TERM is the/DEF only tree set that satisfies all the following conditions : ( C1 ) Decomposition : The tree set is a decomposition of T* , that is , T* would be generated if the trees in the set were combined via the substitution and adjunction operations ./O
The_Verbmobil_treebanks $ 2 $ 37 $ 3 $ PATTERN $ The corpus consists of spoken texts restricted to the domain of arrangement of business appointments.
Related_Work $ 6 $ 174 $ 14 $ CRF $ REA/TERM has a/DEF working implementation , which includes the modules described in this paper , and can engage in a variety of interactions including that in ( 5 ) ./O 
Identifying_'Words' $ 3 $ 51 $ 12 $ PATTERN $ Using statistical measures of significance , it was found that most groups fell well within 5only two individual languages were near exceeding these limits of the proposed Human language word-length profile ( E1liott et al. , 2000 ) .
Maximum_Entropy_Modeling $ 2 $ 52 $ 10 $ CRF $ We build a probability distribution p ( ylx ) , where y/TERM • { 0 , 1 } is a/DEF random variable specifying the potential segmentation position in a context x ./O 
HMM_for_Mono-Lingual_Retrieval $ 2 $ 26 $ 8 $ CRF $ number of occurrences of W in C x • e0e IGx ) = length/TERM of/TERM Cx/TERM which is the/DEF general language probability for word W in language x ./O number of occurrences of W in D • e ( WlD ) = length of D In principle , any large corpus Cx that is representative of language x can be used in computing the general language probabilities . 
Definitions $ 2 $ 35 $ 10 $ PATTERN $ IqJ]l/TERM is the/DEF number of symbols in the jt~ string of the corpus ./O
Implementing_Embedded_MT $ 2 $ 104 $ 0 $ CRF $ 2.4 Lexicon Update The highest portion of the cost of providing a machine translation capability reflects the amount of lexicography that must be done as much as 70 % of the cost of a machine translation engine . 
Shortcomings $ 5 $ 149 $ 10 $ CRF $ frequency/TERM of/TERM answers/TERM : The/DEF frequency of occurrence of facts in a collection of documents has an impact on the performance of systems ./O 
Comparison_experiment $ 3 $ 166 $ 68 $ PATTERN $ That is , if the question is "Who is the author of Options?
The_Classifiers $ 3 $ 91 $ 54 $ PATTERN $ Category choice : Given the F~ and F1 b input vectors A and B , for each F2 node j , the choice function Tj is defined by IA Aw~l IB A w~l = ~a~ + Iw~'l + ( 1 --~ ) ~b + Iw~l' ( S ) where the fuzzy/TERM AND/TERM operation/TERM A is defined by (/DEF p A q ) i --~ min ( pi , qi ) , ( 9 ) and where the norm I-I is defined by IPl -= ~Pi ( 10 ) i for vectors p and q ./O
Abstract $ 0 $ 3 $ 2 $ CRF $ The structure reflects the choices made by the author during the top-down stepwise refinement of the document under control of a DTD grammar . 
_NTCIR_Data_Analysis $ 2 $ 112 $ 42 $ CRF $ This explains poor performance of precoordinated longer phrase based indexing that utilizes phrases as replacements of single words . 
Conclusion $ 4 $ 114 $ 9 $ CRF $ In these ways , YAG/TERM provides/DEF the speed , robustness , flexibility , and maintainability needed by real-time natural language dialog systems ./O 
Support_Vector_Machines $ 2 $ 6 $ 0 $ KP $ Support/TERM Vector/TERM Machines/TERM (/TERM SVMs/TERM )/TERM , first introduced by Vapnik ( Cortes and Vapnik , 1995; Vapnik , 1995 ) , are relatively/DEF new learning approaches for solving two-class pattern recognition problems ./O
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 38 $ 3 $ PATTERN $ Figure 1 : The document titled 'Two Americans l~lown dead in Japan quake' Figure I is the document whose topic is 'Kobe Japan quake' , and the subject of the document ( event 31 words ) is 'Two Americans known dead in Japan quake'.
Experimental_Results $ 7 $ 217 $ 50 $ CRF $ When/DEF the key words for main topics contained at least one of the identification words ,/O we viewed that text as having the corresponding/TERM main/TERM topic/TERM . 
_Introduction $ 1 $ 16 $ 10 $ KP $ annotations which capture information in the raw data at several different conceptual levels or mark up phenomena which refer to more than one level.
Chunking_with_the_Phrase_Rule $ 2 $ 30 $ 7 $ CRF $ The parser follows a sequence of rules in order to build phrases out of parse islands . 
Comparing_the_five_approaches $ 4 $ 77 $ 7 $ PATTERN $ Restricting to LB results , it can be observed that the accuracy obtained in A-B is 47.1% , while the accuracy in B-B ( which can be considered an upper bound for LB in B corpus ) is 59.0% , that is , that there is a difference of 12 points.
OUT $ -1 $ 70 $ 50 $ KP $ Character encoding schemes of CJK languages have several variations ( e.g. , Chinese : GB and BIG-5 , etc.
Error-driven_Learning $ 4 $ 120 $ 3 $ PATTERN $ the lexicon : F~ ( e i ) = F : rr°r ( e i ) o+Ao Here , F ,~ r~°r ( el ) is the chunking error number of the lexical entry e i for the old lexicon r~ Error / x and r~ ,+~ te i ) is the chunking error number of the lexical entry e i for the new lexicon + AO where e~ e A~ ( A~ is the list of new lexical entries added to the old lexicon ~ ) .
Implementation $ 6 $ 137 $ 7 $ PATTERN $ The evaluation function of terminals is actually a constant function whose return value is the value to which the terminal has been bound.
The_Learning_System $ 2 $ 40 $ 14 $ PATTERN $ The categories and rules in the grammar are defined as types in the hierarchy , represented in terms of TDFSS and the feature structures associated with any given category or rule are defined by the inheritance chain.
First_order_weight_determination $ 3 $ 38 $ 5 $ PATTERN $ A measure related to this is Information/TERM Gain/TERM , which represents/DEF the difference between the entropy of the choice with and without knowledge of the presence of a feature (/O cf.
The_results_reported_in_this_paper_are_part_of_a_more $ 1 $ 53 $ 42 $ PATTERN $ ( Which is the longest world river?
Conclusions_and_Future_Work $ 5 $ 144 $ 10 $ CRF $ To extract the corresponding syntax information of English Chinese bilingual corpus by shallow parsing is a direction for future work , also . 
Introduction $ 1 $ 85 $ 76 $ PATTERN $ In the case of a tie , the lower sense nmnber from WordNet is used , since this denotes a more general concept.
Evaluation $ 7 $ 202 $ 18 $ PATTERN $ For example , a user sometimes asks "what is the difference between A and B" , or when the system asks "select from A and B" , a user answers "I don't know".
_The_+END+_ ( null ) _postmodifier_con $ 8 $ 55 $ 2 $ PATTERN $ ( Note that many words effectively get generated high up in the tree; in this example sentence , the last words to get generated are the two the's ) More formally , the lexicalized PCFG that sits behind the parsing model has rules of the form Figure 1.
Conclusions $ 4 $ 106 $ 2 $ CRF $ The tools suggest a group of key items by decreasing order of significance which distinguish one corpus from another . 
Verb_Frequency $ 2 $ 46 $ 1 $ KP $ We would expect factors such as corpus genre ( Business for WSJ vs. mixed for BNC and Brown ) , American vs. British English , and the era the corpus sample was taken in to influence word frequency.
Abstract $ 0 $ 2 $ 1 $ KP $ The English WoRDNET and its aligned Italian version , MULTIWORDNET , both augmented with domain labels , are used as the main information repositories.
Reinterpretation_of_CGS_in_RAGS $ 5 $ 109 $ 9 $ CRF $ Text Planner The input to the Longbow text planner discussed in section 4 above is a representation of a picture in SAGE/TERM format/TERM ( which has been annotated/DEF to indicate the types of complexity of each grapheme )/O together with a goal , which can typically be interpreted as `` describe '' . 
Levels_of_Annotation $ 4 $ 104 $ 10 $ CRF $ Complex schemes like ToBI could be used in this way , or simpler schemes providing labels to distinguish types of accents , associated with words , and types of intonation boundaries . 
Unfolding_and_Specialization $ 2 $ 27 $ 0 $ PATTERN $ The initial grammar is the grammar underlying the subset of correct parses in the training set.
Multiple_heuristics_for_word_sense $ 2 $ 38 $ 13 $ CRF $ sim ( s , s2 ) computes the conceptual similarity between concepts s~ and sz as in the following formula : sim ( sl , s2 ) = 2 x level ( MSCA ( sl , s : ) ) level ( sO + level ( s2 ) where MSCA/TERM ( sl , s2 ) represents the/DEF most specific common ancestor of concepts s~ and s2 and/O level/TERM ( s ) refers to the/DEF depth of concept s from the root node in the WordNetL 2.2/O Heuristic 2 : Prior Probability This heuristic provides prior probability to each sense of a single translation as score . 
Introduction $ 1 $ 14 $ 7 $ CRF $ We describe the unified learning framework and show that , in addition to explaining the success and robustness of the statistics based methods , it also applies to other machine learning methods , such as rule based and memory based methods . 
OUT $ -1 $ 202 $ 131 $ PATTERN $ The first is the database of dependency microcontexts extracted from a large text corpus.
Abstract $ 0 $ 16 $ 15 $ PATTERN $ NACSIS/TERM test/TERM collection/TERM I/TERM ( NTCIR , 1999 ) , which consists/DEF of a collection of abstracts of scientific papers ( 330 ,000 records , 590MB in text ) , two sets of topic description ( 30 topics for training and 53 topics for evaluation ) and relevance judgement , provides us of a good opportunity for this purpose ./O
OUT $ -1 $ 183 $ 0 $ PATTERN $ 4.4 Perplexity and Cross Entropy Cross/TERM entropy/TERM is a/DEF goodness measure for probability estimates that takes into account the accuracy of the estimates as well as the classification accuracy of the system ./O
Architecture_components $ 2 $ 13 $ 4 $ KP $ Tjong Kim Sang ( 2000 ) ) , I use a smaller window , four left and two right , but add the IOB suggestions made by the first level for one token left and right ( but not the focus ) .
OUT $ -1 $ 123 $ 118 $ CRF $ To compute these feature values for a sentence , we used the Remedia/TERM corpus/TERM provided by MITRE which has been hand-tagged/DEF with named entities ./O 
Discussion $ 5 $ 192 $ 16 $ CRF $ Other realization and sentence planning tin { ks-which are needed for most applications and which may profit from a stochastic model include lexical choice , introduction of function words and punctuation , and generation of morphology . 
Introduction $ 1 $ 21 $ 14 $ CRF $ `` Section 2 discusses the properties of numerical evaluation measures , points out several drawbacks associated with intrinsic measures and introduces new measures developed by the authors . 
OUT $ -1 $ 0 $ 0 $ KP $ Demonstration of ILEX 3.0 Michael O'Donnellt ( micko@dai.ed.ac.uk ) , Alistair Knott : ~ ( alik@hermes.otago.ac.nz ) , Jon Oberlandert ( jon@cogsci.ed.ac.uk ) , Chris Mellisht ( chrism@dai.ed.ac.uk ) t Division of Informatics , University of Edinburgh.
_Otherwise ,_add_to_the_current_context_new $ 6 $ 210 $ 127 $ PATTERN $ At first glance it appears that premise to goal is the preferred argmnentation strategy.
Resolution_Procedures $ 2 $ 71 $ 40 $ PATTERN $ If it is a definite noun phrase , then the head noun string is matched against that of previous NPs.
Definitions $ 2 $ 33 $ 8 $ KP $ We will notate the jth String of a corpus C as C[j].
Comparison_experiment $ 3 $ 110 $ 12 $ PATTERN $ ( Who is the author of the book '7 Malavoglia"?
Results_and_discussion $ 4 $ 221 $ 42 $ KP $ KIS KCS % context retrieval increasing with respect to KAS f. ( l , os.
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 224 $ 189 $ CRF $ 'F/A ' shows false alarm rate and 'FI/TERM ' is a/DEF measure that balances recall and precision ./O 
_The_+END+_ ( null ) _postmodifier_con $ 8 $ 77 $ 24 $ PATTERN $ Lexical Model The desired output structure of our combined parser/word-sense disambiguator is a standard , Treebank-style parse tree , where the words not only have parts ef speech , but also WordNet synsets.
Experimental_Results $ 5 $ 93 $ 19 $ PATTERN $ Recall/TERM is the/DEV same as coverage : the ratio between the number of correct parses produced by the specialized grammar and the total number of correct parses ( equalling the total number of sentences in the test set ) ./O
Dialogue_Management $ 2 $ 44 $ 0 $ KP $ 2.1 Reasoning Model A dialogue participant chooses his/her responses to the parter's communicative acts as a result of certain reasoning process.
Query_Expansion $ 4 $ 143 $ 21 $ PATTERN $ We normally expect to see slopes of .7 or more when t.f > 3 , but in this case ( b ( 3 , D , 0 ) = 0.11 ) , there is a considerable shrinking because we very much expected to see the term in the expansion and we didn't.
Abstract $ 0 $ 48 $ 46 $ KP $ The Chinese query is translated into English via looking up the English senses of Chinese query term and words in its associated word list in a Chinese-English dictionary.
Topic_Analysis $ 4 $ 67 $ 0 $ PATTERN $ 4.1 Input and Output In topic analysis , we use STM to parse a given text and output a topic structure which consists of segmented blocks and their topics.
CoreLex-II $ 3 $ 41 $ 5 $ CRF $ In response to this , a new approach was formulated and implemented that addresses both these points . 
Evaluation_Measures $ 2 $ 32 $ 7 $ PATTERN $ To compare two evaluation measures , whose scores may have very different ranges and distributions , one must compare the order in which the measures rank various summaries of a document.
Tagged_Text $ 3 $ 120 $ 106 $ PATTERN $ If a number is a four-digit number starting with 16 to 19 or is followed by A.D or B.C.
Implementation $ 4 $ 180 $ 9 $ CRF $ Document # 3 : Mike/TERM Smith/TERM is a/DEF programmer for XYZ Corporation ./O 
The_NJFun_System $ 2 $ 100 $ 86 $ PATTERN $ After the user's response , the next state represents that N JFun has now greeted the user and obtained the activity value with high confidence , by using a nonrestrictive grammar.
Treebank_Representation $ 4 $ 54 $ 0 $ CRF $ A folded/TERM treebank/TERM is a/DEF representation of a set of parse trees which allows an immediate assessment of the effects of inhibiting specific rule combinations ./O 
_Proposed_method $ 2 $ 41 $ 11 $ PATTERN $ The required linguistic/TERM knowledge/TERM resource/TERM is a/DEF lexical ontology that has the words in the target language and a listing of their associated senses ./O
Introduction $ 1 $ 75 $ 66 $ PATTERN $ For this analysis , we define constants from WordNet 1.6 as denoted in Table 2.
Introduction $ 1 $ 35 $ 22 $ PATTERN $ 2 An Overview of YAG YAG/TERM (/TERM Yet/TERM Another/TERM Generator/TERM )/TERM ( Channarukul , 1999; McRoy et al. , 1999 ) is a/DEF template-based textrealization system that generates text in real-time ./O
Task-inherent_and_Technical $ 2 $ 40 $ 11 $ PATTERN $ For example , there is a separate module on the level of speech recognition which deals with hesitations and self-corrections.
Introduction $ 1 $ 7 $ 2 $ PATTERN $ The field of information/TERM retrieval/TERM ( IR/ACR ) is the traditional discipline that addresses this problem.
Introduction $ 1 $ 32 $ 19 $ CRF $ Attributes in their system assist the realization by propagating information down a tree that specifies the complete syntactic structure of the output text . 
HMM-based_Chunk_Tagger $ 1 $ 62 $ 29 $ PATTERN $ The basic idea of representing the structural tags is similar to Skut and Brants ( 1998 ) and the structural/TERM tag/TERM consists/DEF of three parts : 1 ) Structural relation ./O
Proposed_Architecture $ 1 $ 42 $ 20 $ PATTERN $ Because this is a specific lessons learned about the CyberTrans experience , it is beyond the scope of this paper to compare this architecture with other architectures.
The_hyperonym_problem $ 2 $ 19 $ 1 $ PATTERN $ Roelofs [1996 , p. 308] describes a 'lemma'/TERM as a/DEF representation of the meaning and the syntactic properties of a word ,/O and the task of lemma/TERM retrieval/TERM as a/DEF crucial step in the process of grammatical encoding ,/O where buildsituations of utterance.
Related_Research_formalism_ ( BNs_were_chosen_because_of_their_abil $ 2 $ 42 $ 1 $ PATTERN $ An argument is represented as an cluding a discussion of counterfactual reasoning and Argument/TERM Graph/TERM , which is a/DEF network of nodes that modality ,/O may be found in ( Rescher , 1964 ) .
The_results_reported_in_this_paper_are_part_of_a_more $ 1 $ 42 $ 31 $ PATTERN $ Both of them try to expand a " basic-keyword/TERM " , that is a/DEF keyword direcdy derived from a natural language question ./O
Architecture_of_WIT-Based_Spoken $ 3 $ 82 $ 48 $ CRF $ In the phases in which the system has the initiative , only the initial function and the language model are assigned . 
_Sense_Co/Occurrence_Frequency $ 3 $ 65 $ 38 $ CRF $ score ( S , C ) ( 1 ) = score ( SS , C ' ) score ( SS , GlobalSS ) Where S/TERM is a/DEF sense item of polysemouse word W ,/O C/TERM is the/DEF context containing W , SS/TERM is the corresponding sememe set of S ,/O C/TERM '/TERM is the/DEF set of sememe expansion of words in C and/O GlobalSS/TERM is the/DEF sememe set that containing all of the sememe defined in Hownet ./O 
Analysis_module $ 2 $ 46 $ 6 $ PATTERN $ A grammar defined by means of the grammatical formalism SUG/TERM ( Slot/DEF Unification Grammar )/O is used as input of SUPAR.
Comparative_analysis_~of_Japanese_and $ 3 $ 90 $ 2 $ CRF $ Chinese/TERM is a/DEF non-inflectional language and/O therefore morphological analysis is not essential . 
Limitations $ 5 $ 113 $ 0 $ PATTERN $ GoDiS/TERM is a/DEF small-scale prototype and/O as such it suffers from the familiar drawbacks of many experimental systems : its lexicons and databases are very small , and the domain knowledge is limited.
Explaining_Probabilistic_Methods $ 3 $ 62 $ 14 $ CRF $ ) A statistical/TERM queries/TERM algorithm/TERM is a/DEF learning algorithm that constructs its hypothesis only using information received from an SQ oracle ./O 
Experiments $ 4 $ 85 $ 9 $ KP $ The features themselves were culled using this schema on 2290 sentences from the training data.
Using_linguistic_constraints $ 3 $ 69 $ 9 $ PATTERN $ LHS # RHS filters out rules with a single daughter which is the same category as the mother.
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 160 $ 125 $ CRF $ 'Rec'/TERM (/TERM Recall/TERM )/TERM is the/DEF immber of correct events divided by the total mnnber of events which are selected by a human ,/O and 'Prec'/TERM (/TERM Precision/TERM )/TERM stands for the/DEF number of correctevents divided by the number of events which are selected by our method ./O 
Related_Work $ 2 $ 47 $ 20 $ PATTERN $ The basic semantic relation used in their systems is the synonymy relation.
_Features $ 1 $ 74 $ 3 $ PATTERN $ The recall/TERM is the/DEF number of errors identified by a particular feature divided by the total number of errors ./O
OUT $ -1 $ 200 $ 129 $ CRF $ This is a way to decrease or eliminate the negative influence of synonyms in relevant documents . 
Towards_building_a_parallel_corpus $ 2 $ 53 $ 21 $ PATTERN $ Each node is characterized by a status ( NUCLEUS or SATELLITE ) and a rhetorical/TERM relation/TERM , which is a/DEF relation that holds between two non/Overlapping text spans ./O
Conclusions_and_Future_Work $ 5 $ 262 $ 6 $ PATTERN $ Moreover , the content-based measures which rely on a ground truth are only slightly more correlated to each other than theyare to the measures which perform summary-document comparisons.
Interaction_Grammars $ 3 $ 127 $ 74 $ KP $ ger : : country --> "Germany".
Abstract $ 0 $ 15 $ 5 $ PATTERN $ Within each of these types , there are a number of conceptual/TERM primitives/TERM of that type , which are the/DEF basic building blocks of LCS structures ./O
EXOT $ 9 $ 130 $ 58 $ PATTERN $ ( H3 ) There is a positive correlation between the sum of long run SEMCAT weights and the semantic coherence of a paragraph , the total paragraph SEMCAT weight.
Introduction $ 1 $ 30 $ 19 $ PATTERN $ Ambiguity/TERM and synonymity/TERM of words is a/DEF property of natural language causing a very serious problem in IR ./O
Introduction $ 1 $ 20 $ 15 $ KP $ Each story has an average of 20 sentences , and the question/TERM answering/TERM task/TERM as formulated for a computer program is to/DEF select a sentence in the story that answers to a question ./O
Dialogue_manager $ 6 $ 159 $ 51 $ CRF $ To decide this , the dialogue manager regard s the certainty/TERM score/TERM between the utterance and the most similar KU as an/DEF appropriateness measure of the interpretation ./O 
OUT $ -1 $ 177 $ 135 $ CRF $ The resulting curve is a measure of the correlation between the true distribution probability and the probability of the most likely chunk tag , i.e . 
Results $ 8 $ 191 $ 30 $ PATTERN $ While these numbers are small , this preliminary data seems to suggest again that atelicity/TERM is a/DEF good cue for cotemporality ,/O while telicity is not a sufficient cue.
Description_of_the_algorithm $ 2 $ 15 $ 1 $ KP $ First step is to make automatically labeling and to chunk each sentence from spoken dialog corpus into a set of short subphrases.
Abstract $ 0 $ 154 $ 7 $ KP $ If a new unit contains no referential expression then the algorithm makes no prediction.
Related_work $ 6 $ 191 $ 42 $ KP $ ful parse , the set of candidate rules used in that parse constitutes a model.
Motivation $ 2 $ 44 $ 8 $ CRF $ The START/TERM System/TERM ( Katz , 1990 ; Katz , 1997 ) analyzes/DEF English text and builds a knowledge base from information found in the text ./O 
_The_identity_of_the_speaker ,_denoted_as_the $ 6 $ 116 $ 6 $ CRF $ Corpus/TERM H/TERM is a/DEF subset of Corpus I ./O 
Implementing_Embedded_MT $ 2 $ 81 $ 24 $ CRF $ MT systems were examined in light of the outputs of translation and the types of errors that can be generated by the translation engine . 
System_Overview $ 3 $ 132 $ 96 $ KP $ 57 etrees.
The_interplay_of_focus_and_word $ 2 $ 69 $ 45 $ CRF $ `` In our approach we derive the information necessary for the use of LP-rules from a discourse/TERM model/TERM that relates/DEF various aspects of a discourse to one another ./O 
Complexity_Formulas $ 4 $ 70 $ 14 $ PATTERN $ Since objects can be grouped together into classes , a class/TERM complexity/TERM is the/DEF number of bits conveyed by distinguishing one type of object from that class , plus the maximum object complexity that occurs in that class :/O CC. ,...
Types_of_user_utterances $ 3 $ 43 $ 0 $ KP $ We observed the conversations of users and TAs in the CIMS computer rooms by recording and transcription ( 20 hours observation; 1.5 hours recording ) .
Abstract $ 0 $ 3 $ 2 $ KP $ The design of ADAM architecture is compatible with as many practices of dialogue annotation as possible , as well as approaches to annotation at different levels.
OUT $ -1 $ 76 $ 30 $ PATTERN $ From these basic count measures , we define four derived percentage measures in section 5 and summarize these cases across our three systems in figure 3 of that section.
Conclusion $ 4 $ 86 $ 4 $ CRF $ This work represents a novel approach to translation modeling which is most appropriate for applications like TransType/TERM which need/DEF to make rapid predictions of upcoming text ./O 
Learning_Algorithms_Tested $ 2 $ 72 $ 31 $ CRF $ LazyBoosting/TERM ( Escudero et al. , 2000a ) , is a/DEF simple modification of the AdaBoost.MH algorithm , which consists of reducing the feature space that is explored when learning each weak classifier ./O 
_System_Configuration $ 2 $ 94 $ 70 $ CRF $ The document entitled J.~ZJ~ll/kJl~ ! ~3-~qF0~ ~1~I~~ ( Reorganization of Legend Corporation to welcome the joining of WTO ) receives the relevance rate of 100 % in both searches . 
Robust_Name_Finding $ 3 $ 62 $ 3 $ PATTERN $ A common definition of the extended name finding task , known as the "named entity" task , also includes numeric phrases , such as dates , times , monetary amounts , and percents , which are often the answers to other common questions When?
Tagging_Verb_Groups $ 6 $ 155 $ 8 $ PATTERN $ The value of attribute tag is computed automatically from the verb rule that describes the compound verb group.
Cross-corpora_experiments : $ 6 $ 138 $ 3 $ CRF $ For instance , Table 7 shows a drop in .16 in precision for local content collocations when compared to Table 4 . 
Background : _The_STOP_System $ 2 $ 48 $ 31 $ PATTERN $ For people who answered Not Sure or Yes , we looked at their decisional/TERM balance/TERM , that is the/DEF number of likes and dislikes they had about smoking ,/O and placed them into Lacks Confidence if their dislikes clearly outnumbered their likes , and Classic Precontemplator otherwise.
_Pronominalise_the_Cb_only_after_a_Continue $ 4 $ 46 $ 7 $ CRF $ t~rarisi~ ' : '' : : ' : the : sHng'~ : is : p redicted : Am : its : A°cal ' : c°ntext~ but tions between consecutive sentences in a disthe RETAIN is not ; whenever RETAIN is a cheap course segment and favouring sequences which maintain the same center . 
Stochastic_Surface_Realization $ 2 $ 82 $ 24 $ CRF $ ( n.1 ) , U ) where u/TERM is the/DEF utterance class ./O 
The_TABULATE_ILP_Method $ 3 $ 91 $ 25 $ CRF $ We measure accuracy using the m-estimate/TERM ( Cestnik , 1990 ) , a/DEF smoothed measure of accuracy on the training data which in the case of a two-class problem is defined as : accuracy ( H ) s + m. p+ = ( 1 ) n , -Irrt where/O s/TERM is the/DEF n-tuber of positive examples covered by the hypothesis H ,/O n/TERM is the/DEF total number of examples covered ,/O p+/TERM is the/DEF prior probability of the class (/O 9 , and m/TERM is a/DEF smoothing parameter ./O 
Conclusions_and_loose_ends $ 5 $ 192 $ 68 $ CRF $ The experiment suggests that the converse of the hypothesis might also be true , in which it is claimed that expressions of the form 'the n large CN ' can not be employed to refer to the set S unless S consists of the n largest objects in D : Hypothesis ( .¢= ) : In a situation in which the domain D represents the set of percept_.. : ... ... ... . ~ ~.t.ually : relevmtt , ob_jects > a~ : -expressionof t~he . 
The_following_formulas_summarize_the_relations $ 9 $ 157 $ 76 $ PATTERN $ Here we see evidence for two patterns that recur in the two measures below.
OUT $ -1 $ 0 $ 0 $ KP $ Sentence generation and neural networks 2 .
_Proposed_method $ 2 $ 46 $ 16 $ KP $ In WordNet 1.6 , bank has 10 senses , the 3 topmost frequent senses are : I. a financial institution that accepts deposits and channels the money into lending activities 2. sloping land ( especially the slope beside a body of water ) 3. a supply or stock held in reserve especially for future use ( especially in emergencies ) shore has two senses listed : 1. the land along the edge of a body of water ( a lake or ocean or r/vet ) 2. a beam that is propped against a structure to provide support One would expect that the distance between sense #2 of bank and sense #1 of shore to be smaller than the latter's distance from the other two senses of bank.
Introduction $ 1 $ 11 $ 4 $ CRF $ In such cases , the synonymy relations that hold in the specific domain are only a restricted portion of the synonymy relations holding for a given language at large . 
The_Learning_System $ 2 $ 31 $ 5 $ CRF $ We concentrate on the description of word/TERM order/TERM parameters/TERM , which reject/DEF the basic order in which constituents occur in different languages ./O 
Tree_Structures $ 2 $ 26 $ 10 $ KP $ 87 S Hennessy [NNP] a [D~jj] act ~ I vP to [TO~ow [VB] Figure 1 : Syntactic structure for the sentence "Hennessy will be a hard act to follow".
Only_the_relevant_attributes_of_each_semantic_role $ 5 $ 145 $ 54 $ PATTERN $ Moreover , there is a pronoun , lo ( him ) that functions as complement of the verb vi ( saw ) .
Maximum_Entropy_Modeling $ 2 $ 53 $ 11 $ CRF $ A feature/TERM of/TERM a/TERM context/TERM is a/DEF binary-valued indicator function ] expressing the information about a specific context ./O 
Shallow_Parsing $ 4 $ 77 $ 7 $ KP $ The SNoW/TERM learning/TERM architecture/TERM learns/DEF a sparse network of linear functions , in which the targets ( states , in this case ) are represented as linear functions over a common feature space ./O
OUT $ -1 $ 183 $ 129 $ CRF $ The ideal/TERM answer/TERM is a/DEF full sentence that contains the information given by the question and the information requested ./O 
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 158 $ 123 $ PATTERN $ In Table 1 , 'Event type' illustrates the target events defined by the TDT Pilot Study.
The_lexicon_size_of_a_typical_large-vocabulary $ 9 $ 149 $ 70 $ PATTERN $ We thus plan to report ranked retrieval measures of effectiveness such as average precision in addition to the detection/TERM statistics/TERM ( miss/DEF and false alarm )/O typically reported in TDT.
Background $ 2 $ 52 $ 0 $ KP $ 2.3 Dialogue Primitives Following the procedure outlined in Section 2.4 , the dialogue/TERM manager/TERM calculates/DEF a bag of primitives for each turn and speaker ./O
Middle $ 6 $ 78 $ 41 $ CRF $ It shows that semantic tagging is a challengeable problem in Mandarin Chinese . 
Implementation $ 4 $ 274 $ 103 $ PATTERN $ The structure of the tree represents the rhetorical structure of the briefing.
Speech_Sound_and_Transcription $ 2 $ 18 $ 0 $ PATTERN $ The corpus consists of a collection of 14 taskoriented dialogues , each performed by two native speakers of Japanese.
_Phrasal_Indexing $ 1 $ 25 $ 0 $ KP $ For the baseline run experiments , we utilized the engine of Coneeptbase/TERM Search/TERM 1.2/TERM , a/DEF commercial based search engine adopting vector space model approach ./O
OUT $ -1 $ 108 $ 57 $ CRF $ Bad embeddings are all those left , for example , if there is no available syntactic slot for the embedded part . 
ADVF_Far_adverbs_ ( ~'l $ 7 $ 92 $ 7 $ PATTERN $ In } ( ~ ) D : a set of 3-tuple in the form of {governor , dependant , dependency-relation} , which represents dependency relations between blocks.
_System_Configuration $ 2 $ 68 $ 44 $ PATTERN $ When we make our integrated search and summarization system available to him , the only word in his mind is the name of the company ~ ,~ ( Legend ) .
Conclusion $ 7 $ 270 $ 0 $ CRF $ This paper presented a scheme for integrating natural language processing and information retrieval by adopting a finite-state model of language and a ternary expression representation of document content . 
Background $ 2 $ 59 $ 39 $ KP $ requestValue ( p ) : system requests a value for the paramter p. p E params ( AD ) U {aTask}.
_STRUCTURES_OF_ORGANIZATION $ 2 $ 45 $ 4 $ PATTERN $ If it is a company , to be more informative the line of business usually goes with the key word '~] company' , for instances '~ff~.t..~J food company' , '~ ~ computer company' , ' ~M ~='~ ~ investment consultant company' , but in most cases the keyword '~B] company' will be ignored , such as ~--~ ( President food ) .
Dialog_Management $ 3 $ 102 $ 41 $ CRF $ Thusfor a given path , higher its path utility , greater is the difficulty to understand the concepts it contains and thus higher is the level of expertise required . 
Abstract $ 0 $ 7 $ 2 $ KP $ Grammar specialization is here given a novel formulation as an optimization problem , in which the search is guided by a global measure combining coverage , ambiguity and grammar size.
Theoretical_Ideas , $ 2 $ 57 $ 32 $ PATTERN $ 3.1 Levels of Representation The real PSA/TERM is a/DEF miniature robot currently being developed at NASA Ames Research Center , which is intended for deployment on the Space Shuttle and/Or International Space Station ./O
OUT $ -1 $ 238 $ 48 $ CRF $ The user just selects the utterance to nark up and then clicks on the violation type palette , or , in case it is a new type , clicks on the violated cooperafivity guideline which means that a new violation type is added and text can be entered to describe it , el . 
Experimental_results $ 3 $ 154 $ 100 $ CRF $ Further analysis shows that the new lexicon brings positive effect to 10 queries and negative effect to 4 queries . 
Definitions $ 2 $ 32 $ 7 $ KP $ Corpus/TERM : A/DEF corpus is an ordered set of strings ./O
Dialogue_system_architectures $ 3 $ 86 $ 39 $ PATTERN $ For example , in cases where the background system is distributed and consists of several domain and application system knowledge sources the dialogue system must know which of them to access , in what order , and how the results should be integrated into one answer.
OUT $ -1 $ 188 $ 62 $ CRF $ This is a good performance for single features . 
_Tagging_NuLL-marker_CDM_pairs_ ( via $ 8 $ 73 $ 3 $ CRF $ 4. the/TERM principle/TERM of/TERM superiority/TERM : When/DEF matching a pair of discourse markers for a rhetorical relation , priority is given to the inter-sentence relation whose back discourse marker matched with the first word of a sentence ./O 
CoreLex $ 2 $ 35 $ 11 $ CRF $ The class artifact / attribute / substance for instance includes a number of nouns ( `` chalk , charcoal , daub , fiber , fibre , tincture '' ) that refer to an object that is at the same time an artifact made of some substance and that is also an attribute . 
Interaction_Grammars $ 3 $ 55 $ 2 $ KP $ Context-free grammars and choice trees Let's consider the following context-free grammar for describing simple "addresses" in English such as "Paris , France" : s address --> city , " ," , country.
Generating_from_Bare_Data $ 2 $ 37 $ 25 $ CRF $ * Fact Annotations : ILEX was designed to work with various extra information known about facts , such as the assumed level of interest to the current reader model , the importance of the fact to the system 's educational agenda , and the assumed assimilation of the information ( how well does the system believe the reader to already understand it ) . 
Introduction $ 1 $ 18 $ 13 $ KP $ The SIFAS/TERM ( Syntactic/DEF Marker based Full-Text Abstraction System )/O system has been implemented to use discourse markers in the automatic summarization of Chinese ( T'sou et al.
OUT $ -1 $ 0 $ 0 $ KP $ Reinterpretation of an existing•NLG system in a Generic Generation Architecture L. Cahill , C. Doran~ R. Evans , C. Meilish , D. Paiva , : M. Reape , D. Scott , , N. Tipper .Universities of Brighton and Edinburgh.
YAG's_Template_Specification $ 2 $ 45 $ 2 $ CRF $ Template/TERM slots/TERM are parameters/DEF or variables that applications or users can fill with values ./O 
Introduction $ 1 $ 12 $ 0 $ KP $ Article choice can pose difficult problems in natural language applications.
OUT $ -1 $ 0 $ 0 $ KP $ Dynamic User Level and Utility Measurement for Adaptive Dialog in a Help-Desk System Preetam Maloor Department of Computer Science , Texas A & M University , College Station , TX 77843 , USA preetam@csdl.tamu.edu Joyee Chai Conversational Machines IBM T. J. Watson Research Center , Hawthorne , NY 10532 , USA jchai@us.ibm.com
Abstract $ 0 $ 4 $ 0 $ CRF $ This paper describes a dialog helpsystem which advises users in using computer facilities and software applications provided by the Center for Information and Multimedia Studies , Kyoto University . 
Given_an_input_space_X~*_of $ 3 $ 43 $ 12 $ PATTERN $ In general , and this is the case for linguistic concepts , we can only hope that h is a good approximation of Ci..
_KNOWLEDGE_EXTRACTION $ 3 $ 102 $ 44 $ CRF $ However such a method only identifies the organizations of the known organization types and provides new proper names only . 
Knowledge_base $ 5 $ 97 $ 11 $ KP $ </KU> In Japanese , there are many sentential patterns to express if-then relation.
_Relations_that_are_relevant_and_correct_in $ 3 $ 114 $ 80 $ PATTERN $ At the top of the list are the items that receive the lowest score , i.e.
Learning_Algorithms_Tested $ 2 $ 44 $ 24 $ PATTERN $ In this setting , a Decision/TERM List/TERM is a/DEF list of features extracted from the training examples and sorted by a log-likelihood measure ./O
CoreLex-II $ 3 $ 42 $ 6 $ KP $ Comparison of sense distributions is now performed over synsets on all levels , not just over a small set on the top levels.
OUT $ -1 $ 0 $ 0 $ KP $ Word Alignment of English-Chinese Bilingual Corpus Based on Chunks Sun Le , Jin Youbing , Du Lin , Sun Yufang Chinese Information Processing Center Institute of Software Chinese Academy of Sciences Beijing 100080 P. R. China lesun , ybjin , yfsun , ldu@sonata.iscas.ac.cn
Identifying_Structure_and_the $ 2 $ 21 $ 0 $ PATTERN $ 'Character Set' The initial task , given an incoming bit-stream , is to identify if a language-like structure exists and if detected what are the unique patterns/symbols , which constitute its 'character set'.
ALLiS $ 2 $ 26 $ 21 $ KP $ LT/TERM TTT/TERM is a/DEF good trade/Off between the rapidity of CASS and the rich formalism of XFST ./O
Interaction_Grammars $ 3 $ 104 $ 51 $ PATTERN $ The first rule is then read : "if C is a tree of type city , and Co a tree of type country , then adr ( Co ,C ) is a tree of type address" , and similarly for the remaining rules.
OUT $ -1 $ 145 $ 74 $ CRF $ For example , ( A , Adv , D ) is a dependency type expressing the relationship between words in expression `` very large '' where `` very '' is a depending adverb and `` large '' is a head adjective . 
Middle $ 6 $ 72 $ 35 $ KP $ ( 2 ) High ambiguous words tend to be high frequent.
Summary $ 7 $ 87 $ 0 $ CRF $ RSTTool/TERM is a/DEF robust tool which facilitates manual analysis of a text 's rhetorical structure ./O 
OUT $ -1 $ 143 $ 138 $ PATTERN $ Note that the correlation/TERM metric/TERM C/TERM is the/DEF square root of the X 2 metric ./O
Information_Structures $ 3 $ 81 $ 4 $ CRF $ The descriptions are as follows : Dependency ~ [ patient] < - : ~_~ -[ agent] ~[ ] relations : Definitions : ~ : medicinel~ : jqe~J , ? addictivel~ ~.L : transportl~l_I~ , manner= secretly , crimel~l~ ~[ ] : communityl[ ] ~ In this example , the descriptions specify that a 'community '' is an agent involved in a 'transport ' event transporting the patient `` medicine ' . 
LTAG_formalism $ 2 $ 25 $ 2 $ KP $ We choose LTAGs as our target/TERM grammars/TERM ( i.e. , the/DEF grammars to be extracted )/O because LTAGs possess many desirable properties , such as the Extended Domain of Locality , which allows the encapsulation of all arguments of the anchor associated with an etree.
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 131 $ 96 $ PATTERN $ ( 7 ) denotes that t frequently appears in the particular paragraph pj rather than the document di which includes pj.
Stochastic_Surface_Realization $ 2 $ 75 $ 0 $ CRF $ 2.2 Generating Utterances The input to NLG from the dialogue manager is a frame of attribute-value pairs . 
OUT $ -1 $ 0 $ 0 $ KP $ An Algorithm for Situation Classification of Chinese Verbs Xiaodan Zhu , Chunfa Yuan State Key Laboratory for Intelligent Technology and System , DepL of Computer Science & Technology , Tsinghua University , Beijing 100084 , P.R.C.
Selective_Sampling_Evaluation $ 4 $ 88 $ 12 $ CRF $ The 47 set of probabilities of the possible parse trees for a sentence defines a distribution that indicates the grammar 's uncertainty about the structure of the sentence . 
Abstract $ 0 $ 27 $ 25 $ CRF $ As in Paice ( 1990 ) , summarization techniques in text analysis are severely impaired by the absence of a generally accepted discourse 11 model and the use of superstructural schemes is promising for abstracting text . 
Abstract $ 0 $ 129 $ 98 $ PATTERN $ We denote this as a binary relation Causality ( FrontClause ( 2 ) , BaekClause ( 2 ) ) where FrontClause/TERM ( n ) denotes the/DEF discourse segment that is encapsulated by the Front discourse marker of the corresponding rhetorical relation whose sequence number is n ./O 15 BackClause ( n ) can be defined similarly.
_The_straightforward_unpacking_of_feature $ 3 $ 18 $ 10 $ PATTERN $ The/DEF most basic metric for patterns with symbolic features is/O the Overlap/TERM metric/TERM given in equation 1; where A ( X , Y ) is the distance between patterns X and Y , represented by n features , wi is a weight for feature i , and 5 is the distance per feature.
Introduction $ 1 $ 16 $ 7 $ KP $ DTDs/TERM determine/DEF the logical structure of documents and how to tag them accordingly ./O
Conclusion $ 6 $ 80 $ 3 $ KP $ The new algorithm currently does not use information 93 about the orthography of the word , an important source of information.
OUT $ -1 $ 86 $ 68 $ PATTERN $ A PROPER__NOUN/TERM is defined as a/DEF noun phrase in which all words are capitalized ./O
Introduction $ 1 $ 16 $ 9 $ KP $ This is a linear representation over a new feature space a transformation of the original instance space to a higher dimensional and more expressive space.
_Measurements $ 3 $ 81 $ 14 $ PATTERN $ ) In isolation ( before being specialized to the situation of joint venture capitalization , another cost ) , equal denotes a completely unsaturated relation : k collection ( partition ( measurablestuff ) ) .
Abstract $ 0 $ 103 $ 42 $ KP $ Schwarzschild's covering proposal and van den Berg's assignment-set proposal are perfectly compatible.
Introduction $ 1 $ 10 $ 2 $ PATTERN $ Generally , dependency structure analysis consists of two steps.
Abstract $ 0 $ 81 $ 14 $ CRF $ Given a sentence• S and a type of information T the system verifies if the sentence matches some of the patterns associated with type T. For each matched pattern , the system extracts information from the sentence and instantiates a template of type T. For example , the Content slot of the problem identification template is instantiated with all the sentence • : ( avoiding references , structural elements and parenthetical expressions ) while the What slot 'of the topic of the document template is instantiated with a parsed sentence fragment • to the left or to the right of the make known relation depending on the attribute voice/TERM of/TERM the/TERM verb/TERM ( active/DEF vs. passive )/O . 
Discussion $ 4 $ 74 $ 1 $ PATTERN $ Thus , a contribution of Gso , is the demonstration that from a simple context-free grammar , with a very lightweight formalism , one can extract enough information ( Ontology , Parsebank , Parser Predictions strategy ) to conduct meaningful clarification dialogues.
Optimizations $ 5 $ 89 $ 4 $ PATTERN $ We can similarly define GoodPotential 1 to0 ( S ) , and then define GoodPotential/TERM (/TERM S/TERM )/TERM = max/DEF ( GoodPotential 0 to_l ( S ) ,/O GoodPotential 1 to O ( S ) ) As we construct the RRE-tree , we keep track of the largest Goodness ( S ) we have encountered.
Extending_a_Grammar_to_Enable $ 4 $ 121 $ 48 $ CRF $ `` , in favor of `` his sister 's dog '' , without the application having to request a pronoun explicitly , as in the example shown above , we could add a rule to force the pronominal feature of the inner most possessor to be YES , whenever a ( repeated ) noun phrase is a possessor of a possessor of the primary noun . 
Word_Co/Occurrence_Vector $ 2 $ 32 $ 0 $ KP $ 2.1 Word-Document Co/Occurrence Matrix Word co/Occurrences are directly represented in a matrix whose rows correspond to words and whose columns correspond to documents ( e.g.
System_Overview $ 3 $ 47 $ 11 $ CRF $ X/TERM ° is the/DEF head of X m and the anchor of the etree ./O 
Impact_of_Lexicon_Size $ 9 $ 185 $ 47 $ CRF $ 12 Conclusions and Future Work We proposed an approach to cross-lingual IR based on hidden Markov models , where the system estimates the probability that a query in one language could be generated from a document in another language . 
Enriching_the_Feature_Set_with $ 2 $ 73 $ 0 $ PATTERN $ 2.2 Making Measures of Association Available to the Parser To make the measures of association available to the parser , we started by discretizing each measure , that , is substituting for each continuous measurement a set of binary predicates coarsely describing its approximate value.
Abstract $ 0 $ 1 $ 0 $ CRF $ Word/TERM Sense/TERM Disambiguation/TERM (/TERM WSD/TERM )/TERM is a/DEF central task in the area of Natural Language Processing ./O 
OUT $ -1 $ 82 $ 56 $ PATTERN $ Consider the subphrase estimate for line represents the operation needed to.
_Instrumentalists_not_including_string_players $ 8 $ 143 $ 131 $ CRF $ • 3 inappropriate clusters that relate homonyms , such as band 2 ,7 ( strip or stripe of a contrasting color or material/unofHcial association of people ) .
Architecture $ 2 $ 62 $ 18 $ KP $ Ontology learning operates on the extracted information and is used for three tasks.
Complexity $ 3 $ 83 $ 0 $ KP $ This section puts forward some notes on the complexity of dialogue.
The_REXTOR_System $ 5 $ 167 $ 45 $ CRF $ ( PRPZ/TERM is the/DEF part/Of-speech tag for possessive pronouns ,/O DT/TERM for determiners/DEF ,/O JJX/TERM for adjectives/DEF ,/O J JR/TERM for comparative/DEF adjectives ,/O JJS/TERM for superlative/DEF adjectives ,/O NNX/TERM for singular/DEF or mass nouns ,/O NNS/TERM for plural/DEF nouns ,/O NNPX/TERM for singular/DEF proper nouns ,/O NNPS/TERM for plural/DEF proper nouns ,/O IN/TERM for prepositions/DEF ./O
Rule_Sequence_Learning $ 3 $ 46 $ 5 $ PATTERN $ ( 2 ) A sequence of rules of the form : Change the label of a string from m to n if C ( string ) , where C/TERM is a/DEF predicate over strings and/O m ,n ~ L. A string is labelled by first applying the start-state annotator to it , and then applying each rule , in order.
Previous_Schemes_for_Grunt $ 2 $ 58 $ 30 $ CRF $ In our corpus , only 1 % of the grunts were negative in meaning , and these were all in contexts where a negative answer was expected or likely , so this distinction is a strange choice for a top-level dividing principle .
_Otherwise ,_add_to_the_current_context_new $ 6 $ 125 $ 42 $ PATTERN $ The argument by cases consists of a pair of Argument Graphs , one graph for each case.
Introduction $ 1 $ 9 $ 0 $ PATTERN $ There is a strong relation between a learning strategy , its formal learning framework and its representational theory.
Introduction $ 1 $ 35 $ 28 $ CRF $ REXTOI~ also provides a playground and testbed for future experimentation in linguistically-motivated indexing schemes .
_Annotation_Guidelines $ 4 $ 102 $ 14 $ PATTERN $ Another example is 23 ) , it denotes an subordinate feature , annotated as 31 ) 7.
Approach $ 2 $ 38 $ 11 $ CRF $ Each node has a label , which offers a brief textual description of the node .
The_Corpus $ 6 $ 152 $ 3 $ KP $ In the 80-sentence corpus under consideration , the sentence structure is complex and stylized; with an average of 20 words per sentence.
Performance $ 4 $ 172 $ 9 $ PATTERN $ Hits/TERM denotes how/DEF many of the files passed to IE actually had at least one template in them and Templates shows how many templates were extracted as a result of the query ./O
Social_Goals_and_Conversational $ 3 $ 86 $ 42 $ CRF $ In this case , two plans can apply : the simple plan for refusing , or the elaborated plan for refusing which includes a justification for the refusal .
Abstract $ 0 $ 3 $ 1 $ KP $ Using clustering techniques and features reflecting the diglossia phenomenon , we have successfully discriminated registers in Modem Greek.
_Results_and_comparison_of_ambiguities $ 3 $ 102 $ 21 $ PATTERN $ when one lexeme represents at least 80% of the class.
Modeling_various_degrees_of $ 5 $ 71 $ 8 $ CRF $ is a list of dialogue actions that the agent wishes to carry out .
Whole_Sentence_Maximum $ 2 $ 31 $ 11 $ CRF $ The 5i values are obtained as the solution of the m equations : 1 Z = 0 w wEN ( 3 ) where/ = 1 , ... , m , f # ( w ) = ~=lfi ( w ) and f~ is a training corpus .
Learning_RRE_Rules $ 4 $ 64 $ 11 $ CRF $ Very/TERM Reduced/TERM Regular/TERM Expression/TERM (/TERM VRRE/TERM )/TERM : Given/DEF a finite alphabet E , the set of very reduced regular expressions over that alphabet is defined as : ( 1 ) 'v'a~ E : a is a VRRE and denotes the set { a } ( 2 ) ./O
Types_of_requests_and $ 2 $ 17 $ 1 $ KP $ Bunt ( 1989 ) makes a distinction between factual information acts and dialogue control acts.
Levels_of_Annotation $ 4 $ 112 $ 0 $ PATTERN $ 4.2 The Morpho~Syntactic and Syntactic Levels The ADAM proposal for the morphosyntactic/TERM level/TERM is a/DEF two-layer annotation structure , containing respectively information on word category and morphosyntactic features ( pos tagging ) , and non recursive phrasal nuclei ( called chunks ) ./O
Integration_Process $ 4 $ 92 $ 4 $ KP $ SURGE/TERM ( Elhadad and Robin , 1996 ) is a/DEF comprehensive English Grammar written in FUF ./O
Error_Analysis $ 5 $ 51 $ 1 $ CRF $ 2 Taking each category of the three in turn , problematic constructs included : co/Ordination , punctuation , treating ditransitive VPs as being transitive VPs , confusions regarding adjective or adverbial phrases , and copulars seen as being possessives .
Accommodation_in_GoDiS $ 3 $ 68 $ 17 $ CRF $ IS : PRIVATE : SHARED : PLAN : STACKSET ( AcT1ON ) AGENDA : STACK ( ACTION ) PaL : SET ( PRoP ) I BEL : SET ( PRoP ) QUD : STACK ( QUESTION ) TMP : [ SPEAKER : LU : [ MOVES : BEL : SET ( PRoP ) QUD : STACKSET ( QUESTION ) SPEAKER : PARTICIPANT LU : MOVES : PARTICIPANT ASsoCSET ( MOvE , BOoL ) ASsOCSET ( MOvE , BooL ) ] ] Figure 1 : The type of information state we are assuming This dialogue is incoherent if what is being discussed is when the child Maria is going to be picked up from her friend 's house ( at least under standard dialogue plans-that we might have for such a conversation ) .
Framing_the_generation_problem $ 3 $ 64 $ 14 $ PATTERN $ Our model of prominence is a simple local one similar to ( Strube , 1998 ) .
Support_Vector_Machines $ 2 $ 8 $ 2 $ CRF $ In the field of natural language processing , SVMs are applied to text categorization , and are reported to have achieved high accuracy without falling into over-fitting even with a large number of words taken as the features ( Joachims , 1998 ; Taira and Haruno , 1999 ) First of all , let us define the training data which belongs to either positive or negative class as follows : ( Xl , YX ) , ... , ( Xl , Yl ) Xi 6 R n , Yi 6 { +1 , -1 } xi is a feature vector of the i-th sample represented by an n dimensional vector , yi is the class ( positive ( +l ) or negative ( -1 ) class ) label of the i-th data .
Summarization_Filters $ 4 $ 127 $ 34 $ PATTERN $ For the above example , the pattern would be "Here is @6.label" , where 6 is the number of a non-narrative node , with label being its label.
Our_approach_to_Multilingual $ 2 $ 27 $ 0 $ KP $ Document Authoring Our Multilingual Document Authoring system has the following main features : First , the authoring process is monolingual , but the results are multilingual.
Abstract $ 0 $ 80 $ 19 $ PATTERN $ The problem is the meaning.
OUT $ -1 $ 102 $ 60 $ KP $ The chunk tags are derived from the parse tree constituents , and the part/Of-speech tags were generated by the Brill tagger ( Brill , 1995 ) .
October_2000 $ 7 $ 5 $ 3 $ CRF $ An intimate relationship between the two issues is becoming apparent for example , in the consideration of translation equivalence in parallel corpora , the construction of mullilingual ontologies , and the examination of senses in relation to specific natural language applications such as machine translation , information retrieval , summarization , etc .
Experimental_Results $ 7 $ 171 $ 4 $ CRF $ split of the data 'Apte/TERM split/TERM , ' which consists/DEF of 9603 texts for training and 3299 texts for test ./O
Modelling_Phonological_Dyslexia $ 4 $ 101 $ 20 $ PATTERN $ Because of the absence of some compound words ( e.g. , gentlelman ) from the dictionary , the simulations concerning "parent words" ( e.g. , father is the parent of.fat and her ) for both Test 1 and Test 2 are not perfect.
Support_Vector_Machines $ 2 $ 31 $ 3 $ PATTERN $ Formally , we can define the pattern recognition problem as a learning and building process of the decision function f : lq.
_Tagging_NuLL-marker_CDM_pairs_ ( via $ 8 $ 127 $ 57 $ PATTERN $ RDMs/TERM is a/DEF table searching process ./O
Instructions_and_Interactivity $ 3 $ 38 $ 0 $ KP $ It is obvious that instructional situations profit from an interactive setting .
Automatic_Extraction_of $ 4 $ 87 $ 36 $ PATTERN $ The equation for two-element compound nouns is as follow : P ( x ,y ) I ( x;y ) = log 2 P ( x ) x P ( y ) 61 where x and y are two words in the corpus , and I/TERM (/TERM x;/TERM y/TERM )/TERM is the/DEF mutual information of these two words (/O in this order ) .
OUT $ -1 $ 69 $ 43 $ CRF $ R/TERM is the number of tokens in the target string ./O
Conclusion $ 5 $ 140 $ 8 $ KP $ Second , LexTract can build derivation trees for each sentence in the corpora .
_Instrumentalists_not_including_string_players $ 8 $ 96 $ 84 $ CRF $ Extract the candidate senses that satisfy the parallel polysemy criterion , in three variants : Experiment 1 : sets of senses that have parallel translations in at least two out of the four target languages .
Experimental_Design $ 3 $ 182 $ 16 $ PATTERN $ Content-based measures which depend on a single ground truth gi compute the summary-ground truth similarity sim ( s , gi ) .
October_2000 $ 7 $ 13 $ 12 $ PATTERN $ However it is not clear whether , or where , it is a good measure , and there is some evidence that it does not match our intuitions ( Kilgardff and Rose 1998 , Kilgarriff in press ) .
Dialog_Management $ 3 $ 62 $ 1 $ PATTERN $ 3.1 Content Selection Module The Content/TERM Selection/TERM Module/TERM consists of four components : Level-Adjusting/DEF Agent , UtilityUpdating Agent , Action Planner and Content Selector ./O
OUT $ -1 $ 195 $ 190 $ CRF $ The second tree is a default tree that just classifies any sentence as not an answer .
Evaluation $ 3 $ 193 $ 4 $ PATTERN $ The recall/TERM is the/DEF number of identified errors over the total number of errors ./O
Abstract $ 0 $ 29 $ 27 $ CRF $ This IF representation is neutral between sentences that have different verbs , subjects , and objects such as A double room costs 150 dollars a night , The price of a double room is 150 dollars a night , and A double room is 150 dollars a night .
Reference_Resolution $ 4 $ 104 $ 5 $ PATTERN $ If there is a match , assume the current PN is being used to corefer to the referent of the matching PN .
Related_work $ 6 $ 178 $ 29 $ KP $ Having grammar rules encoded as unit clauses alleviates this problem as does our decision to use lgg rather than rlgg .
Learning_Algorithms_Tested $ 2 $ 36 $ 16 $ CRF $ In the SNo W architecture there is a winnow/TERM node/TERM for each class , which learns/DEF to separate that class from all the rest ./O
Dialog_Management $ 3 $ 99 $ 38 $ PATTERN $ The/TERM utility/TERM of/TERM a/TERM path/TERM in the graph is the/DEF summation of the reward/punishment ratio of all the nodes ( subgoals ) in that path ./O
_Functional_tag_of_the_constituent $ 5 $ 57 $ 1 $ KP $ The functional tag of the constituent embedding the problem in Figure 1 is DIR .
Qualitative_Evaluation_of_the $ 4 $ 110 $ 9 $ PATTERN $ We employ the supertag-based tree model whose parameters consist of whether a lexeme ld with supertag sd is a dependent of lexeme 1 , , , with supertag s , , , .
Introduction $ 1 $ 13 $ 5 $ PATTERN $ Here , an event/TERM is the/DEF subject of a document itself ,/O i .e .
Extending_a_Grammar_to_Enable $ 4 $ 97 $ 24 $ PATTERN $ ( If there is neither possessor nor determiner then the grammar considers the np-type : if it is COMMON , it uses NO ( for indefinite ) and if it is PROPER , it uses NOART The number feature passed is the value passed from the determiner , if there is one , or the value from the current template .
The_TransType_model $ 2 $ 69 $ 54 $ CRF $ The total/TERM probability/TERM of the ith target-text token ti is just the/DEF average of the probabilities with which it is generated by each source text token sj ;/O this is a weighted average that takes the distance from the generating token into account : is1 p ( tils ) = ~p ( tilsj ) a ( jli , Is[ ) j=O ( 3 ) where p/TERM (/TERM ti/TERM Is/TERM j/TERM )/TERM is a/DEF word-for-word translation probability ,/O Isl/TERM is the/DEF length ( counted in tokens ) ofthe source segment s under translation ,/O and a/TERM (/TERM jli/TERM ,/TERM Is\/TERM ]/TERM )/TERM is the/DEF a priori alignment probability that the target-text token at position i will be generated by the source text token at position j ;/O this is equal to a constant value of 1~ ( Is I + 1 ) for model 1 .
Applications_of_LexTract $ 4 $ 219 $ 73 $ CRF $ For Srinivas ' and our grammars , the first line is the results tested on Section 23 , and the second line is the one for Section 22 .
Description_of_the_algorithm $ 2 $ 14 $ 0 $ KP $ There are four main steps in our approach .
Information_Structures $ 3 $ 80 $ 3 $ PATTERN $ The informaton structure of this example consists of two parts , the dependency relations and the HowNet definitions .
Statistical_Semantic_Parsing $ 4 $ 102 $ 3 $ PATTERN $ S is the set of states reachable by the parser .
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 118 $ 83 $ PATTERN $ In a similar way , DispPt denotes dispersion of term t in the level of Paragraph .
More_General_Information $ 6 $ 103 $ 3 $ PATTERN $ " For this more general case , define B/TERM (/TERM X/TERM )/TERM to be B/DEF ( X1 , X2 , . . .X , ) where each Xi is a possible value of X ./O
Compound_Verb_Groups $ 1 $ 16 $ 6 $ KP $ </vg> I <vg> am sorry </vg> that I <vg> did not know </vg> about the conference , I <vg> would have participated </vg> in it .
Evaluation_Measures $ 2 $ 137 $ 112 $ PATTERN $ However , the scores produced by such evaluation measures cannot be used reliably to compare summaries of drastically different lengths , since a much longer summary is more likely than a short summary to produce a term frequency .vector which is similar to the full document's "tf vector , despite the normalization of the two vectors .
Fundamental_Data_Differences $ 2 $ 39 $ 0 $ PATTERN $ 2.2 Uncertainty in Speech Transcriptions One of the primary factors that distinguishes textbased language processing tasks , such as reading comprehension , from spoken-language processing tasks , such as audio comprehension , is the uncertainty inherent in the word sequence output by the speech recognizer .
Word_Clustering $ 3 $ 65 $ 21 $ PATTERN $ For example , Hofmann's is of order O ( ]DIIWI2 ) , while ours is only of O ( ID I + ]WI2 ) , where IDI/TERM denotes the/DEF number of texts and/O IW]/TERM the/DEF number of words ./O
Problem_Space_Modeling $ 2 $ 40 $ 13 $ PATTERN $ Remedy/TERM is the/DEF template that is used to generate natural language responses and explanations corresponding to a particular goal ./O
Conclusion $ 4 $ 55 $ 1 $ KP $ The perplexity of the test sample decreases when a combination of models with k = 2 and k = 3 is used to predict string probabilities .
_The_co-reference_problem_in_summarization $ 4 $ 43 $ 19 $ CRF $ ) The Columbia University system ( McKeown et al . , 1999 ) creates a multi-document summary using machine learning and statistical techniques to identify similar sections 41 and language generation to reformulate the summary .
Abstract $ 0 $ 1 $ 0 $ PATTERN $ Word/TERM Sense/TERM Disambiguation/TERM (/TERM WSD/TERM )/TERM is a/DEF central task in the area of Natural Language Processing ./O
Building_Spoken_Dialo~te_Systems $ 4 $ 115 $ 10 $ PATTERN $ ( l-gatsu ichigatsu month nil i ) The first three elements are the identifier , the pronunciation , and the grammatical category of the word .
Conclusions_and_Future_Work $ 5 $ 259 $ 3 $ PATTERN $ Measures which gauge content similarity produce more highly correlated rankings whenever ground truths do not disagree in focus .
Significance_of_word_contexts $ 2 $ 66 $ 18 $ PATTERN $ The contextual/TERM representation/TERM of/TERM a/TERM word/TERM has been defined as a/DEF characterisation of the linguistic context in which a word appears ./O
Resolution_Procedures $ 2 $ 32 $ 1 $ PATTERN $ Here the interlingua we rely on is a variant of Text/TERM Meaning/TERM Representation/TERM ( TMR/ACR ) ( see http : //crl .nmsu .edu/Research/Projects/mikro/in dex .html ) and we focus on a sample Spanish text and its interlingual analysis , which is known to be reproducible automatically .
Empirical_Evaluation $ 4 $ 124 $ 13 $ PATTERN $ A set of domain knowledge consists of 56 rules with about one to 10 rules for each category was generated .
Corpora $ 3 $ 65 $ 7 $ CRF $ The MEDLINE/TERM database/TERM is an/DEF online collection of abstracts for published journal articles in biology and medicine and contains more than nine million articles ./O
Overview_of_the_system $ 2 $ 13 $ 5 $ PATTERN $ In this case , the symbols are the POS tags ( Ci ) that belong to the corresponding chunk ( Si ) .
Abstract $ 0 $ 2 $ 0 $ PATTERN $ IF ( Interchange Format ) , the interlingua/TERM used/TERM by/TERM the/TERM C-STAR/TERM consortium/TERM , is a/DEF speech-act based interlingua for task/Oriented dialogue ./O
Conceptualizing_Events $ 2 $ 94 $ 64 $ PATTERN $ Investigation/TERM is a/DEF rich source of occurrences that should not happen in civil aircraft WINDOW ,/O TURNING THE HANDLE , PULL , and LET operations .
Abstract $ 0 $ 1 $ 0 $ CRF $ This paper presents the integration of a largescale , reusable lexicon for generation with the FUF/SURGE unification-based syntactic realizer .
_Our_Classification_Algorithm_for $ 2 $ 82 $ 18 $ PATTERN $ In this paper , we render an algorithm to classify verbs into different categories , which is the basis of another research . . . . recognition of sentence situation , which will be discussed in future work .
Abstract $ 0 $ 211 $ 45 $ KP $ " evaluate ( theatre=Ridge ) + requestValue ( time ) Usr : "I don't know .
Results_and_Discussion $ 5 $ 148 $ 17 $ PATTERN $ baseline 2 ) reaches the highest score in both languages , indicating that the combination of domain word frequency ( considered at step 1 of the algorithm ) and domain text frequency ( considered at step 2 ) is a good one .
Statistical_Semantic_Parsing $ 4 $ 139 $ 40 $ PATTERN $ hk covers s ) , we have PCai ( s ) • o~ I hk ) = pc + O " nc Pc -tnc ( 14 ) where Pc/TERM and ne/TERM are the/DEF number of positive and negative examples covered by hk respectively ./O
Introduction $ 1 $ 22 $ 17 $ PATTERN $ Conversely , a question can have multiple correct answers , where each of several individual sentences is a correct answer .
Given_a_context-based_representadon $ 2 $ 30 $ 1 $ KP $ words or word tags ) ,
Maximum_entropy-based_parse $ 2 $ 38 $ 9 $ PATTERN $ In ideal circumstances , where the distribution of features in the training data accurately represents the true probability of the features , the performance of the model should increase asymptotically with each iteration of training until it eventually converges .
The_semantic_behavior_of_the $ 5 $ 94 $ 1 $ PATTERN $ For example , KIKEN_NA JOUTAI ( dangerous ) ( situation ) dangerous situation In this case "dangerous" represents the state concretely .
Discussion $ 5 $ 194 $ 18 $ KP $ FERGUS/TERM currently can perform/DEF punctuation and function word insertion , and morphology and lexical choice are under development ./O
Implementation $ 4 $ 368 $ 197 $ CRF $ For example , Figure 2 illustrates a complex filter created by using a GUI to compose together a named entity extractor , a date extractor , a component which discovers significant associations between the two and writes the result to a table , and a visualizer/TERM which plots/DEF the results as a graph ./O
The_Filtering_Problem $ 1 $ 5 $ 1 $ KP $ FALCon , our embedded MT system , has been designed to assist an English-speaking person in filtering , i .e . , deciding which foreign language documents are worth having an expert translator process further .
Introduction $ 1 $ 21 $ 17 $ KP $ The segmentation/TERM component/TERM provides/DEF a word lattice of the sentence that contains all the possible words ,/O and the final disambiguation is achieved in the parsing process .
OUT $ -1 $ 0 $ 0 $ KP $ Query Translation in Chinese-English Cross-Language Information Retrieval Zhang Yibo , Sun Le , Du Lin , Sun Yufang Chinese Information Processing Center , Institute of Software , Chinese Academy of Sciences , P .O .Box 8718 , Beijing , 100080 , P .R .
Related_Work $ 2 $ 80 $ 53 $ CRF $ The key to our system is a WSD method for open text .
HMM_for_Mono-Lingual_Retrieval $ 2 $ 18 $ 0 $ KP $ Following Miller et al . , 1999 , the IR/TERM system/TERM ranks/DEF documents according to the probability that a document D is relevant given the query Q , P ( D is R IQ ) ./O
OUT $ -1 $ 0 $ 0 $ KP $ Japanese Dependency Structure Analysis Based on Support Vector Machines Taku Kudo and Yuji Matsumoto Graduate School of Information Science , Nara Institute of Science and Technology {taku-ku , matsu}@is , aist-nara , ac .
Examples_of_YAG_in_use $ 3 $ 71 $ 6 $ PATTERN $ Thus , we can read the whole proposition as "Pluto is a member of class dog .
Tree_Structures $ 2 $ 30 $ 14 $ CRF $ The only difference comes from a simplification performed by joining the words into phonological words ( composed of one content word noun , adjective , verb or adverb and of the surrounding function words ) .
OUT $ -1 $ 4 $ 4 $ KP $ Keywords : Cross Language Information Retrieval , Multilingual Information Processing , Chinese , Japanese and Korean ( CJK ) Languages Introduction After the opening of the Cross/TERM Language/TERM Information/TERM Retrieval/TERM ( CLIR/ACR ) track in the TREC-6 conference ( TREC-1998 ) , several reports have been published on cross language information retrieval in European languages , and sometimes , European languages along with one of the Asian languages ( e .g . , Chinese , Japanese or Korean ) .
Abstract $ 0 $ 3 $ 0 $ KP $ When a lexical item is selected in the language production process , it needs to be explained why none of its superordinates gets selected instead , since their applicability conditions are fulfilled all the same .
Introduction $ 1 $ 16 $ 12 $ KP $ All ASR transcriptions we use will be actual output from a broadcast news ASR system with a word error rate of 30% .
_KNOWLEDGE_EXTRACTION $ 3 $ 120 $ 62 $ PATTERN $ it is a known proper name ( bib ) or a location name ( No ) .
OUT $ -1 $ 93 $ 92 $ KP $ al , a2 , refo ) [ compellingness ( o , al , a2 , refo ) [ >px+ko'x , where o , al , a2 and refo are defined as in the previous Def; opop/TERM is an/DEF objective population (/O e .g . , siblings ( o ) ) , and I opopl >2 pe opop; xeX = [compellingness ( p , al , a_~ , refo ) l gx/TERM is the/DEF mean of X ,/O ~x/TERM is the/DEF standard deviation and/O k/TERM is a/DEF user/DEFined constant We/O have defined similar measures for arguing the value of a single entity and we named them s-compellingness and s-notably-compelling? .
The_Interactional_Framework $ 2 $ 25 $ 5 $ PATTERN $ 4 The planner we use is a modification of the DRIPS/TERM decision-theoretic/DEF hierarchical planner (/O Haddawy and Hanks , 1998 ) .
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 159 $ 124 $ PATTERN $ 'Doe' denotes the number of documents .
From_References_to_Coherence $ 5 $ 146 $ 6 $ PATTERN $ Since ~the motherboard" in ( l-a ) is a definite noun phrase and syntactic as well as conceptual information match with the plausible antecedent "the P6LXZ-A' , a referential link can be established , see the ISCOREFERENTIAL relation in Figure 6 .
Tagged_Text $ 3 $ 32 $ 18 $ PATTERN $ The most important named entities in the Remedia corpus are the names of people and the names of places .
Background : _The_STOP_System $ 2 $ 19 $ 2 $ PATTERN $ Internally , STOP/TERM is a/DEF fairly conventional shallow NLG system , with its main innovation being the processing used to control the length of leaflets (/O Reiter , 2000 ) .
Hyperonymy_in_lexical_semantics $ 5 $ 139 $ 42 $ CRF $ This divergence points to the need for a distinction between conceptuaJ , and lexicalg : ranularitv and inheritance : The WordNet chain represents rather a series of concepts than of words entering the lexical choice process , which appears to be better represented by a Cruse-type chain with few designated levels ( but needs to be augmented with near-synonyms for tile 'horing , . . . . . . . . . . . . . . ffntity creature . . . . `` . . . . . . . . li~q form animal , beast , . . . . . . . . animal c > rdate ve~ebrate ~ammal p~cental c~71ivore 7~_1 ine dog J / .
Abstract $ 0 $ 2 $ 1 $ KP $ WIT/TERM features/DEF an incremental understanding mechanism that enables robust utterance understanding and realtime responses ./O
Introduction $ 1 $ 64 $ 56 $ PATTERN $ The main focus of the paper is the description of two features which are particularly useful for attribution determination : prototypical agents and actions .
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 126 $ 91 $ PATTERN $ In formula ( 5 ) , di is the i-th document and consists of the number of n paragraphs ( see Figure 4 ) .
Learning_Algorithms_Tested $ 2 $ 50 $ 30 $ CRF $ Decision/TERM Lists/TERM were one/DEF of the most successful systems on the 1st Senseval competition for WSD (/O Kilgarriff and Rosenzweig , 2000 ) .
_Head_countability_preferences_of_the_head $ 7 $ 60 $ 0 $ KP $ of the NP : In case the head of an NP is a noun we also use its countability as a feature .
_The_session_was_also_attended_by_observers_from_the_following_international_organizations : $ 7 $ 192 $ 141 $ CRF $ This example is the most explicit form an initializer can take as it contains a lexical element which corresponds to each of the four functions outlined above .
Interaction_Grammars $ 3 $ 85 $ 32 $ KP $ city ( ham ) --> [] .
Our_approach_to_Multilingual $ 2 $ 50 $ 23 $ CRF $ This nlicro-structure can in general be dealso different styles or modes of communication , lerlniued by studying a corpus of documents and by 25 exposing the structure of choices that distinguish a given document from other documents in this class .
Results_and_Discussion $ 5 $ 142 $ 11 $ KP $ WDD in parallel texts .
Evaluation $ 4 $ 86 $ 10 $ KP $ Random mapping Heuristic 1 Precision ( % ) 49 .85 75 .21 Coverage ( % ) 100 .0 59 .51 Heuristic 2 74 .66 100 .0 Heuristic 3 71 .87 100 .0 Heuristic 4 55 .49 29 .36 Heuristic 5 : 56 .48 63 .01 Heuristic 6 67 .24 64 .14 Table 1 : Individual heuristics performance Summing Logistic regression Decisioin tree Preeisi0n ( % ) 84 .61 86 .41 93 .59 Coverage ( % ) 100 .0 100 .0 77 .12 Table 2 : Performance and comparison of the decision tree based combination We performed 10-fold cross validation to evaluate the performance of the combination of all the heuristics using the decision tree we split the data into ten parts , reserved one part as a validation set , trained the decision tree on the other nine parts and then evaluate the reserved part .
Grammar_Induction $ 3 $ 62 $ 1 $ KP $ When the training corpus consists of a large reservoir of fully annotated parse trees , it is possible to directly extract a grammar based on these parse trees .
Abstract $ 0 $ 12 $ 10 $ PATTERN $ Introduction WordSmith/TERM Tools/TERM ( Scott , 1998 ) offers/DEF a program for comparing corpora , known as KeyWords ./O
Building_a_reusable_lexical_chooser $ 2 $ 22 $ 5 $ PATTERN $ Conceptual/TERM elements/TERM are by definition domain and application dependent ( they are the/DEF primitive concepts used in an application knowledge base )/O .
Solving_the_generation_problem $ 5 $ 144 $ 4 $ PATTERN $ As we have seen , ( 5b ) represents a common pattern of description; this particular example is motivated by an exchange two human subjects had in our study , cf .
The_TABULATE_ILP_Method $ 3 $ 67 $ 1 $ KP $ 3 .1 The Basic TABULATE Algorithm Most ILP methods use a set-covering method to learn one clause ( rule ) at a time and construct clauses using either a strictly top-down/TERM ( general/DEF to specific )/O or bottom-up/TERM ( specific/DEF to general )/O search through the space of possible rules ( Lavrac and Dzeroski , 1994 ) .
Abstract $ 0 $ 2 $ 0 $ CRF $ The Maximum/TERM Entropy/TERM principle/TERM (/TERM ME/TERM )/TERM is an/DEF appropriate framework for combining information of a diverse nature from several sources into the same language model ./O
Overview $ 1 $ 13 $ 9 $ KP $ • The types of NEs collected here are much more accurate than the four basic types defined in MUC .
Models $ 2 $ 27 $ 7 $ KP $ It can be shown ( Della Pietra et al . , 1995 ) that these are the also the values which minimize the Kullback-Liebler divergence D ( p[[q ) between the model and the reference distribution under the constraint that the expectations of the features ( ie , the components of f ) with respect to the model must equal their expectations with respect to the empirical distribution derived from the training corpus .
Selection_of_candidate_strings $ 2 $ 25 $ 1 $ KP $ Of the 85 ,135 words in our system's dictionary , 9217 of them are monosyllabic , 47778 are disyllabic , 17094 are m-syllabic , and the rest has four or more characters .
The_Feasibility_of_the_STL $ 3 $ 110 $ 68 $ CRF $ For example , if Di imposes a uniform distribution , then DI ( x ) = 1/emax where every sentence expresses at least 1 parameter and emax/TERM is the/DEF maximum number of parameters expressed by any sentence ./O
_Features $ 1 $ 150 $ 79 $ CRF $ Figure 7 shows that if there are hidden words , the language model accuracy dropped from 60 % to 25 % .
Interclausal_Coherence $ 4 $ 128 $ 38 $ PATTERN $ The Panasonic/TERM LC90S/TERM is a/DEF 19"-display ./O
The_MATE_Markup_Framework $ 3 $ 177 $ 89 $ PATTERN $ Example : The declaration Occursln : href ( lxanscription , u ) allows an attribute used as , e .g . , Occursln="base~_123" , where base is a coding file using the transcription module and u_123 is the value of the id attribute of a t~ element in that file .
Discussion $ 4 $ 108 $ 2 $ PATTERN $ A marked difference is that in RISE , the rules are the instances in kNN classification ( and due to the careful general±sat±on strategy of RISE , they can be very instance-specific ) , while in RBM , the rules are the features by which the original instances are indexed .
Robust_Name_Finding $ 3 $ 96 $ 37 $ CRF $ A closely related statistical approach to named entity tagging specifically targeted at speech data was developed at Sheffield by Gotoh and Renals ( 2000 ) .
Towards_building_a_parallel_corpus $ 2 $ 66 $ 34 $ PATTERN $ Assume now that it is the task of an MGEN system to produce from a knowledge base texts ( 4 ) and ( 6 ) .
The_model $ 1 $ 38 $ 15 $ KP $ These morphemes were then linearized in the second network .
Theoretical_Ideas , $ 2 $ 59 $ 34 $ KP $ The PSA will primarily be controlled by voice commands through a hand-held or head-mounted microphone , with speech and language processing being handled by an offboard processor .
Complexity $ 3 $ 95 $ 12 $ KP $ The DM knows about ( and thus can treat or react on ) M different semantic objects .
Abstract $ 0 $ 73 $ 12 $ PATTERN $ The process of extension/TERM simply consists of deriving/DEF a more elaborate form with a richer meaning using the generator's linguistic resources --/O it is useful to think of obtaining this by carrying out a step of derivation in a lexicalized grammar ( Stone and Doran , 1997 ) --and then consulting the model of the context to obtain an updated interpretation .
Bridging_Natural_Language_and $ 4 $ 81 $ 2 $ CRF $ Despite its limitations , a finite-state grammar seems to provide the best natural language model for information retrieval purposes .
_Annotation_Guidelines_I : $ 3 $ 63 $ 17 $ CRF $ GP/TERM : A/DEF GP/ is a phrase headed by locational noun or locational adjunct ./O
Complexity_Formulas $ 4 $ 68 $ 12 $ PATTERN $ The object complexity of object j is the sum of all its attributes' complexities : OC°bj$ "~E ACatt~ ,obji i A simple sum is used because identifying one object uniquely corresponds to knowing each of its attributes .
OUT $ -1 $ 63 $ 37 $ KP $ It is based on string edit distance between the output of the generation system and the reference corpus string .
Acquisition_Process $ 4 $ 101 $ 2 $ PATTERN $ GermaNet/TERM is the/DEF German counterpart to the well known WordNet ./O
Human_Annotation_of $ 2 $ 66 $ 1 $ KP $ We measured stability/TERM ( the/DEF degree to which the same annotator will produce an annotation after 6 weeks )/O and reproducibility/TERM ( the/DEF degree to which two unrelated annotators will produce the same annotation )/O , using the Kappa/TERM coefficient/TERM K/TERM ( Siegel and Castellan , 1988; Carletta , 1996 ) , which controls/DEF agreement P ( A ) for chance agreement P ( E ) : K = P{A ) -P ( E ) 1-P ( Z ) Kappa/O is 0 for if agreement is only as would be expected by chance annotation following the same distribution as the observed distribution , and 1 for perfect agreement .
MIMIC's_Concept-to-Speech $ 4 $ 60 $ 23 $ PATTERN $ INFERRABLE information is not explicitly exchanged between the system and 2In the examples , small capitalization denotes a word is accented .
Discussion $ 5 $ 203 $ 27 $ PATTERN $ ) Presumably , the quality of most generation systems can only be assessed at a system level in a task/Oriented setting ( rather than by taking quantitative measures or by asking humans for quality assessments ) .
Treebank_Representation $ 4 $ 54 $ 0 $ PATTERN $ A folded/TERM treebank/TERM is a/DEF representation of a set of parse trees which allows an immediate assessment of the effects of inhibiting specific rule combinations ./O
Discussions $ 6 $ 105 $ 36 $ PATTERN $ They are : a ) dis'=-deglndis dis b ) dis' deg • dis c ) dis' /deg d ) dis' = -dis In deg Where dis'/TERM denotes the/DEF revional distance and/O dis/TERM denotes the/DEF original distance ./O
OUT $ -1 $ 165 $ 160 $ CRF $ We pick as the answer to the question the sentence whose feature vector was classified positive with the highest confidence , or in the absence of such , the sentence classified negative with the lowest confidence .
Collaborative_Agents $ 1 $ 103 $ 99 $ PATTERN $ " "What is the workPhone?
Utterance_Units $ 4 $ 39 $ 0 $ CRF $ In the transcription , an utterance/TERM is defined as a/DEF continuous speech region delimited by pauses of 400 msec or longer ./O
Definitions $ 2 $ 37 $ 12 $ PATTERN $ A corpus/TERM position/TERM for a corpus C is a/DEF tuple ( j ,k ) , meaning the k th symbol in the jtb string in the corpus , with the restrictions : 1 _< j _<[ C[and 0 _< k _<[ CU] [ ./O
Abstract $ 0 $ 7 $ 5 $ KP $ AE aims at retrieving those sentences from documents that contain the explicit answer to a user query .
Conclusion $ 6 $ 140 $ 0 $ CRF $ In this paper we described a novel language model of incorporating long-distance lexical dependencies based on context co/Occurrence vectors .
Discourse_coherence_and $ 1 $ 22 $ 14 $ PATTERN $ In many NLG systems , aggregation/TERM is a/DEF post planning process whose preferences are only partially taken into account by the text planner ./O
Introduction $ 1 $ 28 $ 21 $ PATTERN $ Adding redundancy to a signal before transmission is a well-known technique in digital communication to allow for the recovery of errors due to noise in the channel , and this is the key to the success of ECOC .
Conceptualizing_Events $ 2 $ 108 $ 78 $ CRF $ This could be done by a proposition like turn ( ok314 , goal ( tw-echo ) ) , which is a potential increment for a preverbal message .
Evaluation_Measures $ 2 $ 38 $ 13 $ KP $ The ranks assigned by an evaluation measure produce equivalence classes of extract summaries; each rank/TERM equivalence/TERM class/TERM contains/DEF summaries which received the same score ./O
Implementation $ 3 $ 86 $ 4 $ CRF $ It offers all the classical functions for text edition plus a pop-up menu which contains the more probable words or sequences of words that may complete the ongoing translation .
Learning_RRE_Rules $ 4 $ 67 $ 14 $ PATTERN $ Say we have a training corpus C . For every string C[j]~ C , Tmth[C[j]] ~ {0 ,1 } is the true label of C[j] and Guess[C[j]] is the current guess of the label of C[j] .
OUT $ -1 $ 0 $ 0 $ KP $ Generation from Lexical Conceptual Structures David Traum and Nizar Habash UMIACS , University of Maryland {traum , habash}@cs , umd .
Evaluation $ 4 $ 95 $ 7 $ CRF $ But interestingly this practical ew~luation confirmed our theoretical evaluation that atranslation can be produced with TRANSTYPE by typing less than 40 % of the characters of a translation .
Comparing_Three_Treebank $ 4 $ 77 $ 10 $ KP $ NP ) V@ NP!
Abstract $ 0 $ 2 $ 1 $ CRF $ This paper realizes a practical system for Chinese parsing by using a hybrid model of phrase structure partial parsing and dependency parsing .
Resolution_Procedures $ 2 $ 37 $ 6 $ KP $ A TMR/TERM includes/DEF , among other representational objects , instantiations of object types , relation types and property types ./O
Abstract $ 0 $ 153 $ 122 $ PATTERN $ ' From the above tagging , we can obtain the following discourse structure with embedding relations : A dversativity ( & F ( 14 ) , Sufficiency ( F rontClause ( 15 ) , BackClause ( 15 ) ) ) where &F/TERM (/TERM n/TERM )/TERM denotes the/DEF Front discourse segment of an inter-sentence rhetorical relation whose sequence number is n ./O We can define &B ( n ) similarly .
Supervised_Training $ 2 $ 61 $ 21 $ KP $ with tf/TERM ( t , d ) > 1 ~re t is an/DEF estimate of the total number of relevant where : D ( description ) , E ( query expansion ) documents ./O
Introduction $ 1 $ 37 $ 33 $ CRF $ However , as higher values of k lead to an enormous number of possible rules , huge data sets would be necessary in order to have a reliable estimate of the probabilities for values above k = 3 .
Architecture_and_Principles $ 3 $ 38 $ 15 $ CRF $ The third structure is a list marked by bullets .
Acquisition_Process $ 4 $ 142 $ 43 $ PATTERN $ Unfortunally the stem reference contained in the ontology points to the concept ale , which is a sub concept of alcoholic beverage ) , in this case the stem reference is reassigned to the dictionary concept .
Abstract $ 0 $ 6 $ 5 $ KP $ Introduction Verb subcategorizafion probabilities play an important role in both computational linguistic applications ( e .g .
Introduction $ 1 $ 48 $ 39 $ PATTERN $ A "meta-chain"/TERM is a/DEF representation of every possible lexical chain that can be computed starting with a word of a given sense ./O
Setting $ 3 $ 93 $ 14 $ CRF $ The topical context is formed by Cl/TERM ,/TERM .../TERM ,/TERM Cm/TERM , which stand/DEF for the unordered set of open class words appearing in the sentence 7/O .
Introduction $ 1 $ 15 $ 10 $ CRF $ In the QA track , each participant is given a list of 200 questions , and the goal is to locate answers to these questions from a document database consisting of hundreds of thousands of documents ( about two gigabytes of text ) .
Background : _The_STOP_System $ 2 $ 17 $ 0 $ KP $ The STOP/TERM system/TERM generates/DEF personalised smokingcessation leaflets , based on the recipient's responses to a questionnaire about smoking beliefs , concerns , and experiences ./O
Multiple_heuristics_for_word_sense $ 2 $ 25 $ 0 $ KP $ disambiguation As the mapping method described in this paper has been developed for combining multiple individual solutions , each single heuristic/TERM must be seen as a/DEF container for some part of the linguistic knowledge needed to disarnbiguate the * This/O research was supported by KOSEF special purpose basic research ( 1997 .92000 .8 #970-1020-301-3 ) Corresponding author 142 ambiguous WordNet synsets .
Hyperonymy_in_lexical_semantics $ 5 $ 131 $ 34 $ CRF $ 2 • Given an activated concept , which more general lexical items are considered in tile choice process ; are there any restrictions on .-lexical inheritance?
Conclusions $ 6 $ 137 $ 0 $ CRF $ The use of error-correcting/TERM output/TERM codes/TERM ( ECOC/ACR ) for representing natural language classes has been empirically validated for a suite of linguistic tasks .
Results $ 5 $ 116 $ 19 $ KP $ The merged model , on the other hand , does not reach this peak , but overfitting is not present .
Abstract $ 0 $ 16 $ 15 $ KP $ Summarization-based algorithms for text categorization are outlined in Section 3 .
Introduction $ 1 $ 9 $ 3 $ CRF $ This is the reason for many researchers having focused on massive acquisition of lexical knowledge and semantic information from pre-existing lexical resources as automatically as possible .
Abstract $ 0 $ 9 $ 4 $ KP $ This algorithm has been implemented in an experimental NLG program using ProFIT .
_Features $ 1 $ 74 $ 3 $ CRF $ The recall/TERM is the/DEF number of errors identified by a particular feature divided by the total number of errors ./O
Tagged_Text $ 3 $ 177 $ 163 $ KP $ For ANFIS , the set of sentence-question pairs was divided into five groups according to question type .
Knowledge_base $ 5 $ 104 $ 18 $ CRF $ • Matching of the depth of the phrases in parse trees : 1 point • Matching of the type of the phrases ( phrase types differ depending on surface cases and verb conjugations , etc ) : 1 point user question are summed up and normalized by the maximum/TERM matching/TERM score/TERM (/TERM MMS/TERM )/TERM as follows ( the/DEF MMS is the similarity score with the same sentence )/O : The sum of scores of~ 2 phrase similarities ] The MMS of ~ ( The MMS of~ the user question] × \the KU case] The above score is given to the KU as its certainty score .
The_REXTOR_System $ 5 $ 135 $ 13 $ PATTERN $ The template/TERM consists/DEF of a series of legal tokens ,/O which are shown in Table 1 .
Problem_Space_Modeling $ 2 $ 47 $ 20 $ CRF $ One good indication of such importance , for example , in the Acyclic Problem Graph , is the branch factor of each node .
Experimental_setting $ 2 $ 63 $ 1 $ CRF $ Section 3 ) , Yarowsky ( 1993 ) used a measure of entropy as well as the results obtained when tagging heldout data with the collocations organized as decision lists ( el .
Qualitative_Evaluation_of_the $ 4 $ 136 $ 35 $ CRF $ \Ve proposed a mmlber/Of'models : in which various conLfrom the two tree models binations of intrinsic metrics were used to predict user judgments of understanding and quality .
Framing_the_generation_problem $ 3 $ 66 $ 16 $ PATTERN $ We can assume that houses and their rooms are hearer-new until REA describes them; and that just those entities mentioned in the prior sentence are in-focus .
Abstract $ 0 $ 3 $ 1 $ PATTERN $ Preliminary results 1 across three language-specific FALCon 2 systems show that , with one exception , the derived measures consistently yield the same performance ranking : Haitian Creole at the low end , Arabic in the middle , and Spanish at the high end .
Conclusions $ 6 $ 141 $ 4 $ KP $ A voting algorithm for learning blocks of bits proves as accurate as an expensive feature-selecting algorithm .
Computing_referring_expressions $ 4 $ 152 $ 2 $ PATTERN $ The problem is the subject of this section .
ALLiS $ 3 $ 20 $ 0 $ CRF $ ALLiS/TERM (/TERM Architecture/TERM for/TERM Learning/TERM Linguistic/TERM Structures/TERM )/TERM ( D~jean , 2000a ) is a/DEF symbolic machine learning system which generates categorisation rules from a tagged and bracketed corpus ./O
Example_Dialogue $ 3 $ 34 $ 11 $ PATTERN $ None of the above ( U3 ) U : none ( U4 ) S : "retrieve all messages from bob that were sent after christmas" is a way to express : 1 . move mail 2 . list mail 0 .
OUT $ -1 $ 232 $ 42 $ KP $ Both these attributes refer to an orthographic transcription , wref delimits the word ( s ) which caused or might cause a communication problem , and uref refers to one or more entire utterances which caused or might cause a problem .
Building_Spoken_Dialo~te_Systems $ 4 $ 112 $ 7 $ CRF $ ( case head ) It means that the case feature is used and it is a head feature 3 .
Introduction $ 1 $ 10 $ 4 $ KP $ Robust parsing is often achieved by combining a full parser with a partial parser and fragment-combining rules , but even then some utterances may be correctly recognized , only to be parsed incorrectly or not at all .
_The_identity_of_the_speaker ,_denoted_as_the $ 6 $ 161 $ 51 $ CRF $ Another possible extension involves the inclusion of the speech topic .
Comparative_analysis_~of_Japanese_and $ 3 $ 102 $ 14 $ PATTERN $ However , in Chinese , gl~/TERM represents postal/DEF stamp and/O the constituent/TERM characters/TERM represent "postal"/DEF and "ticket" , respectively ./O
_Generation_of_the_surface_phrase_from_the $ 4 $ 76 $ 39 $ PATTERN $ Task-based/TERM evaluation/TERM in general consists/DEF of the following three steps : ( l ) Data preparation : Assume an information need , create a query for the information need , and prepare simulated search results with different types of summaries ./O
OUT $ -1 $ 112 $ 70 $ KP $ Considering chunk tags within a contextual window of the target word raises a problem with C4 .5 .
The_model $ 1 $ 93 $ 70 $ KP $ The output of NN 11 is now a linear" structure where we Iozou , the phrase types .
The_Verbmobil_treebanks $ 2 $ 47 $ 13 $ PATTERN $ A sample annotation conformant to the Verbmobil annotation scheme is the annotation of ( 1 ) shown in Fig .
_INTRODUCTION $ 1 $ 20 $ 14 $ CRF $ For instance , identification of names of Chinese people is very much relied on the surnames , which is a limited set of characters .
Discussion $ 4 $ 111 $ 5 $ PATTERN $ Second , the method is a potentially helpful extension to memory-based learning of language processing tasks .
Introduction $ 1 $ 11 $ 7 $ CRF $ The objective of standardizing efforts in discourse is to promote interactions among discourse researchers and thereby provide a solid foundation for corpus-based discourse research , dispensing with duplicating resource making efforts and increasing sharable resources .
Architecture_of_WIT-Based_Spoken $ 3 $ 35 $ 1 $ CRF $ 3 .1 Speech Recognition The speech/TERM recognition/TERM module/TERM is a/DEF phonemeHMM-based speaker-independent continuous speech recognizer that incrementally outputs face Toolldt ./O
OUT $ -1 $ 115 $ 114 $ KP $ The opposing evidence ( i .e . , ContrastingSubObjectives ) , that must be considered , but not in detail , is also expressed in natural language .
OUT $ -1 $ 111 $ 93 $ PATTERN $ For example , the question "What is the name of the creek?
The_Generation_System $ 3 $ 83 $ 41 $ PATTERN $ On the other hand , ambiguity can also be introduced at the decomposition stage , if multiple lexical entries can match a single structure The result of the decomposition process is a match-structure indicating the hierarchical relationship between all lexical entries , which , together cover the input CLCS .
_Our_Classification_Algorithm_for $ 2 $ 64 $ 0 $ KP $ Verbg Situation 2 .1 Guiding Thoughts ( 1 ) Our algorithm is for information processing eMa[5] uses three pairs of phase features in classifying , but from which we can not get an automatic classification algorithm for computers; the classification can only be done manually .
The_lexicon_and_its_benefits_to $ 3 $ 40 $ 11 $ PATTERN $ An alternation is a variation in the realization of verb arguments .
Statistics_Based_Hybrid_Approach_to $ 2 $ 21 $ 3 $ CRF $ CutTenfly , we are considering 7 Chinese base phrases in our research , namely base/TERM adjective/TERM phrase/TERM ( BADJP/ACR ) , base/TERM adverbial/TERM phrase/TERM ( BADVP/ACR ) , base/TERM noun/TERM phrase/TERM ( BNP/ACR ) , 73 base/TERM temporal/TERM phrase/TERM ( BTN/ACR ) , base/TERM location/TERM phrase/TERM ( BNS/ACR ) , base/DEF verb/TERM phrase/TERM ( BVP/ACR ) and base/TERM quantity/TERM phrase/TERM ( BMP/ACR ) Though theoretically definitions for these base phrases are still unavailable , Appendix I lists the preliminary illustrations for them in BNF format ( necessary account for POS annotation can also be found ) . . To frame the identification of Chinese base phrases , we fm'ther develop the following concepts : Definition 1 : Chinese/TERM based/TERM phrases/TERM are recognized as atomic/DEF parts of a sentence beyond words that posses certain functions and meanings ./O
Algorithms_and_Implementation $ 2 $ 26 $ 6 $ PATTERN $ The base/TERM model/TERM defines the/DEF distance between a test item and each memory item as the number of features for which they have a different value ./O
System_Design $ 3 $ 73 $ 22 $ KP $ 33 Korean Documents Parser Tagged l Korean Documents ( LexiconK°rean 1 ~ Syntactic . . . . . . Eaglish Grammar Structure ( English ) RealPro English Lexicon / ' S~'ntactic Realizer Sentence ( English ) t Parsed Document ~ : : i~i°~'~vii~i?
Analyst_Scenario $ 2 $ 29 $ 10 $ KP $ Korean Source Report Et-~ "~I-DlXi'~ ~oo ~ Cll~o" oj_a , xd~ .
Rhetorical_structure_and $ 2 $ 24 $ 0 $ KP $ text structure To distinguish clearly between FthetRep and DocRep , we need to define the kinds of information that should be included in the two representations .
Abstract $ 0 $ 12 $ 11 $ KP $ Phrasal terms are utilized either as replacement of single words or as supplemental units for single words , but according to our experience , phrasal terms as replacement of single words do not perform well .
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 46 $ 11 $ PATTERN $ Let us consider further a broad coverage domain which consists of a small number of sanaple news documents about the same topic , 'Kobe Japan quake' .
Abstract $ 0 $ 61 $ 5 $ CRF $ For instance , many systems have a referring expression generation module whose task is to complete a semantic representation which lacks those structures which will be realised as NPs .
Clustering_into $ 5 $ 75 $ 6 $ KP $ Speech and language technology researchers have used wordbigram and n-gram models in speech recognition , and variants of PoS-bigram models for Part/Of-Speech tagging .
OUT $ -1 $ 79 $ 33 $ KP $ Forty-nine of the OCR-ed " words " are treated as " not/DEF found words "/O ( NFWs/TERM ) by the MT engine , even though they may in fact be actual Spanish words .
MIMIC's_Concept-to-Speech $ 4 $ 82 $ 45 $ PATTERN $ " denotes a downstepped accent ( see ( Pierrehumbert , 1980 ) ) .
Applications $ 5 $ 135 $ 2 $ CRF $ We can indicate which topic is from which text or even which block of a text .
Comparison_experiment $ 3 $ 170 $ 72 $ CRF $ For each question k we obtained three sets VKm .k , VKXS , k and VKCS , k of ( pos , assessment ) pairs corresponding to the three search methods , where pos/TERM is the/DEF position • of the document in the ordered list returned by the search method ,/O and assessment/TERM is the/DEF assessment of one participant ./O
Constraint_Satisfaction_with $ 3 $ 68 $ 9 $ PATTERN $ A natural cost function is to use the classifiers probabilities P ( o ) and P ( c ) and define , for a phrase e = ( o , c ) , c ( e ) = 1 P ( o ) P ( c ) which means that the error in selecting e is the error in selecting either o or c , and allowing those 108 to overlap 4 .
OUT $ -1 $ 102 $ 84 $ PATTERN $ Rule #1 is the generic word matching function shared by all question types .
DTD_abstraction $ 2 $ 58 $ 6 $ CRF $ ( Ahonen , 1995 ) uses a method to build document instances from tagged texts that consists of a deterministic finite automaton for each context model .
Introduction $ 1 $ 44 $ 36 $ PATTERN $ The fact that HowNet has verified this thesis over 65 ,000 concepts is a good proof of its robustness .
Building_a_reusable_lexical_chooser $ 2 $ 17 $ 0 $ KP $ for generation While reusable components have been widely used in generation applications , the concept of a " reusable lexical chooser " for generation remains novel .
Introduction $ 1 $ 12 $ 5 $ KP $ NLG researchers have addressed this issue in various ways , but everyone assumes some kind of structural compatibility between rhetorical structure and text structure .
Conclusions_and_Future_Work $ 5 $ 272 $ 16 $ PATTERN $ When human-generated ground truths are available , perhaps some combination of recall and the content-based measures could be used .
Stochastic_Surface_Realization $ 2 $ 89 $ 31 $ PATTERN $ There is the problem , as in speech recognition using n-gram language models , that long-distance dependency cannot be captured .
_Tagging_NuLL-marker_CDM_pairs_ ( via $ 8 $ 122 $ 52 $ PATTERN $ Rule 22 : ( 1 , lift 3 .4 ) B2 = p Fcom > 1 class 2 [0 .667] which can be explained as : if the second word after the RDM is a preposition , and there is more then one commas before the current RDM , then the location of the NULL marker is two commas away from the RDM .
Generation_of_Vague_Descriptions $ 4 $ 92 $ 11 $ PATTERN $ The result would be a new list L = {yellow ,chihuahua ,largestl} , where 'largestt'/TERM is the/DEF property 'being the unique largest element of C' ./O
The_REXTOR_System $ 5 $ 122 $ 0 $ CRF $ Using its finite-state language model , the REXTOR/TERM System/TERM generates/DEF a set of ternary expressions that correspond to content of a part/Of-speechtagged input document ./O
_Introduction $ 1 $ 13 $ 7 $ PATTERN $ Section 2 first gives a general outline of the trainable method we have defined to extract Chinese entity names and their relations , then describes person name extraction , entity name classification and relation extraction in detail .
Abstract $ 0 $ 7 $ 2 $ KP $ The individual FB1 scores for NPs were 92 .19% , VPs 92 .70% and PPs 96 .69% .
_Features $ 1 $ 73 $ 2 $ KP $ We use the recall and precision measurements for evaluation .
Statistical_Semantic_Parsing $ 4 $ 100 $ 1 $ KP $ Given a sentence I • Sentences , the set Q ( 1 ) = {q • Queries I ( l , q ) • Parser} is the set of queries that are correct interpretations of I .
Conclusion $ 9 $ 204 $ 22 $ PATTERN $ Here is the latest document summary .
Acquiring_Lexical_Translations $ 3 $ 64 $ 13 $ PATTERN $ A/TERM string/TERM in/TERM a/TERM bilanguage/TERM corpus/TERM consists/DEF of sequences of tokens where each token ( wi-xi ) is represented with two components : a source word ( ]possibly an empty word ) as the first component and the target word ( possibly an empty word ) that is the translation of the source word as the second component ./O
Interaction_Grammars $ 3 $ 99 $ 46 $ KP $ fra : : country --> [] .
The_results_reported_in_this_paper_are_part_of_a_more $ 1 $ 40 $ 29 $ PATTERN $ Section 4 describes and discusses the results obtained , while in the conclusions we propose some directions for future work .
Automatic_Extraction_of $ 4 $ 63 $ 12 $ CRF $ The rule extraction scope is limited to the end of a sentence or , if there is a conjunctive/TERM ending/TERM ( eCC/ACR ) in the sentence , only to the conjunctive ending of the sentence .
Word_Co/Occurrence_Vector $ 2 $ 49 $ 17 $ PATTERN $ The i : j-th element denotes the number of documents in which both words w , : and wj appear , F ( wi , wj ) ( Figure 2 ) .
Previous_Work $ 3 $ 74 $ 20 $ CRF $ Similarly , PLNLP/TERM ( Heidorn , 1972 ; Jensen et al . , 1993 ) is a/DEF programming language for writing phrase structure rules that include specific conditions under which the rule can be applied ./O
Knowledge_Representations $ 2 $ 73 $ 42 $ PATTERN $ For example , if the user's first query were " I want to go to Denver next Friday morning , returning the following Wednesday , " the system would record that this is a round trip flight and would save the return date ( unresolved , in case there was a recognition error on the forward leg date ) in the user model .
Evaluation $ 3 $ 189 $ 0 $ PATTERN $ In the evaluation , the training data is the PH corpus and the test data is the YZZK magazine articles ( 4+ Mbytes ) , downloaded from the Internet .
Related_work $ 6 $ 165 $ 16 $ PATTERN $ In Wirth's work the resolvent of the PPT represents the partial proof and a more general purpose metainterpreter is used .
Abstract $ 0 $ 56 $ 52 $ CRF $ We suspect that some of these errors may be systematic due to the properties of the language model used or due to language specific properties .
Support_Vector_Machines $ 2 $ 19 $ 13 $ CRF $ SVMs/TERM can be regarded as an/DEF optimization problem ; finding w and b which minimize [ [ w[ [ under the constraints : yi[ ( w • xi ) + b] > 1 ./O
_The_co-reference_problem_in_summarization $ 4 $ 55 $ 31 $ CRF $ Following is a list of requirements for multi-document summarization : • clustering/TERM : The/DEF ability to cluster similar documents and passages to find related information ./O
The_following_formulas_summarize_the_relations $ 9 $ 171 $ 90 $ PATTERN $ The cross-system comparisons using the measures presented , with one exception , yielded the following expected rankings : ( i ) the GT-MT pass exhibits better performance than the ScanOCR-MT pass and ( ii ) the Haitian Creole system is at the low end , Arabic is in the middle , and Spanish is at the high end .
Informational_content_of_sentences $ 2 $ 24 $ 0 $ KP $ Cluster-based/TERM sentence/TERM utility/TERM ( CBSU/TERM , or utility ) refers to the/DEF degree of relevance ( from 0 to 10 ) of a " particular sentence to the general topic of the entire cluster (/O for a dis cussion of what is a topic , see [Allan et al.
Introduction $ 1 $ 27 $ 21 $ PATTERN $ 'instance ~-to a set of alternatives C : the adverb describes a property that makes ~ unique in C . Thus in ( 3a ) Mary is unique among some set C of individuals in passing .
OUT $ -1 $ 87 $ 33 $ KP $ ( Sugar maple trees )
OUT $ -1 $ 130 $ 59 $ KP $ 27 z~ejm~ stay Sb , V ) zmi~nit Figure 4 : The DMCS of the sentence from Figure 2 .
Introduction $ 1 $ 29 $ 24 $ CRF $ He selects the Mann-Whitney/TERM test/TERM that : uses/DEF ranks of frequency data rather than the frequency values themselves to compute the statistic ./O
Multiple_heuristics_for_word_sense $ 2 $ 39 $ 14 $ CRF $ Therefore we will give maximum score to the synset of a monosemous translation , that is , the translation which has only one corresponding synset .
OUT $ -1 $ 198 $ 72 $ CRF $ Wiebe ( 1994 ) also reports segment-based agenthood as one of the most successful features .
Reference_Resolution $ 4 $ 131 $ 32 $ CRF $ # .2.1 Explicit pronouns The basic form-based strategy for resolving pronominal reference is to begin by inspecting in reverse order of mention those referring expressions whose forms are compatible with the morphological constraints imposed by the pronominal .
Principles_and_Parameters $ 1 $ 7 $ 0 $ KP $ Chomsky ( 1981 ) ( and elsewhere ) has proposed that all/DEF natural languages share the same innate universal principles (/O Universal/TERM Grammar/TERM -UG/ACR ) and differ only with respect to the settings of a finite number of parameters .
Abstract $ 0 $ 147 $ 14 $ PATTERN $ Section 2 describes HuDL in greater detail and section 3 discusses system integration and the IMA .
Hyperonyms_in_NLG_systems $ 4 $ 72 $ 13 $ PATTERN $ The latter are explicitly represented in his system as a . list/TERM of/TERM attributes/TERM 'to communicate about an entity' , which is a/DEF subset of the overall knowledge the system has of that entity ./O
Learning_Phrase-based_Variable $ 4 $ 74 $ 5 $ CRF $ state recognizes a symbol wi E lZU { e } , where e/TERM is the/DEF empty string ./O
Conclusion $ 9 $ 198 $ 16 $ CRF $ Here is an overview of the chronology of events .
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 251 $ 216 $ PATTERN $ Each of the documents is flagged as to whether it discusses the target event , and these flags ( 'YES' , 'NO' ) are the only information used tbr training the system to correctly classiC " the target event .
OUT $ -1 $ 61 $ 35 $ CRF $ The first metric , simple/TERM accuracy/TERM , is the/DEF same string distance metric used for measuring speech recognition accuracy ./O
Learning_Verb_Rules $ 3 $ 91 $ 42 $ CRF $ The meanings of non/TERMinals used in the rule are following : 221 be/TERM (/TERM )/TERM represents auxiliary/DEF verb b~t ,/O cond/TERM (/TERM )/TERM represents various/DEF forms of conditionals by , aby , kdyby ,/O reflex_pron/TERM (/TERM )/TERM stands for reflexive/DEF pronoun se ( si ) ,/O gap/TERM (/TERM )/TERM is a/DEF special predicate for manipulation with gaps ,/O and k5/TERM (/TERM )/TERM stands for arbitrary/DEF non-auxiliary verb ./O
Extending_a_Grammar_to_Enable $ 4 $ 84 $ 11 $ PATTERN $ The top text in a rectangle specifies a slot name , and the bottom text is the name of a template ~kssigned to this slot , .
Introduction : $ 1 $ 16 $ 11 $ CRF $ In this paper , we consider the latter approach using sample selection , an interactive learning method in which the machine takes the initiative of selecting potentially beneficial training examples for the humans to annotate .
The_remaining_pronouns_are_not_in_third_person_or $ 9 $ 284 $ 11 $ PATTERN $ These problems are the generation of intersentential anaphora , the detection of coreference chains and the generation of Spanish zero-pronouns into English .
Abstract $ 0 $ 1 $ 0 $ KP $ RSTTool/TERM is a/DEF graphical tool for annotating a text in terms of its rhetorical structure ./O
Abstract $ 0 $ 2 $ 1 $ KP $ An XML/TERM document/TERM is a/DEF mixture of structure ( the tags ) and surface ( text between the tags ) ./O
Abstract $ 0 $ 2 $ 1 $ KP $ Text/TERM meaning/TERM representation/TERM is composed of a/DEF set of ontological concept instances along with ontological links among them ./O
Introduction $ 1 $ 7 $ 3 $ KP $ The Deep/TERM Read/TERM reading/TERM comprehension/TERM prototype/TERM system/TERM ( Hirschman et al . , 1999 ) achieves/DEF a level of 36% of the answers correct using a bag/Of-words approach together with limited linguistic processing ./O
Note_that_the_nouns_at_this_point_are_types_not $ 4 $ 177 $ 62 $ CRF $ [ Och & Ney , 2000] The latter result has to be considered with caution in the present experimental design context since the evaluation of the alignments was done with a human translation on a closed domain corpus , for only one of the languages under consideration in the current investigation .
The_Structure_of_a_Relational $ 3 $ 96 $ 0 $ PATTERN $ 3 .3 Terminology In the following discussion , we will use the following terminology : * Predicate : each column of an entity file defines a predicate .
Method $ 2 $ 52 $ 7 $ PATTERN $ The filter is the only component of this system which we experiment with here .
OUT $ -1 $ 117 $ 75 $ KP $ Since C4.5 generates probabilities for each classification decision , they can be redirected into the input for the next position .
Experiments_and_Discussion $ 4 $ 133 $ 40 $ CRF $ Accuracy Table 4 shows the relationship between the dimension of the kernel function and the parsing accuracy under the condition k -5 .
Corpora $ 3 $ 60 $ 2 $ KP $ 3 .1 MUC-6 The/TERM corpus/TERM for/TERM MUC-6/TERM ( MUC , 1995 ) contains/DEF 60 articles , from the test corpus for the dry and formalruns ./O
OUT $ -1 $ 183 $ 129 $ PATTERN $ The ideal/TERM answer/TERM is a/DEF full sentence that contains the information given by the question and the information requested ./O
Context-dependent_Lexicons $ 3 $ 84 $ 1 $ KP $ In this section , we will attempt to add more contextual information to approximate P ( t i/G~ ) .
Experiments $ 6 $ 103 $ 8 $ KP $ For the RRE-based system , we mapped the +/5 word window of context into a string as follows ( where wi/TERM is a/DEF word and/O ti/TERM is a/DEF part of speech tag )/O : learned using the standard feature set .
Conclusion $ 7 $ 204 $ 1 $ PATTERN $ To our knowledge our system is the only one that uses semantic representation as basis for summarizing .
Abstract $ 0 $ 6 $ 3 $ CRF $ Regression analysis of the experimental results reveals that , in order for ECOC to be successful for language learning , the use of the Modified/TERM Value/TERM Difference/TERM Metric/TERM ( MVDM/ACR ) is an/DEF important factor , which is explained in terms of population density of the class hyperspace ./O
SYSTEM : _U_S-air_flight_63_57_departs_la $ 9 $ 125 $ 118 $ PATTERN $ Also , the tools presented here tend to reduce the growth in code size with complexity ( as measured by the number of possible constraints ) .
_Instrumentalists_not_including_string_players $ 8 $ 114 $ 102 $ PATTERN $ A group of musicians playing popular music for dancing 14 ( b ) ( c ) ( d ) Sense 1 is a specialization of Sense 2 , and this pattern is repeated in French and German .
Introduction $ 1 $ 6 $ 2 $ CRF $ A venture capitalist who wants to invest in an MT start-up needs to know a different set of attributes about the system than does a developer who needs to see if the most recent software changes improved ( or degraded ) the system .
Results_and_Discussion $ 5 $ 149 $ 18 $ PATTERN $ In addition , the fact that results are the same for both languages indicates that the method can smooth the coverage differences among the wordnets .
Abstract $ 0 $ 123 $ 28 $ KP $ For Encarta , the addition of [InformationStatus] yielded only a modest improvement in accuracy .
SYSTEM : _U_S-air_flight_63_57_departs_la $ 9 $ 21 $ 14 $ CRF $ A constraint on a data record is the condition that some given constraint function has a given value .
Introduction $ 1 $ 20 $ 14 $ PATTERN $ Section 4 discusses additional robustness techniques at the recognizer level , and Section 5 describes dialogue-level robustness techniques .
Abstract $ 0 $ 59 $ 2 $ CRF $ These types include : the explicit topic of the document , the situation , the identification of the problem , the 'identification of the solution , the research goal , the explicit topic of a section , the • authors ' development , the inferences , the description of a topical entity , the definition • of a topical entity , the relevance of a topical enthy , the advantages , etc .
Dependency_Analysis_using $ 3 $ 68 $ 5 $ CRF $ Dbest = argmax P ( D[ B ) D If we assume that the dependency probabilities are mutually independent , P ( DIB ) could be rewritten as : rn-1 P ( DIB ) = ~I P ( Dep ( i ) =j Ifit ) i=1 fit = { fl , . . . , fn } e R n . P/TERM (/TERM Dep/TERM (/TERM i/TERM )/TERM =/TERM J/TERM If0/TERM )/TERM represents the/DEF probability that bi depends on ( modifies ) b t ./O fit/TERM is an/DEF n dimensional feature vector that represents various kinds of linguistic features related with the chunks bi and b t ./O We obtain Dbest taking into all the combination of these probabilities .
The_CGS_system $ 4 $ 74 $ 6 $ CRF $ The output of the planner is a partially ordered plan with speech-acts as leaves .
Summary_and_Future_Work $ 5 $ 206 $ 3 $ PATTERN $ We believe that this is a direct consequence of the use of the dialogue control table .
Implementation $ 4 $ 240 $ 69 $ PATTERN $ Regarding the number of terms contained in one sentence as a constant , topic sentences are ext : racted in O ( skh ) time where s/TERM is the/DEF total number of sentences in the document set ./O
The_Argument_Generator $ 1 $ 20 $ 0 $ PATTERN $ The architecture/TERM of/TERM the/TERM argument/TERM generator/TERM is a/DEF typical pipelined architecture comprising a discourse planner , a microplanner and a sentence real izer ./O
Burstiness $ 3 $ 118 $ 11 $ CRF $ Kwok ( 1996 ) suggested average/TERM term/TERM frequency/TERM , avtf/DEF = TF ( t ) /df ( t ) ,/O be used as a tie-breaker for cases like this , where TF ( t ) = ~a if ( t , d ) is the standard notion of frequency in the corpus-based NLP .
Research_context : _hypertext $ 1 $ 46 $ 42 $ CRF $ The part inside the bold sub-frame is the input to the sentence planner Last year , the most atypical sales variations from one month to the next occurred for : ® Birch Beer with a 42 % national increase from September to October ; ® Diet Soda with a 40 % decrease in the Eastern region from July to August .
Robust_Name_Finding $ 3 $ 79 $ 20 $ PATTERN $ Finding names in speech data is a very new topic of research , and most previous work has consisted of the direct application of text-based systems to speech data , with some minor adaptations .
Sample_Selection $ 2 $ 43 $ 15 $ PATTERN $ Although there exist abundant collections of raw text , the high expense of manually annotating the text sets a severe limitation for many learning algorithms in natU is a Set of unlabeled candidates .
Introduction $ 1 $ 24 $ 18 $ KP $ The sentence " A nurse inspected each patient .
_Features $ 1 $ 104 $ 33 $ KP $ recall x precision ) of detecting language model errors by examining the logarithm conditional probabilities on the maximum likelihood path .
OUT $ -1 $ 107 $ 56 $ CRF $ A nor- mal embedding is one satisfying condition 1 , 3 and 4 and the embedded/TERM part/TERM is a/DEF relative clause which provides additional information about the referent ./O
The_Learning_System $ 2 $ 60 $ 34 $ PATTERN $ For the learner , the information about subjects ( subjdir = backward ) has already been acquired while learning intransitive verbs , and the learner does not need to learn it again for transitive verbs , which not only inherit this information , but also have the direction for the object defined by vargdir ( vargdir = forward ) , as shown in figure 3 .
Towards_building_a_parallel_corpus $ 2 $ 53 $ 21 $ CRF $ Each node is characterized by a status ( NUCLEUS or SATELLITE ) and a rhetorical/TERM relation/TERM , which is a/DEF relation that holds between two non/Overlapping text spans ./O
Memory-Based_Language $ 1 $ 7 $ 1 $ KP $ These exemplars take the form of a vector of , typically , nominal features , describing a linguistic problem and its context , and an associated class symbol representing the solution to the problem .
Vector_Space_Model : _Western_and_Asian $ 4 $ 135 $ 4 $ PATTERN $ The vector/TERM simply consists/DEF of an ordered list of terms ,/O and therefore , the contextual cues have also disappeared .
Abstract $ 0 $ 7 $ 3 $ KP $ Bayesian , decision tree and neural network classifiers ) to discover language model errors .
Tagged_Text $ 3 $ 70 $ 0 $ KP $ 2 .2 .1 The Lexicon There were two methods we used to construct the lexicon : open/TERM lexicon/TERM , which includes/DEF all words from the development set along with all determiners , pronouns , prepositions , particles , and conjunctions (/O these words are essential to achieving good sentence segmentation ) , and closed/TERM lexicon/TERM , which includes/DEF all of the development and testing words 2 ./O
Statistical_Semantic_Parsing $ 4 $ 141 $ 42 $ PATTERN $ hk does not cover s ) , we have PCai ( s ) • OS 7 " I hk ) -p " + 8 .n , , Pu +nu ( 15 ) where Pu and nu are the n , ,rnber of positive and negative examples rejected by hk respectively .
_Annotation_Guidelines_I : $ 3 $ 52 $ 6 $ CRF $ This set defines the domain of expressed syntactic information ( instead of projected or inherited information ) .
Abstract $ 0 $ 11 $ 9 $ PATTERN $ Obviously , the latter is a very expensive task since there are so many documents in a collection and there is not yet a reliable machine translation system that can be used to process automatically .
Dialogue_Structure_and $ 6 $ 95 $ 10 $ PATTERN $ The DS/TERM tag/TERM consists/DEF of a topic break index ( TBI ) , a topic name and a segment relation ./O
October_2000 $ 7 $ 17 $ 16 $ KP $ Thus , similarity such that a part/Of-speech tagger developed for one corpus works well in the other , may differ from similarity for Machine Translation .
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 78 $ 43 $ PATTERN $ In Document level , there is a small number of sample news documents about the same topic .
_Decision_trees $ 4 $ 76 $ 11 $ CRF $ Figure 2 gives a simpler decision tree that predicts the grammatical relation of a mention for Enearta at variety of kernel functions .
Task-inherent_and_Technical $ 2 $ 40 $ 11 $ CRF $ For example , there is a separate module on the level of speech recognition which deals with hesitations and self-corrections .
Feature_merging_and_overfitting_reduction $ 3 $ 57 $ 0 $ KP $ The idea behind feature/TERM merging/TERM is to/DEF reduce overfitting through changes made directly to the model ./O
Verb_Frequency $ 2 $ 45 $ 0 $ KP $ Because word frequency is known to vary with corpus genre , we used the frequency differences for our target verbs as a measure of corpus difference .
Acquisition_Process $ 4 $ 102 $ 3 $ CRF $ Presently it builds a lexical semantic network for 16 .000 German words , where three different types of word classes are distinguished : nouns , verbs and adjectives .
Implementing_Embedded_MT $ 2 $ 119 $ 62 $ PATTERN $ The second software engineering challenge stemming from this is the amount of time necessary to bring up a translation engine .
Embedding_Translation_in_an $ 6 $ 126 $ 24 $ CRF $ The objective of this experiment is to measure the performance of a translation system in the context of an application .
OUT $ -1 $ 145 $ 74 $ KP $ For example , ( A , Adv , D ) is a dependency type expressing the relationship between words in expression " very large " where " very " is a depending adverb and " large " is a head adjective .
The_TransType_model $ 2 $ 15 $ 0 $ KP $ 2 .1 User Viewpoint Our interactive translation system is illustrated in figure 1 for an English to French translation .
The_hyperonym_problem $ 2 $ 26 $ 8 $ PATTERN $ I will call it the hyperonym/TERM problem/TERM [ . . .] : When/DEF lemma A's meaning entails lemma B's meaning , B is a hyperonym of A ./O
Maximum_Entropy_Modeling $ 2 $ 42 $ 0 $ KP $ Sentence patterns or pattern ruels specify the sub-structures of the sentences . 
Middle $ 6 $ 139 $ 102 $ KP $ Some synsets are selected and regarded as the mapping of the Cilin sense tag . 
BOV_Stem_NE_Defaults_Qspecific_41 $ 5 $ 79 $ 49 $ PATTERN $ If the first NP/TERM ( noun-phrase/DEF ) in the sentence following the match is a pronoun , choose that sentence : Q : Why did Chris write two books of his own?
Introduction $ 1 $ 2 $ 0 $ KP $ In the shared task for CoNLL-2000 , words and tags form the basic multi-valued features for predicting a rich phrase segmentation code . 
Introduction $ 1 $ 23 $ 7 $ KP $ idf would assign values well outside this range . 
Building_Spoken_Dialo~te_Systems $ 4 $ 147 $ 42 $ CRF $ The network/TERM name/TERM is the/DEF identifier of the language model for the speech recognition ./O
OUT $ -1 $ 0 $ 0 $ KP $ Multilingual Summary Generation in a Speech-To-Speech Translation System for Multilingual Dialogues* Jan Alexandersson , Peter Poller , Michael Kipp , Ralf Engel DFKI GmbH Stuhlsatzenhausweg 3 66123 Saarbrficken {alexanders son , poller , engel , kipp}@dfki , de
Information_Structures $ 3 $ 83 $ 6 $ CRF $ The ' medicine/TERM ' is a/DEF material of ' addictive ' products ./O
_Rare_w~_contains_a_hyphen $ 9 $ 64 $ 9 $ CRF $ The model was trained and tested on the part/Ofspeech tagged WSJ section of the Penn Treebank . 
SYSTEM : _i_see_a_few_flights_from_new $ 5 $ 2 $ 0 $ KP $ york to washington national which depart about ten A M on january twenty seventh . 
Generation_and_linguistic_representation $ 4 $ 133 $ 61 $ CRF $ : x VP V NP : p j I surrounding c semantics : surround/TERM (/TERM x/TERM ./TERM p/TERM )/TERM ( 4a ) provides a structure/DEF that could substitute for the G node in ( 3 ) to produce semantically and pragmatically coordinated speech and gesture ./O 
Implementing_PbA $ 3 $ 52 $ 11 $ CRF $ An arc is placed from node i to node j if there is a matched substring starting with Li and ending with Lj . 
Abstract $ 0 $ 22 $ 21 $ PATTERN $ However , traditional Dependency gammar realizes the dependency relations between any of two specific words , then numerous word based dependency knowledge should be constructed , this is a time-consuming task . 
Subcategorization_Frequency $ 3 $ 99 $ 43 $ PATTERN $ The verb ' fight " is the only verb that has a different transitivity bias in each of the three corpora; with all other verbs , at least two corpora share the same bias . 
Abstract $ 0 $ 100 $ 39 $ PATTERN $ As a matter of fact , the three never wrote a musical individually or as a single ties whose union or sum is the overall plural argument . 
Conclusion $ 7 $ 189 $ 0 $ PATTERN $ Starting from the question what are the elementary units to consider for a text ' s discourse structure , I presented an account for prepositional phrases with adjunct-status as discourse units . 
Note_that_the_nouns_at_this_point_are_types_not $ 4 $ 197 $ 82 $ CRF $ Resnik proposed an unsupervised method for sense disambiguation using selectional preference information , thereby using grammatical relations between words in a corpus in order to arrive at the correct sense for a word . 
Experimental_setting $ 2 $ 65 $ 3 $ PATTERN $ As Yarowsky shows , both measures correlate closely , so we 208 only used the experimental results of decision Word PoS #Senses #Ex . 
_Preliminary_Evaluation $ 3 $ 90 $ 35 $ CRF $ Therefore , we produced the alignments for the 6 parallel corpora a parallel corpus comprises the English corpus and its translation into one of the three languages using one of the MT packages with English as the target language . 
_KNOWLEDGE_EXTRACTION $ 3 $ 171 $ 113 $ PATTERN $ When the proper name X is a new word , it will be segmented into shorter segments ( xl+x2+ . . . +xn ) . 
Discourse_coherence_and $ 1 $ 42 $ 34 $ PATTERN $ ( Cheng , 1998 ) describes interactions that need to be taken into account in aggregation . 
Tree_Structures $ 2 $ 35 $ 19 $ PATTERN $ B1 , B2 and B3 are the break-related nodes . 
Stochastic_Machine_Translation $ 2 $ 46 $ 4 $ PATTERN $ The target string IfV~ is then chosen from all possible reorderings 2 of I?VT = argmax P ( Ws , WT ) ( 2 ) WT [TV~ = arg max P ( I~VT I AT ) ( 3 ) WTE~W T where AT/TERM is the/DEF target language model and/O AWT/TERM are the/DEF different reorderings of WT ./O 
OUT $ -1 $ 0 $ 0 $ KP $ The hyperonym problem revisited : Concep , tual and : : lexical . 
_Features $ 3 $ 23 $ 0 $ KP $ Nineteen linguistic features were annotated , along with information about the referent of each mention . 
_Pronominalise_the_Cb_only_after_a_Continue $ 4 $ 40 $ 1 $ KP $ Strategy 3 is favoured by ( GJW ) who cite psychological evidence that " the Cb is preferentially realised by a pronoun in English and by equivalent forms ( i.e. , , . zero pronouns ) in other languages " ( op cit . , p . 214 ) . 
Results $ 3 $ 85 $ 28 $ KP $ Keyword totals in the same grouping are not statistically different . 
Experiment $ 3 $ 110 $ 28 $ CRF $ As expected , when one computes the recall and precision figures with respect to the nuclearity and relation assignments , one also factors in the nuclearity status and the rhetorical relation that is associated with each span . 
OUT $ -1 $ 141 $ 39 $ KP $ We perform a retrieval experiment to evaluate the automatically extracted rules . 
Experiments $ 5 $ 105 $ 22 $ CRF $ We believe that this is the main factor of the significant difference in performance . 
Experimental_Results $ 5 $ 184 $ 34 $ CRF $ Prob-Parser/TERM (/TERM B/TERM )/TERM is the/DEF probabilistic parser using a beam width of B ./O TABULATE is CHILL using the TABULATE induction algorithm with determ ; nistic parsing . 
Related_Work $ 3 $ 126 $ 28 $ PATTERN $ Accuracy Measurements of Parsing with Different Measures of Association are especially interested in experimenting with a Maximum Entropy model . 
Abstract $ 0 $ 3 $ 1 $ CRF $ On a new data set we have constructed for the task , while we were disappointed not to find parsing improvement over a traditional parsing model , our model achieves a recall of 84 . 0 % and a precision of 67 . 3 % of exact synset matches on our test corpus , where the gold standard has a reported inter-annotator agreement of 78 . 6 % . 
Introduction $ 1 $ 21 $ 14 $ PATTERN $ " Section 2 discusses the properties of numerical evaluation measures , points out several drawbacks associated with intrinsic measures and introduces new measures developed by the authors . 
Comparison_experiment $ 3 $ 167 $ 69 $ PATTERN $ " it is not sufficient that the string " Robert Sheckley " or " Sheckley " is in the text , but the document has to say that Robert Sheckley is the author of Options . 
Interlingua_system_ ( ISS ) $ 3 $ 82 $ 13 $ PATTERN $ 4 A clause/TERM could be defined as " a/DEF group of words containing a verb "/O . 
Results $ 3 $ 63 $ 11 $ PATTERN $ The data/TERM consists/DEF of fourtuples of words , extracted from the Wall Street Journal Treebank ./O 
The_lexicon_size_of_a_typical_large-vocabulary $ 9 $ 103 $ 24 $ PATTERN $ Word/TERM segmentation/TERM is a/DEF natural by-product of large vocabulary Mandarin speech recognition ,/O and white space provides word boundaries for the English queries . 
OUT $ -1 $ 160 $ 58 $ PATTERN $ We can conclude that the filtering method H is the best , considering the effectiveness and the efficiency at the same time . 
Introduction $ 1 $ 4 $ 1 $ KP $ SVMs/TERM are so-called large/DEF margin classifiers and are well-known as their good generalization performance ./O 
Interaction_Grammars $ 3 $ 122 $ 69 $ KP $ grammar ( program ) , but which introduce terminals specific , to ~the_ language -at . 
_Our_Classification_Algorithm_for $ 2 $ 85 $ 21 $ CRF $ The following is the set of collocational features . 
Extending_Domain_Semantics_for $ 5 $ 163 $ 6 $ PATTERN $ The expression specification first of all defines which verb to use in the expression . 
Abstract $ 0 $ 15 $ 12 $ KP $ Artificial/TERM neural/TERM networks/TERM are a/DEF classification technique that is robust and resistant to noisy input , and learns to classify inputs on the basis of training examples , without specific rules that describe how the classification is to be done ./O 
Towards_building_a_parallel_corpus $ 2 $ 40 $ 8 $ CRF $ Hence , tile/TERM mappings/TERM in ( 7 ) provide/DEF an explicit representation of the way information is re/Ordered and re-packaged when/O translated from Japanese into English . 
Models_and_Modifications $ 2 $ 48 $ 35 $ PATTERN $ Pi/TERM (/TERM c~/TERM )/TERM is the/DEF probability of beginning a derivation with c~ ;/O Ps/TERM (/TERM o~/TERM I/TERM 77/TERM )/TERM is the/DEF probability of substituting o~ at 7 ;/O Pa/TERM (/TERM /~/TERM I/TERM r//TERM )/TERM is the/DEF probability of adjoining ~ at 7/ ;/O finally , Pa/TERM (/TERM NONE/TERM I/TERM 7/TERM )/TERM is the/DEF probability of nothing adjoining at ~/ ./O 
Introduction $ 1 $ 19 $ 9 $ KP $ Semantically annotated documents are accessed using the vocabulary provided by a domain-specific ontology . 
Implementation $ 3 $ 155 $ 37 $ KP $ The nodes in the output vector represents different syntactic categories , so we also get a surface syntactic structure directly output from the net , which could be used for stress information etc . 
_Instrumentalists_not_including_string_players $ 8 $ 111 $ 99 $ PATTERN $ As it is said in ( Seto , 1996 ) , ~ ( . . ) There often is a one-tone correspondence between different languages in their lexiealization behaviour towards metonyrny , in other words , metonymically related word senses are often translated by the same word in other languages " . 
Conclusions_and_future_work $ 6 $ 220 $ 3 $ PATTERN $ The statistical results partially justify our claim that it is the preferences among generation features that decide the coherence of a text . 
Experimental_Setting $ 4 $ 110 $ 9 $ PATTERN $ The corpus consists of 168 parallel news ( i.e. 
Implementation $ 5 $ 141 $ 7 $ PATTERN $ The result is a lattice of possible realizations , representing both the preserved ambiguity from previous processing phases and multiple ways of linearizing the sentence . 
Introduction $ 1 $ 23 $ 19 $ CRF $ This paper presents WIT 1 , which is a toolkit IWIT/TERM is an/DEF acronym of Workable spoken dialogue lnter150 for building spoken dialogue systems that integrate speech recognition , language understanding and generation , and speech output ./O 
Principles_and_Parameters $ 1 $ 8 $ 1 $ KP $ The syntactic component of a grammar in the principles and parameters ( henceforth P&P ) framework , is simply a collection of parameter values /One value per parameter . 
Translations_selection_and_phrase $ 2 $ 50 $ 0 $ KP $ keeping It is a naive method to translate a Chinese query only by looking up each Chinese term to get its English senses in a Chinese-English dictionary . 
Conceptualizing_Events $ 2 $ 104 $ 74 $ PATTERN $ Figure 2 sketches such a cascade of dependent parallel processes in our model of the conceptualizer : The cascade/TERM consists/DEF of the processes construction , selection , linearization , and pvm-generation ( preverbal-message-generation ) ./O 
Abstract $ 0 $ 10 $ 9 $ KP $ Dialog strategies and management should be adjusted to the evolving state of the user . 
Applications_of_LexTract $ 4 $ 184 $ 38 $ PATTERN $ T To summarize , we have just showed that , 7The number 97 . 2% is the sum of two numbers : the first one is the percentage of matched template tokens ( 82 . 1% from Table 2 ) . 
Prosody_Prediction $ 4 $ 68 $ 6 $ PATTERN $ To decide on the prosody for these unlinked parts is a problem . 
_Limitations $ 5 $ 34 $ 8 $ KP $ TRIPS-911 can currently interpret expressions with respect to the actual time . 
OUT $ -1 $ 180 $ 143 $ CRF $ In addition , by no means is the UNL system committed to event replication as it is the case of human translation . 
_Background $ 1 $ 28 $ 13 $ PATTERN $ 1 . 2 Grounding and Common Ground Units Grounding/TERM is the/DEF process by which information contributed by participants in interaction is taken to have entered the ' common ground ' , or mutual knowledge of the participants (/O Clark & Schaefer 1989 , Clark 1996 , Traum 1994 ) . 
OUT $ -1 $ 158 $ 116 $ PATTERN $ Two measures that can be used in generating confidence scores are proposed in this section . 
_Instrumentalists_not_including_string_players $ 8 $ 57 $ 45 $ PATTERN $ While in this particular example there is a clear relation between senses ( sense 1 is involved in the action specified in sense 2 ) , it seems extremely difficult to find general clustering techniques based on Word . Net hierarchy to capture all potential IR clusters . 
Evaluation $ 4 $ 144 $ 29 $ CRF $ Such SActually , cousin/TERM is one/DEF of the three relations which indicate the grouping of related senses of a word ./O 
_Preliminary_Evaluation $ 3 $ 80 $ 25 $ CRF $ GIZA/TERM is an/DEF intermediate program in a statistical machine translation system ,/O EGYPT . 
Introduction $ 1 $ 24 $ 15 $ PATTERN $ The output of the discourse component [Seneff ( 1996 ) ] is the framein-context , which is transformed into a flattened Eform/TERM ( electronic/DEF form )/O by the generation server . 
Analysis_module $ 2 $ 45 $ 5 $ KP $ In pronominal anaphora resolution in both the Spanish and English languages , the system has achieved an accuracy of 84% and 87% respectively . 
_Our_Classification_Algorithm_for $ 2 $ 65 $ 1 $ CRF $ In linguistics , telicity/TERM is a/DEF phase feature used in classifying ./O 
OUT $ -1 $ 21 $ 18 $ PATTERN $ Texts consists of Spanish descriptions of specimens . 
Introduction $ 1 $ 9 $ 2 $ KP $ Error-correcting/TERM output/TERM codes/TERM ( ECOC/TERM ) have been introduced/DEF to machine learning as a principled and successful approach to distributed class encoding (/O Dietterich and Bakiri , 1995 ; Ricci and Aha , 1997 ; Berger , 1999 ) . 
Psycholinguistic_production $ 3 $ 50 $ 15 $ CRF $ When a lexical concept is activated , the mechanism of activation spread : ing ensures that ~the : : ~directly : . . ecm : nected : : lemma . . . . receives tile highest activation , and not a lemma associated with a hyperonym of the lexical concept ( which is connected by an ISA-link ) . 
Rules_as_features $ 1 $ 16 $ 8 $ CRF $ For example , ( Domingos , 1996 ) describes the RISE/TERM system/TERM , in/DEF which rules are ( carefully ) generalised from instances , and in which the k-NN classification rule searches for nearest neighbours within these rules when classifying new instances ./O 
Issues_and_proposals $ 3 $ 137 $ 94 $ CRF $ This means that the standard segmentation of a dialog into utterances may have to be modified for the purposes of an RST analysis , although a segmentation into utterances and one into minimal units will be very similar . 
Abstract $ 0 $ 148 $ 6 $ PATTERN $ The main source are the New Drug Authorizations ( Autorisation de Mise sur le March~ ) , regulatory documents written by pharmaceutical laboratories and approved by legal authorities . 
Highlight $ 3 $ 51 $ 0 $ PATTERN $ Highlight/TERM ( Thomas et al . , 2000 ) is a/DEF generalpurpose IE engine for use in commercial applications ./O 
_INTRODUCTION $ 1 $ 39 $ 33 $ CRF $ Unfortunately there is no wellprepared knowledge sources containing the above information . 
Interlingua_system_ ( ISS ) $ 3 $ 82 $ 13 $ CRF $ 4 A clause/TERM could be defined as " a/DEF group of words containing a verb "/O . 
OUT $ -1 $ 202 $ 76 $ PATTERN $ The Formulaic feature , which is not very strong on its own , is the most diverse , as it contributes to the disambiguation of six categories directly . 
Algorithms_and_Implementation $ 2 $ 28 $ 8 $ CRF $ The/DEF heuristic approximation of computationally expensive pure MBL variants ,/O ( IGTREE/TERM ) , creates an oblivious decision tree with features as tests , ordered according to information gain of features . 
The_Feasibility_of_the_STL $ 3 $ 54 $ 12 $ CRF $ our purposes , the number of relevant parameters , r/TERM , is the/DEF total number of parameters that need to be set in order to license all and only the sentences of the target language ./O 
Architecture $ 3 $ 61 $ 10 $ PATTERN $ Our architecture consists of four main modules ( see figure 2 ) . 
_Instrumentalists_not_including_string_players $ 8 $ 50 $ 38 $ CRF $ As we mentioned before , the clusters applied on the EWN InterLingual Index which relied solely on hierarchical information in Wordnet , produced a slight decrease of retrieval performauce in an experiment using 1LI records as indexing units . 
Task_description $ 2 $ 9 $ 0 $ KP $ Text/TERM chunking/TERM consists of dividing/DEF a text into phrases in such a way that syntactically related words become member of the same phrase ./O 
Generating_Summaries $ 4 $ 135 $ 43 $ CRF $ The goal of the operators uses an interface based on a triple with the following usage : o < description > This is the input position of the operator . 
Using_CST_for_information_fusion $ 5 $ 146 $ 9 $ CRF $ The third technique , information/TERM extraction/TERM [ Radev & McKeown 98 ] identifies/DEF salient semantic roles in text ( e . g . , the place , perpetrator , and effect of a terrorist event ) and converts them to semantic templates ./O 
Discourse_coherence_and $ 1 $ 28 $ 20 $ CRF $ For example , comparing the first two descriptions of . a necklace in Figure 1 , 2 is less coherent than 1 because of the shifting from the description of the necklace to that of the designer , which is a side effect of embedding . 
OUT $ -1 $ 87 $ 69 $ PATTERN $ A NAME/TERM is defined as a/DEF PROPER_NOUN that contains at least one HUMAN word ./O 
Analysis_module $ 2 $ 49 $ 9 $ CRF $ SUPAR/TERM allows/DEF to carry out either a full or a partial parsing of the text , with the same parser and grammar ./O 
See_ ( Chu-Carroll_and_Carberry_1998 ) _tbr_an $ 4 $ 116 $ 7 $ CRF $ The only requirement for this method is the specification of a sensible task . 
OUT $ -1 $ 76 $ 75 $ PATTERN $ If the subject is a single entity e , the value of the subject for an objective o is vo ( e ) , and it is positive when it is greater than 0 . 5 , the midpoint of [0 ,1] ( negative otherwise ) . 
Text_Meaning_Representation $ 5 $ 111 $ 9 $ CRF $ In the example , ASPECT-7 is applied within the scope of GOVERNMENT-ACTIVITY in which TELIC with value YES indicates the GOVERNMENTACTIVITY is complete that means the action 113 of opening foreign trade policy is done . 
Research_context : _hypertext $ 1 $ 15 $ 11 $ PATTERN $ In a nutshell ' , while NLG is the only technology able to completely fulfill the reporting needs of i See Favero ( 2000 ) for further justification for this view , as well as for details on the motivation and technology underlying MATRIKS . 
Clustering_into $ 5 $ 78 $ 9 $ CRF $ This can be computed for given word-pair type ( wl , w2 ) by recording each word-pair token ( wl , w2 , d ) in a corpus , where d/TERM is the/DEF distance or number of intervening words ./O 
Lexical_Conceptual_Structure $ 2 $ 9 $ 0 $ KP $ Lexical/TERM Conceptual/TERM Structure/TERM is a/DEF compositional structure that captures a concept ./O
Introduction $ 1 $ 23 $ 17 $ CRF $ Section 2 presents the transformation based learning paradigm ; Section 3 describes the algorithm for construction of the decision tree associated with the transformation based list ; Section 4 describes the experiments in detail and Section 5 concludes the paper and outlines the future work . 
Modeling_various_degrees_of $ 5 $ 79 $ 16 $ CRF $ One subfield is a set of propositions which the agent assumes for the sake of the conversation . 
Introduction $ 1 $ 16 $ 10 $ PATTERN $ 116 As ( lc ) records , these are the intersecting dotted segments of ( 1 a ) , and can be designated as such . 
Mining_Discourse_Marker_Using $ 6 $ 188 $ 18 $ CRF $ This information/TERM gain/TERM measures/DEF the expected reduction in entropy and defines one branch for the possible subset Si of the training examples ./O 
The_Classifiers $ 3 $ 67 $ 0 $ CRF $ Adaptive/TERM Resonance/TERM Associative/TERM Map/TERM ( ARAM/TERM ) is a/DEF class of predictive serforganizing neural networks that performs incremental supervised learning of recognition categories ( pattern classes ) and multidimensional maps of patterns ./O 
Mapping_a_document_collection_into $ 2 $ 118 $ 71 $ PATTERN $ Document maps topic-document relevance shown by document proxy placement and color gradation : In a document map , the horizontal placement of each dot represents the degree of relevance of the corresponding document to the topic . 
Abstract $ 0 $ 1 $ 0 $ KP $ Conversational/TERM grunts/TERM , such/DEF as uhhuh , un-hn , rnrn , and oh are ubiquitous in spoken English , but no satisfactory scheme for transcribing these items exists ./O 
INTRODUCTION $ 1 $ 25 $ 19 $ PATTERN $ The third difficulty is the problems of ambiguities , such as structure ambiguities , syntactic ambiguities and semantic ambiguities . 
The_Importance_of $ 1 $ 22 $ 18 $ CRF $ It would also facilitate the systematic corpus-based study of the meanings and functions of these sounds 4 . 
Multiple_heuristics_for_word_sense $ 2 $ 35 $ 10 $ CRF $ Hi ( s , ) = max support ( s , , ew~ ) 1 ~ ' ~ , ( n-1 ) +a k , =l where EWi = ( ewl s , ~ synset ( ew ) } In this formula , Hi/TERM (/TERM s/TERM )/TERM is a/DEF heuristic score of synset s ,/O s/TERM is a/DEF candidate synset ,/O ew/TERM is a/DEF translation into English ,/O n/TERM is the/DEF number of translations and/O synset/TERM (/TERM ew/TERM )/TERM is the/DEF set of synsets of the translation ew ./O 
Robustness $ 4 $ 107 $ 16 $ CRF $ Wrong Index : The robustness preprocessor tests whether the index points to the root of the graph or one of the subgraphs . 
POS_Assignment $ 3 $ 95 $ 35 $ KP $ If a character string reaches the threshold of more than one P ( Cat ) , it will be assigned more than one syntactic category . 
Architecture_and_Principles $ 3 $ 35 $ 12 $ CRF $ Candidate NEs Initializers and Candtdate ate~ Index pages I Filter I Typed NEs Figure 1 : Architecture trieved through one of the strings ( 1 . a-d ) 3 The first collection is an enumeration and consists of a coordination of three NEs . 
First_order_weight_determination $ 3 $ 38 $ 5 $ CRF $ A measure related to this is Information/TERM Gain/TERM , which represents/DEF the difference between the entropy of the choice with and without knowledge of the presence of a feature (/O cf . 
Multiple_heuristics_for_word_sense $ 2 $ 63 $ 38 $ PATTERN $ And we uses the 25 semantic tags of WordNet as sense tag : n6 ( s , ) = max p ( t , I x ) xeDef with p = , Z ( l_a ) /2 × ~ ' ( 1~ ~ ) p ( ti l x ) = Freq ( ti , x ) Freq ( x ) In this formula , Defis the set of content words of a Korean definition sentence , t is a semantic tag corresponding to the synset s and n refers to Freq ( x ) . 
OUT $ -1 $ 120 $ 18 $ PATTERN $ MI/TERM ( Mutual/TERM Information/TERM ) is a/DEF measure of word association , and used under the assumption that a highly associated word n-gram is more likely to be a compound noun ./O 
Discussion $ 6 $ 220 $ 42 $ KP $ A ternary expression representation of natural language mimics its syntactic organization , and hence sentences that differ in surface form but are close in meaning will not map into the same structure . 
Abstract $ 0 $ 12 $ 10 $ CRF $ Finally , there is a real demand for working AE systems in techni : cal domains . 
Error-driven_Learning $ 4 $ 121 $ 4 $ PATTERN $ If F o ( e i ) > 0 , we define the lexical entry ei as positive for lexicon ~ . 
Introduction $ 1 $ 6 $ 2 $ KP $ Disambiguation can be separated between MS/TERM ( morpho-syntactic/DEF , i.e. 
The_hyperonym_problem $ 2 $ 18 $ 0 $ KP $ Following tile psycholinguistics literature , the hyperonym problem is regarded as all aspect of lemrna retrieval . 
Maximum_Entropy_Modeling $ 2 $ 53 $ 11 $ PATTERN $ A feature/TERM of/TERM a/TERM context/TERM is a/DEF binary-valued indicator function ] expressing the information about a specific context ./O 
Tagged_Text $ 3 $ 146 $ 132 $ KP $ The semantic and syntactic information is coded as shown in Figure 2 ( using XML tags ) . 
Introduction $ 1 $ 14 $ 10 $ PATTERN $ There is a great diversity among the web pages in terms of document length , style , and content . 
The_TABULATE_ILP_Method $ 3 $ 97 $ 0 $ CRF $ 3.3 Noise Handling A clause needs no further refinement when it meets the following criterion ( as in RIPPER ( Cohen , 1995 ) ) : P - . __ . 2_ > ( 6 ) p+n where p/TERM is the/DEF number of positive examples covered by the clause ,/O n/TERM is the/DEF number of negative examples covered ./O 
Generalisation_operators $ 4 $ 122 $ 9 $ PATTERN $ Unfortunately , the fourth sentence needs to use the missing rule twice to get a parse , and it is a fundamental limitation of our approach that a missing rule can only be recovered from a failed parse if it is required only once . 
_Category_of_the_constituent_embedding_the $ 4 $ 55 $ 1 $ KP $ See Figure 1 : The category of the constituent embedding the NP the problem is PP . 
Performance $ 4 $ 176 $ 13 $ CRF $ Figure 8 is similar but has the additional overhead of around 2/3 of all files having hits where Figure 7 has only 1/20 . 
Shallow_Parsing $ 4 $ 74 $ 4 $ PATTERN $ Our modeling of the problem is a modification of our earlier work on this topic that has been found to be quite successful compared to other learning methods attempted on this problem ( Mufioz et al . , 1999 ) and in particular , better than the IO modeling of the problem ( Mufioz et al . , 1999 ) . 
Abstract $ 0 $ 20 $ 16 $ PATTERN $ perplexity measures [Keene and O ' Kane , 1996] or topic detection [Mahajan et al . , 1999] ) selects or combines with a more appropriate language model . 
The_Objective_Function $ 3 $ 41 $ 0 $ PATTERN $ Previous research approached the task of determining which rule combinations to allow either by a process of manual trial and error or by statistical measures based on a collection of positive examples only : if the original grammar produces more than a single parse of a sentence , only the " correct " parse was stored in the treebank . 
Explaining_Probabilistic_Methods $ 3 $ 62 $ 14 $ PATTERN $ ) A statistical/TERM queries/TERM algorithm/TERM is a/DEF learning algorithm that constructs its hypothesis only using information received from an SQ oracle ./O 
OUT $ -1 $ 0 $ 0 $ KP $ Comparing corpora with WordSmith Tools : How large must the reference corpus be?
Abstract $ 0 $ 26 $ 24 $ PATTERN $ In the following example , the DA/TERM consists/DEF of a speaker tag ( a : for agent ) , the speechact give-information , and two main concepts , +price and +room ./O 
Abstract $ 0 $ 19 $ 15 $ CRF $ A more error-driven approach is the use of hybrid language models , in which some detection mechanism ( e . g . 
Proper_name_recognition $ 4 $ 113 $ 39 $ PATTERN $ ( 9 ) biZ~ ~ ( i0 ) the bill is . . . ( 11 ) the bill clinton is . . . ( 12 ) the bill clinton administration is The lexically ambiguous bill , interpreted as a proper name in isolation , becomes a common noun if preceded by a determiner . 
ALLiS $ 2 $ 5 $ 0 $ KP $ ALLiS/TERM (/TERM Architecture/TERM for/TERM Learning/TERM Linguistic/TERM Structure/TERM )/TERM ( D6jean , 2000a ) , ( D6jean , 2000b ) is a/DEF symbolic machine learning system ./O 
Introduction $ 1 $ 19 $ 15 $ KP $ o Entities/TERM , representing/DEF objects ( individuals ) of the world ./O 
Introduction $ 1 $ 68 $ 0 $ KP $ 2.4 Runtime Analysis In this analysis , we will not consider the computational complexity of part of speech tagging , as that is not the focus of this research . 
Introduction $ 1 $ 10 $ 3 $ KP $ With ECOC , monadic classes are replaced by codewords , i.e. 
The_REXTOR_System $ 5 $ 169 $ 47 $ CRF $ The first extraction rule defines a NounGroup/TERM as a/DEF sequence consisting of : an optional possessive pronoun or determiner , any number of adjectives , one or more nouns ( of any type ) ./O 
The_Structure_of_a_Relational $ 3 $ 101 $ 27 $ PATTERN $ ® Fact : each entry in a record defines what we call a fact about that entity , a A/TERM fact/TERM consists/DEF of three parts : its predicate name , and two arguments , being the entity of the record , and the filler of the slot ./O 
Abstract $ 0 $ 4 $ 2 $ KP $ MT has been used to facilitate cross-language information/DEF retrieval/TERM ( IR/TERM ) , topic detection and other , wide-scoped scenarios . 
PAR_as_anIL $ 4 $ 23 $ 12 $ KP $ Directed motion actions , such as enter and exit , don ' t bring with them the manner by which the action is carried out but they have a inherent termination condition . 
_The_session_was_also_attended_by_observers_from_the_following_international_organizations : $ 7 $ 215 $ 164 $ CRF $ This study is another application that demonstrates the usability of the WWW as a resource for NLP ( see , for instance , ( Grefenstette , 1999 ) for an application of using WWW frequencies in selecting translations ) . 
Markov_Modeling $ 2 $ 23 $ 0 $ PATTERN $ HMM/TERM is a/DEF probabilistic finite state automaton used to model the probabilistic generation of sequential processes ./O 
Introduction $ 1 $ 39 $ 35 $ CRF $ It offers a tool for helping people edit templates and see what text would be realized from a template , given a set of values for its slots . 
Note_that_the_nouns_at_this_point_are_types_not $ 4 $ 146 $ 31 $ CRF $ FG/TERM is the/DEF French translation of the Brown corpus rendered by the MT system GL ;/O GG/TERM is the/DEF German translation by GL ;/O SG/TERM is the/DEF Spanish translation by GL ;/O SS/TERM is the/DEF Spanish translation by the MT system SYS ;/O and MSp/TERM is the/DEF merged Spanish translations from both NIT systems ./O 
Introduction $ 1 $ 8 $ 2 $ CRF $ Machine translation between any two languages often requires the generation of information that is implicit in the source language . 
Systems_Foundations $ 3 $ 29 $ 5 $ KP $ MIMIC/TERM " provides/DEF movie listing information involving knowledge about towns , theaters , movies and showtimes ,/O as demonstrated in Figure 1 . 
_Semantic_classes_of_the_head_of_the_NP : _If $ 8 $ 66 $ 0 $ PATTERN $ the head of the NP is a noun we also take into account its semantic classification in a large semantic hierarchy . 
Abstract $ 0 $ 25 $ 0 $ CRF $ Algorithm is based on rules for discriminating among the two types of anaphor based on the predicative contexts in which the anaphors occur . 
Evaluation_of_NLG_Models $ 3 $ 101 $ 7 $ PATTERN $ Unfortunately , this is not the case for our argument generator , where the input consists of a possibly complex and novel argument subject ( e.g. , a new house with a large number of features ) , and a complex model of the user' s preferences . 
Robustness $ 4 $ 122 $ 31 $ PATTERN $ Other examples for generation constraints that can conflict with the input are the occurrence of some specific cyclic subparts of graphs , selfreferring predicates , and chains of predicates which are not realizable in generation . 
OUT $ -1 $ 131 $ 77 $ KP $ a QA system will return : cp However , an AE/TERM system/TERM will return/DEF all the sentences in the text that directly answer the question ,/O among them ( 1 ) . 
Generation_of_Vague_Descriptions $ 4 $ 116 $ 35 $ PATTERN $ The step from the superlative descriptions of case i to the analogous 'absolute' descriptions is a small one . 
Brief_System_Description $ 2 $ 13 $ 4 $ PATTERN $ The Ontology/TERM is a/DEF directed acyelic graph automatically derived from the Grammar in which the nodes correspond to grammar nonterminals ( NTs ) and the arcs record immediate dominance relation ,/O i.e. , the presence of , say , NTi in a right-hand/TERM side/TERM ( RHS/ACR ) alternative of NTj will result in an arc from NTi to NTj . 
Generating_Summaries $ 4 $ 94 $ 2 $ CRF $ It solves the task of mapping the DIREX structures selected in the dialogue nmmory into sequences of full fledged semant . ic sentence descriptions ( VITs ) , thereby performing the following steps : * Document Planning : Extracting , preparing and dividing the content of the dialogue memory into a predefined format . 
Solving_the_generation_problem $ 5 $ 150 $ 10 $ PATTERN $ Along with these goals , the dialogue manager supplies its communicative/TERM context/TERM , which represents/DEF the centrality of the house in attentional prominence , cognitive status and information structure ./O 
Brief_introduction_to_neural_networks $ 2 $ 117 $ 14 $ PATTERN $ Since the knowledge the network acquires is a result of the mappings , how the input and output is represented is of great importance .
Abstract $ 0 $ 8 $ 7 $ CRF $ The main methodological considerations associated with our current work are : a ) how natural dialogues can be reliably annotated to allow independent comparisons and correlations of prosodic and structural features , b ) the identification and classification of units of dialogue that reflect the ' joint action ' feature of interactive discourse ( ie . 
Abstract $ 0 $ 8 $ 4 $ CRF $ The Inductive/TERM Logic/TERM Programming/TERM learning/TERM method/TERM that we have developed enables us to automatically/DEF extract from a corpus N-V pairs whose elements axe linked by one of the semantic relations defined in the qualia structure in GL , and to distinguish them , in terms of surrounding categorial context from N-V pairs also present in sentences of the corpus but not relevant ./O 
The_CGS_system $ 4 $ 79 $ 11 $ PATTERN $ There are ' two , situations where the text planning module helps specifically in the generation of referring expressions : ( 1 ) when the complexity for expressing a graphic demands an example and 5 " Graphemes are the basic building blocks for constructing pictures . 
Semantic_Lexicon $ 3 $ 50 $ 4 $ KP $ The semantic/TERM zone/TERM maps/DEF a sense into an ontological concept in the case of single sense , or to several concepts in the case of multiple senses ./O 
Background $ 2 $ 60 $ 40 $ KP $ requestValue/TERM ( p=v ) : system/DEF asks whether the value v of parameter p is correct ./O 
Clustering $ 3 $ 68 $ 8 $ CRF $ The TF/TERM column/TERM indicates/DEF the average term frequency of a given term within the cluster ./O 
The_formula_is_valid_when_J_>_R_ ( that_is ,_the_judges $ 7 $ 117 $ 31 $ CRF $ In Table 10 we show the normalized performance ( D ) of MEAD , for the six clusters at nine compression rates . 
Related_Work $ 4 $ 128 $ 37 $ CRF $ As explained in Section 2.1 . , an agent ' s utility/TERM function/TERM is a/DEF weighted sum of individual utility functions , which represent the preference assume that weights Wi and Wj are set , respectively , to 20 and 10 ./O 
Abstract $ 0 $ 4 $ 1 $ KP $ Currently the major part of this architecture consists of a set of datatype definitions for specifying the input and output formats for modules within NLG systems . 
Stochastic_Surface_Realization $ 2 $ 75 $ 0 $ PATTERN $ 2.2 The input to NLG from the dialogue manager is a frame of attribute-value pairs . 
_Amanda_Huggenkiss ,_she_proposes_to_meet_you $ 2 $ 110 $ 103 $ PATTERN $ Thus we will search in the hearer ' s discourse and physical context which are the activated instances of the concepts event , person , keyword and time/location . 
Implementation $ 4 $ 339 $ 168 $ PATTERN $ Among the most important wins over the traditional " piping " approach to filter assembly is the ability to impose build-time restrictions on the component assembly , disallowing " illegal " compositions . 
The_Generator_in_an_Applied $ 3 $ 121 $ 5 $ PATTERN $ The analysis component contains a morphological analyser , and it is the base forms of sit is likely that a modest increase in speed could be obtained by specifying optimisation levels in Flex and gcc that are higher than the defaults . 
Conclusions_and_Future_Work $ 5 $ 141 $ 7 $ CRF $ The corpus we use in our experinaent is a relative small corpus about computer handbook , in which the terms are translated with high consistency . 
MALIN $ 4 $ 112 $ 6 $ PATTERN $ XMALIN/TERM (/TERM Multi-modal/TERM Application/TERM of/TERM LINLIN/TERM )/TERM is a/DEF refinement of the LINLINsystem ( Ahrenberg et al . , 1990 ; JSnsson , 1997 ) to handle also multi-modal interaction and more advanced applications ./O 
Introduction : _multi-document $ 1 $ 43 $ 40 $ PATTERN $ The next section describes the multi-dimensional space for the document collection . 
Problem_Space_Modeling $ 2 $ 42 $ 15 $ KP $ Reward/TERM and Punishment/TERM are the/DEF utility metrics corresponding to each sub-goal ( Winlder , 95 1972 ) depending upon the hypothesis of uncertainty of understanding and the level of importance ./O 
The_interplay_of_focus_and_word $ 2 $ 26 $ 2 $ PATTERN $ has either been selected from a set of alternative beliefs ascribed t . o the listener , or it is a revision of certain beliefs ( in case of contrastive focus ) , or the focused phrase expresses ' new ' information the listener does not know or is not able to infer from his beliefs [Halliday , 1967] . 
Generation_of_Multiple_Quantifiers $ 5 $ 224 $ 39 $ PATTERN $ If the name of the surgeon is not available but the identifiers for the surgeon entities across the propositions are the same , the system will generate " The same surgeon operated on each patient . 
Memory-based_learning $ 3 $ 74 $ 17 $ KP $ The first quantity is normalized with the a priori probabilities of the various feature values of feature F : H ( C ) Eveva es ( F ) P ( v ) × H ( QF=v] ) ( 6 ) Here , H/TERM (/TERM C/TERM )/TERM is the/DEF class entropy , defined as H ( C ) =~ P ( c ) log 2P ( c ) ./O 
Abstract $ 0 $ 22 $ 12 $ KP $ The lexicon entry or Root/TERM LCS/TERM (/TERM RLCS/TERM )/TERM of one sense of the Chinese verb xuel_jian3 is as follows : ( 1 ) ( act_on loc ( * thing 1 ) ( * thing 2 ) ( ( * [on] 23 ) loc ( *head* ) ( thing 24 ) ) ( cut+ingly 26 ) ( down+/m ) ) The top node in the . 
Introduction $ 1 $ 8 $ 2 $ PATTERN $ In fact , the criteria regarding language style may differ for each search and therefore due to the large number of texts there is a requirement to perform style categorisation in an automated manner . 
Word_Sense_Dis~mbiguation $ 4 $ 227 $ 135 $ PATTERN $ The novelty of this method consists of the fact that the disambiguation process is done in an iterative manner . 
The_following_formulas_summarize_the_relations $ 9 $ 170 $ 89 $ PATTERN $ We present measures to evaluate filtering performance and preliminary results on Spanish , Arabic and Haitian Creole FALCon systems . 
Hyperonymy_in_lexical_semantics $ 5 $ 103 $ 6 $ CRF $ And hence , there is a difference between fish ISA creature and fish ISA animal . 
_Sense_Co-occurrence_Frequency $ 3 $ 73 $ 46 $ PATTERN $ N ~ g ( SU ) , g ( SU ' ) ( 7 ) Where f/TERM (/TERM SU/TERM ,SU/TERM '/TERM )/TERM is the/DEF co-occurrence frequency corresponding to sememe pair ( SU , SU ' ) in SCFD ./O 
Introduction $ 1 $ 46 $ 37 $ KP $ TM2/TERM contains/DEF elements which are translation segments ranging from whole sections of a document or multisentence paragraphs to smaller units , such as short phrases or proper names ./O 
OUT $ -1 $ 69 $ 16 $ KP $ Mandarin Chinese presents a challenge for word-level indexing by LVCSR , because of the ambiguity in tokenizing a sentence into words ( as mentioned earlier ) . 
Human_Annotation_of $ 2 $ 65 $ 0 $ KP $ Argumentative Zones We have previously evaluated the scheme empirically by extensive experiments with three subjects , over a range of 48 articles ( Teufel et al . , 1999 ) . 
OUT $ -1 $ 69 $ 64 $ KP $ The TREC-8 QA test scores of ( Radev et al . , 2000 ) were also considerably lower than best QA test scores . 
OUT $ -1 $ 0 $ 0 $ KP $ Using Co-occurrence Statistics as an Information Source for Partial Parsing of Chinese Elliott Franeo DRABEK The State Key Laboratory for Intelligent Technology and Systems Department of Computer Science Tsinghua University , Beijing 100084 elliott_drabek@ACM . org Qiang ZHOU The State Key Laboratory for Intelligent Technology and Systems Department of Computer Science Tsinghua University , Beijing 100084 zhouq@slOOOe . cs . tsinghua . edu . cn
Building_Spoken_Dialo~te_Systems $ 4 $ 125 $ 20 $ PATTERN $ Each definition is a pair comprising a network name and a set of phrase category names . 
Rules_as_features $ 1 $ 8 $ 0 $ KP $ A common machine-learning solution to classification problems is rule induction ( Clark and Niblett , 1989 ; Quinlan , 1993 ; Cohen , 1995 ) . 
Generation_and_linguistic_representation $ 4 $ 96 $ 24 $ CRF $ The : -organization of ' these entries assures ' that -rasing the same mechanism as with speech -REA ' S gestures draw on the single available conceptual representa174 tion and that both REA ' S gesture and the relationship between gesture and speech-vary as a function of pragmatic context in the same way as natural gestures and speech do . 
The_Generation_System $ 3 $ 68 $ 26 $ PATTERN $ 3 . 1 . 3 Alignment/Decomposition The heart of the lexical access algorithm is the decomposition process . 
Abstract $ 0 $ 4 $ 2 $ CRF $ Given an argument generated by the system and an interpretation of a user ' s rejoinder , the generation of the rebuttal takes into account the intended effect of the user ' s rejoinder , determined on a model of the user ' s beliefs , and its actual effect , determined on a model of the system ' s beliefs . 
Generation_of_Crisp_Descriptions $ 3 $ 54 $ 0 $ CRF $ Generation of descriptions covers a number of tasks , one of which consists of finding a set L of properties which allows a reader to pick out a given unique individual or set of individuals . 
Research_focus : _content_aggregation $ 2 $ 134 $ 73 $ PATTERN $ buildFactoringStrategy/TERM (/TERM Matrix/TERM )/TERM : returns/DEF inside a list a pair ( Dim , increasing ) where Dim is the matrix ' s dimension ( i.e. , column ) with the lowest number of distinct values ./O 
Data_Collection_and_Evaluation $ 4 $ 176 $ 30 $ CRF $ IBR/TERM measures/DEF the average number of new attributes introduced per user query ./O 
Ordering_Structures $ 6 $ 76 $ 6 $ KP $ At the VP level , it is thus useless of learn contexts in which VBG does not occur in a VP ( cases which mainly correspond to occurrences of VBG in NP ) . 
Comparison_experiment $ 3 $ 151 $ 53 $ PATTERN $ For example , if the question is " Who is the inventor of the electric light?
Evaluation $ 3 $ 102 $ 11 $ KP $ recall ( 5 ) precision + recall 3 . 2 Results Table 1 gives the raw results for the 14 verbs using each method . 
Conceptualizing_Events $ 2 $ 106 $ 76 $ PATTERN $ One central parameter of incremental processing , which is highly relevant for the format of preverbal messages , is the size of the increments . 
Abstract $ 0 $ 5 $ 1 $ CRF $ We show that there is a strong relationship between a learning strategy , its formal learning framework and its logical representational theory . 
Introduction $ 1 $ 39 $ 30 $ KP $ Once the corpus was segmented the next step was to align it . 
_WSJ910702-0053_36_At_a ,_meeting_in_South_Africa_this_week ,_the_African_National_Congress ,_the_major_black $ 9 $ 189 $ 10 $ CRF $ This paper presented a statistical method of generating extraction based multi-document summaries . 
OUT $ -1 $ 75 $ 55 $ CRF $ The/TERM Common/TERM CJK/TERM Ideograph/TERM section/TERM of the Unicode encoding scheme includes/DEF all characters encoded in each individual language and encoding scheme ./O 
Theoretical_Ideas , $ 2 $ 120 $ 95 $ PATTERN $ There is one script/TERM interpreter/TERM , which functions/DEF both as a script executive and a script evaluator ,/O and one set of rules/TERM which defines/DEF the procedural semantics of script actions ./O 
_Relations_that_are_relevant_and_correct_in $ 3 $ 185 $ 151 $ CRF $ We took the simplifying assumption that WordNet be complete , thus aiming at assigning at least one WordNet sense to each term that appeared in both WordNet and ASRS . 
Abstract $ 0 $ 1 $ 0 $ CRF $ This paper describes an automatic method for extracting systematic polysemy from a/DEF hierarchically organized semantic lexicon (/O WordNet/TERM ) . 
Generation_of_Vague_Descriptions $ 4 $ 85 $ 4 $ CRF $ We will take them to be of the form n crn , where n/TERM is a/DEF positive natural number ./O 
Generation_and_linguistic_representation $ 4 $ 105 $ 33 $ CRF $ Our device for this is a construction/TERM SYNC/TERM which pairs/DEF a description of a gesture G with the syntactic structure of a spoken constituent c :/O SYNC ( 2 ) G C The temporal interpretation of ( 2 ) mirrors the rules for surface synchrony between speech and gesture presented in ( Cassell et al. , 1994 ) . 
OUT $ -1 $ 75 $ 74 $ PATTERN $ The value of different subjects is measured as follows . 
_Department_of_State ,_State_Department ,_State $ 5 $ 133 $ 1 $ PATTERN $ We will pay special attention to localcontent/TERM collocations/TERM , as they are the/DEF strongest , and also closer to strict definitions of collocation ./O 
Abstract $ 0 $ 1 $ 0 $ CRF $ This paper describes a method of comparing corpora which uses frequency profiling . 
Setting $ 3 $ 80 $ 1 $ KP $ This corpus was collected by Ng and colleagues ( Ng and Lee , 1996 ) and it is available from the Linguistic Data Consortium ( LDC ) 5 . 
Abstract $ 0 $ 47 $ 2 $ KP $ Semantic ( Concrete ) semantic representations provide a complete notation for " logical forms " where there is no longer any reference to ,the knowledge base . 
Generalisation_operators $ 4 $ 115 $ 2 $ CRF $ We will use the artificial dataset given in Fig 4 which displays 4 different patterns of gap threading . 
The_Learning_System $ 2 $ 58 $ 32 $ PATTERN $ As a result , intransitive/DEF verbs are/O defined as S\NP/TERM , figure 1 , for the grammar to account for these sentences . 
Learning_RRE_Rules $ 4 $ 74 $ 21 $ PATTERN $ The root node corresponds to the null RRE , and so the position/TERM set/TERM consists of the/DEF beginning of each string in the training set ./O 
The_model $ 1 $ 30 $ 7 $ KP $ The hotel Regina has thirty single rooms The hotel/TERM Regina/TERM is an/DEF expensive hotel ./O 
YAG ' s_Template_Specification $ 2 $ 43 $ 0 $ CRF $ Language A template/TERM is a/DEF pre-defined form with parameters that are specified by either the user or the application at run-time ./O 
OUT $ -1 $ 209 $ 155 $ KP $ Our test/TERM queries/TERM are real/DEF world queries that express a concrete information need ./O 
Method $ 2 $ 66 $ 21 $ CRF $ pro ( 1 _p ) n-m ( 2 ) The probability of the event happening m or more times is : = ( 3 ) k=rn Finally , P/TERM (/TERM m+/TERM , n/TERM , p/TERM e/TERM )/TERM is the/DEF probability that m or more occurrences of cues for scfi will occur with a verb which is not a member ofscfi , given n occurrences of that verb ./O 
Related_Work $ 4 $ 127 $ 36 $ KP $ From the result state of each alternative , the planner then tries to predict the reaction of A by simulating the execution of the React action by A ( see figure 5 ) , and commits to the plan whose resulting state after the predicted reaction yields the greater utility according to B ' s preferences ( see Figure 6 ) . 
Abstract $ 0 $ 37 $ 35 $ KP $ Fullfledged QA on the basis of natural language texts is far beyond the present state of the art . 
Adequacy $ 4 $ 115 $ 15 $ CRF $ Regarding unambignity , the scheme is an improvement but has one failing : repetition of a letter represents either extended duration or the presence of multiple syllables . 
Introduction $ 1 $ 17 $ 13 $ KP $ An event/TERM cluster/TERM , produced by a TDT system , consists/DEF of chronologically ordered news articles from multiple sources , which describe an event as it develops over time ./O 
Overview_of_the_Approach $ 2 $ 47 $ 27 $ PATTERN $ Let ' s go through a simple trace of parsing the request " What is the capital of Texas?
OUT $ -1 $ 2 $ 2 $ KP $ Our work exploits chunking in two principal ways . 
Proof_of_concept_generator $ 4 $ 144 $ 11 $ PATTERN $ The output text is a sub-task including a title and instructions of different types ( only the first three instructions are given in the Figures ) to be performed by the same person ( e . g . 
Experiments $ 5 $ 99 $ 5 $ CRF $ `` F-score/TERM '' is a/DEF measurement combining `` Recall '' and `` Predsion '' and/O defined in Equation 3 . 
Experimental_Setup $ 5 $ 116 $ 3 $ KP $ After replacing the words with part-of-speech tags , the vocabulary size of the corpus is reduced to 47 tags . 
Analysing_Czech_texts $ 3 $ 105 $ 34 $ CRF $ One of the attributes is the analytic/TERM function/TERM that expresses/DEF the syntactic function of the word ./O 
Introduction $ 1 $ 34 $ 23 $ KP $ LLR can be used in a form ( -2logA ) which is X 2 distributed . 
Abstract $ 0 $ 128 $ 4 $ CRF $ These studies frequently involve the construction of an idealized/TERM language/TERM sample/TERM which is ( at best ) an/DEF accurate subset of sentences that a child might hear ./O 
Error-driven_Learning $ 4 $ 119 $ 2 $ PATTERN $ For a new lexical entry e i , the effectiveness/TERM F~/TERM (/TERM e/TERM i/TERM )/TERM is measured/DEF by the reduction in error which results from adding the lexical entry to -~ Error ( e , ) ./O 
Complexity_Formulas $ 4 $ 56 $ 0 $ KP $ Now that the domain is specified , we can analyze its semantics by estimating the number of bits of information conveyed by referring to each different aspect of the domain . 
Results $ 5 $ 133 $ 37 $ KP $ ture hyperspace the number of class boundaries to be learned per bit function reduces . 
_Introduction $ 1 $ 16 $ 12 $ PATTERN $ In the present study , we treat the task of determining the appropriate distribution of mentions in text as a machine learning classification problem : what is the probability that a mention will have a certain grammatical relation given a deh set of linguistic features?
The_remaining_pronouns_are_not_in_third_person_or $ 9 $ 274 $ 1 $ KP $ J0 For instance : " all the pronouns in third person and singular whose antecedents are proper nouns have boon translated into he ( antecedent with masculine gender ) or she ( antecedent with feminine gender ) ; otherwise they have been translated into it " . 
Abstract $ 0 $ 3 $ 2 $ KP $ The parsers combine lexical indices such as discourse/DEF markers with formatting instructions (/O HTML/TERM tags/TERM ) for analyzing enumerations and associated initializers . 
Models $ 2 $ 57 $ 37 $ PATTERN $ p ( TIS ) -1~IT] , where p/TERM is the/DEF model being evaluated ,/O and (/TERM S/TERM ,/TERM T/TERM )/TERM is the/DEF test corpus ,/O considered to be a set of statistically independent sentence p ( w[hi ,s ) = q ( wlhi ) exp ( ~ses asw + aA ( i ,j~ ,O ,B ( s ,t ) ) pair s ( s ,t ) . 
Highlight $ 3 $ 81 $ 30 $ CRF $ Keyword/TERM based/TERM search/TERM is a/DEF special case where the user specifies one or more keywords which they want to find in a document ./O 
Collaborative_Agents $ 1 $ 68 $ 64 $ PATTERN $ AGENT : " There is a conflict of meeting with Brian at three P . M . Thursday with meeting with Irene Landoz at three P . M . 
Given_an_input_space_X~*_of $ 3 $ 110 $ 79 $ PATTERN $ The motivation is that we now assume that a context for a word belonging ( also to ) Ct is a valid context for any word in that category . 
Learning_Frameworks $ 2 $ 28 $ 8 $ KP $ For example , when using Hidden/TERM Markov/TERM Model/TERM ( HMM/ACR ) as a generative model for pos tagging , estimating the probability of a sequence of tags involves assuming that the pos tag ti of the word wi is independent of other words in the sentence , given the preceding tag ti-1 . 
Introduction $ 1 $ 12 $ 4 $ CRF $ This situation raises an old question and opens a new area of research : can one separate content from presentation ?
OUT $ -1 $ 137 $ 95 $ PATTERN $ The following describes the active learning algorithm used in the experiments : TBLDT ) to obtain chunk probabilities on the rest of the training data ; ing set , specifically the samples that optimize an evaluation function f , based on the class distribution probability of each sample ; fication 3 to the training pool and retrain the system ; 5 . 
Note_that_the_nouns_at_this_point_are_types_not $ 4 $ 173 $ 58 $ PATTERN $ The most interesting result is the result of the MSp condition in table 1 , which indicates that 81.8% of the target data can be sense tagged with an accuracy of 79% , significantly higher than chance ( 25.6% ) as well as it is higher than the default tagging of 67.6% . 
Abstract $ 0 $ 54 $ 0 $ CRF $ the linear ordering of the constituents of the Rhetorical Representation with a POSITION feature , as well as two other features , TEXT-LEVEL/TERM , which takes/DEF values such as paragraph or sentence ;/O and LAYOUT/TERM , which takes/DEF values such as wrapped-text and vertical list ./O 
Experiment $ 3 $ 104 $ 22 $ CRF $ In order to provide a better estimate of how close two discourse trees were , we computed PositionDependent and -Independent recall and precision figures for the sentential/TERM level/TERM ( where/DEF units are given by edus and spans are given by sets of edus or single sentences )/O ; paragraph/TERM level/TERM ( where/DEF units are given by sentences and spans are given by sets of sentences or single paragraphs )/O : and text/TERM level/TERM ( where/DEF units are given by paragraphs and spans are given by sets of paragraphs )/O . 
Abstract $ 0 $ 7 $ 4 $ KP $ An overview of HowNet and information structure are described in this paper . 
System_Description $ 2 $ 15 $ 4 $ CRF $ In addition to the control module , which wires together the other modules , there are six modules in • GoDiS : input/TERM , which receives/DEF input from the user ;/O interpret/TERM , which interprets/DEF utterances as dialogue moves with some content ;/O generate/TERM , which generates/DEF natural language from dialogue moves ;/O output/TERM , which produces/DEF output to the user ;/O update/TERM , which updates/DEF the information state based on interpreted moves ;/O and select/TERM , which selects/DEF the next move ( s ) to perform ./O 
Text_Summarization $ 1 $ 25 $ 5 $ PATTERN $ In regard to the position method , Hovy and Lin ( 1997 ) considered the title is the most likely to bear topics . 
Examining_SCF_Correlation $ 2 $ 40 $ 9 $ PATTERN $ The latter verbs were chosen so that one of the verbs is a synonym , and the other a hypernym , of a test verb . 
OUT $ -1 $ 0 $ 0 $ KP $ Multi-Document Summarization By Sentence Extraction Jade Goldstein* Vibhu Mittal t Jaime Carbonell* Mark Kantrowitzt jade@cs . cmu . edu mittal@jprc . com jgc@cs . cmu . edu mkant@jprc . com *Language Technologies Institute Carnegie Mellon University Pittsburgh , PA 15213 U . S . A . tJust Research 4616 Henry Street Pittsburgh , PA 15213 U . S . A . 
Implementing_PbA $ 3 $ 41 $ 0 $ KP $ In PbA , an unknown word is pronounced by matching substrings of the input to substrings of known , lexical words , hypothesizing a partial pronunciation for each matched substring from the phonological knowledge , and assembling the partial pronunciations . 
The_steps_in_the_strategy_are_marked_with_the $ 7 $ 154 $ 32 $ PATTERN $ Finally , ( Klein 1994 ) is the previous work most relevant to our proposal . 
Normalization_and_tagging $ 3 $ 57 $ 0 $ KP $ The label normalization groups three components , which clean up and tokenize the input . 
Dialogue_Management $ 2 $ 126 $ 82 $ PATTERN $ Communicative/TERM space/TERM is defined by a/DEF number of coordinates that characterise the relationships of participants in a communicative encounter ./O 
Introduction $ 1 $ 14 $ 7 $ PATTERN $ The distribution environment of a word is the set of words of other parts of speech that can be collocated with it . 
Surface_realization_as_grammatical $ 5 $ 142 $ 0 $ PATTERN $ competition The resulting input for grammatical competition is a blend of semantic and pragmatic information . 
Previous_work $ 2 $ 27 $ 3 $ CRF $ D~Jean ( 1998 ) uses an approach derived from Harris ( 1951 ) where word-splitting occurs if the number of distinct letters that follows a given sequence of characters surpasses a threshoid . 
Conclusions $ 6 $ 144 $ 5 $ CRF $ This explanation of the motivations leading to cooperation provides an explicative model that is uniform with the treatment of deontic reasoning in agent theories ( Conte et al . , 1998 ) , ( Boella and Lesmo , 2000 ) and the definition of cooperation proposed in ( Boella et al . , 2000 ) . 
Introduction $ 1 $ 38 $ 30 $ PATTERN $ Our classification , which is a further development of the scheme in Teufel and Moens ( 1999 ) , can be described procedurally as a decision tree ( Figure 2 ) , where five questions are asked about each sentence , concerning intellectual attribution , author stance and continuation vs . contrast . 
_IBM_expected_[SBAa_each_employee_to $ 5 $ 43 $ 31 $ CRF $ Most recently , in ( Stetina et al . , 1998 ) , the authors made use of head-driven bilexical dependencies with syntactic relations to attack the problem of generalized word-sense disambiguation , precisely one of the two problems we are dealing with here . 
Abstract $ 0 $ 4 $ 0 $ KP $ The bigram language models are popular , in much language processing applications , in both Indo-European and Asian languages . 
System_Overview $ 2 $ 37 $ 11 $ CRF $ A bottom-up algorithm then constructs a lattice that encodes the strings represented by each level of the derivation tree . 
Feature_Selection_and_Extraction $ 2 $ 35 $ 13 $ PATTERN $ During keyword extraction , the document is first segmented and converted into a keyword frequency vector ( t fl , t f2 , . . . , t . f M ) , where tfi/TERM is the/DEF in-document term frequency of keyword wi ,/O and M/TERM is the/DEF number of the keyword features selected ./O 
Examples_of_LT_XML_Queries $ 4 $ 45 $ 0 $ KP $ The default category of a tag is given by the ration between its number of occurrences in the structure we want to recognise and the number of occurrences in the training corpus . 
Selective_Sampling_Evaluation $ 4 $ 96 $ 20 $ CRF $ The entropy/TERM H/TERM ( V/TERM ) is the/DEF expected negative log likelihood of random variable V : H ( V ) = -EX ( logdv ( V ) ) ) ./O 
Abstract $ 0 $ 35 $ 34 $ KP $ By partial parsing and skip strategy , this parser can handle long , complicated , or even faulty sentences . 
Using_KeyWords $ 1 $ 31 $ 0 $ CRF $ A KeyWord/TERM list/TERM is a/DEF portion of the study corpus word list ./O 
Overview_of_Ontology $ 2 $ 27 $ 0 $ CRF $ An ontology/TERM is a/DEF body of knowledge about the world ./O 
Generation_of_Crisp_Descriptions $ 3 $ 79 $ 25 $ PATTERN $ In the present context , however , the limitation is inessential , since what is crucial here is the interaction between an Adjective and a ( possibly complex ) Common Noun following it : in more complex constructs of the form ' NP and the Adj CN ' , only CN affects the meaning of Adj . 
Extending_Domain_Semantics_for $ 5 $ 175 $ 18 $ CRF $ He is the Head of Division and is a professor . 
Abstract $ 0 $ 201 $ 31 $ PATTERN $ The value in column ' Quality ' is the average acceptability for the abstract . 
Abstract $ 0 $ 19 $ 17 $ KP $ MT supports the translingual aspects of the effort and can be viewed as an embedded tool which facilitates other technologies . 
Theoretical_Ideas , $ 2 $ 27 $ 2 $ PATTERN $ Logic/TERM is indeed an/DEF excellent way to think about representing static relationships like database queries ,/O but it is much less clear that it is a good way to represent commands . 
Centroid-based_summarization $ 4 $ 80 $ 8 $ CRF $ Ci/TERM is the/DEF centroid score of the sentence ,/O P~/TERM is the/DEF positional score of the sentence ,/O and F~/TERM is the/DEF score of the sentence according to the overlap with the first sentence of the document ./O 
Abstract $ 0 $ 14 $ 4 $ CRF $ The type/TERM of/TERM an/TERM LCS/TERM node/TERM is one/DEF of Event , State , Path , Manner , Property or Thing ,/O loosely correlated with verbs prepositions , adverbs , adjectives and nouns . 
Introduction $ 1 $ 48 $ 39 $ CRF $ In this syntactic category , we single out a subclass of sentences according to one possible semantic connection between the two clauses . 
Maximum_Entropy_models $ 2 $ 13 $ 9 $ CRF $ e~ i Aifi ( h , w ) P ( wlh ) = Z ( h ) where fi/TERM (/TERM h/TERM ,/TERM w/TERM )/TERM refers to a/DEF ( binary valued ) feature function that describes a certain event ;/O Ai/TERM is a/DEF parameter that indicates how important feature fi is for the model and/O Z/TERM (/TERM h/TERM )/TERM is a/DEF normalisation factor ./O 
Evaluation_and_results $ 4 $ 67 $ 4 $ CRF $ Weighted/TERM accuracy/TERM is a/DEF measure that weights higher the hits and misses 100 for the preferred class ./O 
OUT $ -1 $ 17 $ 17 $ KP $ Using Han character oriented document and query vectors , within the framework of the vector space information retrieval , we then evaluate the effectiveness of the cross language IR with respect to their monolingual counterparts . 
Introduction $ 1 $ 5 $ 1 $ KP $ Annotated dialogue corpora are of crucial importance for the development of vocal applications . 
Experimental_results $ 3 $ 56 $ 2 $ KP $ Corpus/TERM A/TERM consists of local/DEF news with more than 325 million characters ./O 
LTAGs_and_Extraction $ 3 $ 52 $ 25 $ PATTERN $ X m is further expanded into a spine-etree whose head X ° is the anchor of the whole mod-etree . 
Abstract $ 0 $ 20 $ 18 $ CRF $ Unlike previous approaches , their system summarizes a series of news articles on the same event , producing a paragraph consisting of one or more sentences . 
Statistical_Semantic_Parsing $ 4 $ 101 $ 2 $ PATTERN $ A parse/TERM state/TERM consists of a/DEF stack of lexicalized predicates and a list of words from the input sentence ./O 
Levels_of_Annotation $ 4 $ 94 $ 0 $ KP $ The ADAM' s five levels of annotation were mainly chosen in consideration of their interest for practical applications of the annotated material . 
Summary $ 6 $ 226 $ 13 $ PATTERN $ Simplifying the domain specification task is a necessity as text generation systems move outside of research labs and into the real world , where the domain developer may not be a computational linguist , but a museum curator , personnel officer or wine salesman . 
Architecture $ 2 $ 63 $ 19 $ PATTERN $ One task is the acquisition of new structures , the second task is the evaluation of given structures . 
Generation_of_Crisp_Descriptions $ 3 $ 63 $ 9 $ CRF $ 4 Informally and forgetting about the special treatment of head nouns what happens is the following : Tile algorithm iterates through a list P in which the properties appear in order of ' preference ' ; for each attribute , it checks whether specifying a value for that attribute would rule out at least one additional member of C ; if so , the attribute is added to L , with a suitable value . 
Problem_Space_Modeling $ 2 $ 55 $ 28 $ CRF $ Each timeout has a reward and a punishment . 
Towards_building_a_parallel_corpus $ 2 $ 36 $ 4 $ PATTERN $ At the elementary unit level , the correspondence between Japanese sentence ( 4 ) and its English translation ( 6 ) can be represented as in ( 7 ) , where jC-e/TERM denotes the/DEF fact that the semantic content of unit j is realized fully in unit e ;/O jD-e/TERM denotes the/DEF fact that the semantic content of unit e is realized fully in unit ./O 
The_computation_of_the_velocity_is_easily ,_done_from $ 4 $ 184 $ 31 $ PATTERN $ ( In step 2 the ' new ' node is the ' old " one , not the problem node!
The_hyperonym_problem $ 2 $ 33 $ 15 $ PATTERN $ on the other hand , it can usually be ignored , as most of today ' s practical applications either do not require the production of a more general word ( i.e. . there is a one-to-one mapping from concept to word ) or can rely on fairly simple mechanisms that . ,avoid ,lexical repetitions bv choosing from a fixed , pre-defined set of near-synonyms . 
Given_an_input_space_X~*_of $ 3 $ 41 $ 10 $ PATTERN $ Because S is a finite sequence , only concepts with a finite number of positive examples can be learned with total success , i.e. 
Genetic_Algorithms_for_Assigning $ 2 $ 61 $ 43 $ CRF $ The different values for crossover ranged from 0.65 to 0.95 , in steps of 0.05 . 
Tagged_Text $ 3 $ 237 $ 223 $ PATTERN $ The next most dominant feature is the SemType value object , which appears in the NP for 29.41% of the answer sentences and PP NeedSemType for 15.68% of the answer sentences . 
OUT $ -1 $ 112 $ 10 $ CRF $ Figure 5 shows a process of the compound noun indexing with an example . 
Abstract $ 0 $ 38 $ 5 $ CRF $ Face to face with the reality of use this realization has been most widely accepted in areas of linguistics which deal with language acquisition and teaching . 
Middle $ 6 $ 213 $ 176 $ PATTERN $ The overall performance in the sense tagger is 76.04% . 
Background : _The_STOP_System $ 2 $ 27 $ 10 $ PATTERN $ STOP/TERM is a/DEF different type of application in that ( 1 ) there are many possible leaflets which can be generated ( and the system cannot tell which is best ) , and ( 2 ) no human currently writes personalised smoking-cessation leaflets ( because manually writing such leaflets is too expensive ) ./O 
Tree_Structures $ 2 $ 29 $ 13 $ PATTERN $ The lower part represents the phonological phrases into which the whole sentence is divided by the binary structure , and uses the same representation levels as in the syntactic structure . 
Extending_a_Grammar_to_Enable $ 4 $ 121 $ 48 $ PATTERN $ " , in favor of " his sister ' s dog " , without the application having to request a pronoun explicitly , as in the example shown above , we could add a rule to force the pronominal feature of the inner most possessor to be YES , whenever a ( repeated ) noun phrase is a possessor of a possessor of the primary noun . 
Background $ 2 $ 88 $ 68 $ CRF $ 2 inforrnPositive/TERM (/TERM p=v/TERM )/TERM : user/DEF confirms that the value of parameter p is v ./O p E params ( AD ) U { aTask } . 
Abstract $ 0 $ 2 $ 0 $ CRF $ This paper describes an implemented system which uses centering theory for planning of coherent texts and choice of referring expressions . 
_The_session_was_also_attended_by_observers_from_the_following_international_organizations : $ 7 $ 109 $ 58 $ PATTERN $ The productivity is the ratio of the number of candidates to the size of the collection . 
Hyperonymy_in_lexical_semantics $ 5 $ 121 $ 24 $ PATTERN $ There is a general rule that requires speakers to use this term in order to obtain an unmarked utterance in a given context : - : - . unless . this would result in an ' abnormal communication ' , in which case the speaker should deviate from neutral level , but only to the minimum degree required to ensure normality . 
Comparison_experiment $ 3 $ 139 $ 41 $ CRF $ ( who is the father of the relativity theory ?
Generation_of_Multiple_Quantifiers $ 5 $ 208 $ 23 $ PATTERN $ But in " Every patient with a balloon pump had hypertension " , the existentially quantified expression " with a balloon pump " is a restrictive modifier of its head . 
The_Complexity_of_Extracting_a $ 2 $ 36 $ 24 $ PATTERN $ The semantic/TERM vicinity/TERM of/TERM a/TERM node/TERM in a network consists of the/DEF nodes and the arcs reachable from that node by traversing a small number of arcs ./O 
Dialogue_Structure_and $ 6 $ 119 $ 34 $ PATTERN $ Elliptical/TERM coupling/TERM is the/DEF pattern of [A : I] [B : I A : R] , equivalent to the one in which B ' s second response is omitted in coupling ./O 
The_MATE_Approach $ 2 $ 37 $ 1 $ PATTERN $ Then it describes how a toolbox ( the MATE Workbench ) has been implemented to support the markup framework by enabling annotation on the basis of any coding scheme expressed according to the framework . 
Research_focus : _content_aggregation $ 2 $ 97 $ 36 $ CRF $ This time it is the product dimension that has been left-shifted and that provided the basis for row sorting , row grouping and cell merging . 
Dependency_Analysis_using $ 3 $ 67 $ 4 $ PATTERN $ Statistical/TERM dependency/TERM structure/TERM analysis/TERM is defined as a/DEF searching problem for the dependency pattern D that maximizes the conditional probability P ( DIB ) of the in20 put sequence under the above-mentioned constraints ./O 
Introduction $ 1 $ 20 $ 13 $ PATTERN $ Considering this , we assumed that it is a very promising domain for an experimental dialogue system . 
Learning_Algorithms_Tested $ 2 $ 31 $ 11 $ PATTERN $ For k ' s greater than 1 , the resulting sense is the weighted majority sense of the k nearest neighbours --where each example votes its sense with a strength proportional to its closeness to the test example . 
Introduction $ 1 $ 11 $ 5 $ KP $ PbA requires a dictionary in which text and phonemics have been aligned , so that pronunciations corresponding to matching orthographic substrings can be identified . 
Approach $ 2 $ 39 $ 33 $ PATTERN $ The first is the singlepass method . 
Discussion $ 6 $ 125 $ 11 $ CRF $ If the extended information is not explicitly presented in the text , the default value provides the assumption based on the world knowledge . 
Translations_selection_and_phrase $ 2 $ 59 $ 9 $ PATTERN $ e1/TERM ,/TERM e2/TERM ,/TERM .../TERM ,/TERM e/TERM , are the/DEF segmented Chinese words of the query after removing the stop words ./O 
Learning_Linear_Classifiers $ 4 $ 115 $ 20 $ CRF $ SNoW determines the features ' weights using an on-line/TERM algorithm/TERM that attempts/DEF to minimize the number of mistakes on the training data using a multiplicative weight update rule (/O Lit88 ) . 
Abstract $ 0 $ 29 $ 4 $ PATTERN $ Another cause of error is the lack of information about abstract nominals . 
Middle $ 6 $ 56 $ 19 $ CRF $ A word/TERM token/TERM is an/DEF occurrence of a type in the corpus ./O 
Middle $ 6 $ 149 $ 112 $ PATTERN $ ( 1 ) IfSj/TERM is the/DEF only one syuset that has been mapped to Cilin tags ,/O we choose a Cilin tag and map Si to it . 
Comments $ 6 $ 75 $ 15 $ PATTERN $ Overall accuracy : 94.88% phrasal recognition ) is a fairly easy task . 
Related_Work $ 7 $ 170 $ 1 $ PATTERN $ ( 2000 ) describes a text mining tool that performs document clustering and text summarization . 
Dialogue_Management $ 2 $ 165 $ 121 $ PATTERN $ The static/TERM part/TERM consists/DEF of preconditions , goal , content ( immediate act ) and consequences ./O 
Introduction $ 1 $ 3 $ 0 $ KP $ As conversational systems move from the realm of science fiction and research labs into people ' s everyday life , and as they evolve from the plain , systemdirected interactions ~ la press or say one of socalled interactive voice response systems based on isolated-word recognizers and fixed-menu navigation , to the more open , mixed-initiative dialogues carried out in spoken dialogue systems based on large-vocabulary continuous speech recognizers and flexible dialogue managers ( see , e . g . , ( Allen et al . , 1996 ; Denecke , 1997 ; Walker et al . , 1998 ; Rudnicky et al . , 1999 ; Zue et al . , 2000 ) ) , the overall experiential quality of the human-computer interaction becomes increasingly important . 
Content_Presentation $ 4 $ 110 $ 17 $ PATTERN $ The informative/TERM abstract/TERM is the/DEF information obtained/DEF by this process as it is shown in Figure 1 ./O 
Qualitative_Evaluation_of_the $ 4 $ 170 $ 69 $ CRF $ We therefore use models that just use the simple tree accuracy and the number of substitutions as independent variables• Second , we note that once we have done so , a perfect sentence gets a score of 0.8689 ( for understandability ) or 0.6639 ( for quality ) . 
Maximum_Entropy_Modeling $ 2 $ 54 $ 12 $ PATTERN $ Given a training sample of size N , ( Xl ,Yl ) , . . , ( XN ,YN ) , an empirical/TERM probability/TERM distribution/TERM can be defined as y/DEF ) = y ) N where # ( x , y ) is the number of occurrences of ( x , y ) ./O 
Storing_the_corpus_in_a_database $ 4 $ 132 $ 11 $ PATTERN $ The colnmn/TERM cl_id/TERM in the table node_pair_/ , for example , is a/DEF foreign key referring to the colnmn clad in the table pair_class ./O 
Introduction $ 1 $ 32 $ 16 $ CRF $ The column labeled idf/TERM is the/DEF mean idf for the terms in each bin ./O 
Abstract $ 0 $ 2 $ 1 $ KP $ The query tool is developed to search the Verbmobil treebanks annotated at the University of Tfibingen . 
OUT $ -1 $ 20 $ 19 $ PATTERN $ Then , we provide details on the measures of argument strength and importance used in selecting and ordering argument support . 
Implementation $ 3 $ 82 $ 0 $ CRF $ From an implementation point of view , the core of TransType relies on a flexible object oriented architecture , which facilitates the integration of any model that can predict units ( words or sequence of words ) from what has been already typed and the source text being translated . 
Content_Presentation $ 4 $ 98 $ 5 $ CRF $ Instead other types require additional re-generation : for the topic of the document template the generation/TERM procedure/TERM is as follows : (/DEF i ) the verb form for the predicate in the Predicate slot is generated in the present tense ( topical information is always reported in present tense ) , 3rd person of singular in active voice at the beginning of the sentence ; ( ii ) the parsed sentence fragment from the N ' hat slot is generated in the middle of the sentence ( so the appropriate case for the first element ) ./O  
Evaluation_Measures $ 2 $ 52 $ 27 $ PATTERN $ The recall-based measures introduce a bias since they are based on the Opinions of a small number of assessors . 
Abstract $ 0 $ 4 $ 3 $ KP $ REXTOR/TERM (/TERM Relations/TERM EXtracTOR/TERM )/TERM is an/DEF implementation of this model ; in one uniform framework , the system provides two separate grammars for extracting arbitrary patterns of text and building ternary expressions from them ./O 
Prerequisites $ 2 $ 19 $ 0 $ CRF $ VERBMOBIL/TERM is a/DEF speech-to-speech translation project , which at present is approaching its end and in which over 100 researchers 1 at academic and industrial sites are developing a translation system for multilingual negotiation dialogues ( held face to face or via telephone ) using English , German , and Japanese ./O 
Given_an_input_space_X~*_of $ 3 $ 85 $ 54 $ CRF $ Given an unknown word w ' occurring in a context represented by f ' k , the WSD algorithm assigns w ' to the category in C that maximizes the similarity between f ' k and one of its members . 
Dialogue_Management $ 2 $ 89 $ 45 $ PATTERN $ The second part of the reasoning model consists of reasoning schemas , that supposedly regulate human action-oriented reasoning . 
Stochastic_Surface_Realization $ 2 $ 79 $ 21 $ CRF $ act-query content depart_time depart_city New York arrive_city San Francisco depart_date 19991117 } Figure 4 : an input frame to NLG The generation engine uses the appropriate language model for the utterance class and generates word sequences randomly according to the language model distributions . 
Conclusion $ 4 $ 131 $ 13 $ PATTERN $ Finally , what may sound right for a human speaker may sound awkward for a computer , but we believe that mimicking a human , especially a domain expert , is the best we can do , at least for now . 
Tree_Generalization_using_Tree-cut $ 2 $ 48 $ 2 $ PATTERN $ A thesaurus/TERM tree/TERM is a/DEF hierarchically organized lexicon where leaf nodes encode lexical data 21 ( i.e. , words ) and internal nodes represent abstract semantic classes ./O
Abstract $ 0 $ 5 $ 0 $ PATTERN $ Weighted/TERM Probability/TERM Distribution/TERM Voting/TERM ( WPDV/TERM ) is a/DEF newly designed machine learning algorithm , for which research is currently aimed at the determination of good weighting schemes ./O 
Summarization_Filters $ 4 $ 130 $ 37 $ CRF $ For example , Figure 2 illustrates a complex filter created by using a GUI to compose together a named entity extractor , a date extractor , a component which discovers significant associations between the two and writes the result to a table , and a visualizer which plots the results as a graph . 
Abstract $ 0 $ 1 $ 0 $ CRF $ This paper describes a Japanese dialogue corpus annotated with multi-level information built by the Japanese Discourse Research Initiative , Japanese Society for Artificial Intelligence . 
OUT $ -1 $ 58 $ 53 $ CRF $ Also , an answer/TERM as defined in the TREC-8 QA task is a/DEF 50-byte or 250byte answer string ,/O whereas an answer/TERM is a/DEF complete sentence in the reading comprehension task ./O 
Introduction $ 1 $ 6 $ 1 $ KP $ Topic/TERM analysis/TERM consists/DEF of two main tasks : topic identification and text segmentation (/O based on topic changes ) . 
Motivation $ 2 $ 36 $ 0 $ PATTERN $ We believe that , for humans , natural language is the best mechanism for information access . 
_KNOWLEDGE_EXTRACTION $ 3 $ 89 $ 31 $ CRF $ The samples of organizations from the CKIP dictionary As we observed , the morphological structure of an organization name usually is a compounding of a proper name and a organization type . 
Introduction $ 1 $ 16 $ 8 $ CRF $ Therefore , they offer the following approximation : P ( w . ilw -1 ) wiJwi_N+l ) ( I ) A common value for N is 2 ( bigram language model ) or 3 ( trigram language model ) ; only a short local context of one or two words is considered . 
Corpus_comparison_based_on $ 6 $ 112 $ 3 $ PATTERN $ The entropy/TERM for/TERM NE/TERM classes/TERM H/TERM (/TERM C/TERM )/TERM is defined by = E/DEF p ( c ) log 2 p ( c ) H ( C ) cEC where/O : n ( O p ( c ) = " N n/TERM (/TERM c/TERM )/TERM : the/DEF number of words in class c N/TERM : the/DEF total number of words in text We can calculate the entropy for features in the same way ./O   
Domain_Terms $ 3 $ 38 $ 7 $ PATTERN $ Attributes of an object are the things that the program needs to know about the object in order to use it in the domain . 
Abstract $ 0 $ 4 $ 0 $ KP $ We present discourse annotation work aimed at constructing a parallel corpus of Rhetorical Structure trees for a collection of Japanese texts and their corresponding English translations . 
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 238 $ 203 $ PATTERN $ ~Doc/TERM denotes the/DEF number of documents ./O 
The_TransType_model $ 2 $ 33 $ 18 $ CRF $ The core/TERM of/TERM TRANSTYPE/TERM is a/DEF completion engine which comprises two main parts : an evaluator which assigns probabilistic scores to completion ./O 
Quality_of_the_first_order_weights $ 4 $ 101 $ 43 $ PATTERN $ The disadvantage is a much more time-intensive hill-climbing procedure , but when developing an actual production model , the weights only have to be determined once and the results appear to be worth it most of the time . 
Abstract $ 0 $ 38 $ 11 $ PATTERN $ The two-and-a-half-year VIRPI project consists of three parts . 
Introduction $ 1 $ 32 $ 19 $ KP $ Attributes in their system assist the realization by propagating information down a tree that specifies the complete syntactic structure of the output text . 
Task_description $ 2 $ 13 $ 4 $ KP $ A tag next to the open bracket denotes the type of the chunk . 
OUT $ -1 $ 41 $ 37 $ KP $ Each textual module has a particular communicative goal and a precise theme according to the ATA 100 norms . 
Introduction $ 1 $ 13 $ 6 $ KP $ An ontology/TERM is a/DEF set of knowledge concepts about the world ./O 
About_Theory_Refinement $ 2 $ 16 $ 8 $ PATTERN $ ples : ( a ) Identify the revision points ( b ) Correct them The first step consists of acquiring an initial grammar ( or more generally a knowledge base ) . 
Experiments $ 4 $ 87 $ 8 $ PATTERN $ GRAPHON/TERM , finally , is/DEF a grapheme-to-phoneme conversion task for English based on the English Celex lexical database ./O 
Levels_of_Annotation $ 4 $ 95 $ 1 $ KP $ In spite of the number of levels considered , and their sometimes conflicting requirements , we tried to develop a coherent , unitary approach to design and application of annotation schemes . 
The_CGS_system $ 4 $ 69 $ 1 $ KP $ Its architecture is a pipeline with several modules , shown in the left hand part of Figure 1 . 
Corpus_comparison_based_on $ 6 $ 154 $ 45 $ CRF $ Coverage/TERM means that how/DEF many pairs which appeared in a test set also appear in a trainlug set ./O 
Abstract $ 0 $ 70 $ 39 $ PATTERN $ Moreover , primary discourse markers can also be classified as simple adverbials , as is the case in English : ( I ) Even though a child , John is so tall that he has problem getting half-fare . 
_Interaction_Recorder : _for_the_recording $ 9 $ 171 $ 33 $ CRF $ Our results indicate that machine learning is an effective approach to improving the accuracy of discourse marker tagging . 
Conclusions $ 8 $ 250 $ 1 $ PATTERN $ Topic/TERM analysis/TERM consists of two/DEF main tasks : text segmentation and topic identification ./O 
Introduction $ 1 $ 37 $ 26 $ PATTERN $ Section 3 describes features incorporated into the model and the process of identifying potential segmentation positions . 
Results $ 8 $ 164 $ 3 $ KP $ Concerning tense , our " gold/TERM standard/TERM " is the/DEF set of human translations ./O
Introduction $ 1 $ 16 $ 5 $ PATTERN $ Regardless of this , there is a ceiling on the performance of these systems at around 80% token recall. Where token/TERM recall/TERM is the/DEF percentage of SCF tokens in a sample of manually analysed text that were The approaches to extracting SCF information from corpora have frequently employed statistical methods for filtering ./O 
Introduction $ 1 $ 28 $ 16 $ CRF $ We have chosen Pustejovsky ' s Generative/DEF Lexicon (/O GL/TERM ) framework ( Pustejovsky , 1995 ; Bouillon and Busa , 2000 ) to define what a relevant NV link is , that is , what is a N-V pair in which the N and the V are related by a semantic link which is close , and which can therefore be used to expand indexes . 
Evaluation $ 4 $ 117 $ 13 $ KP $ The improved KL indicates that the method improves the overall accuracy of SCF distributions . 
OUT $ -1 $ 96 $ 54 $ PATTERN $ The splitting criterion used in the experiments is the information gain measure . 
Discussion $ 5 $ 93 $ 0 $ CRF $ We showed that the learnability result of Valiant for learning boolean concepts can be transformed to a learnability result for pattern languages by looking at the transformation of the underlying representational theories ; i.e. 
Tree_Generalization_using_Tree-cut $ 2 $ 49 $ 3 $ CRF $ A tree-cut/TERM is a/DEF partition of a thesaurus tree ./O 
Our_approach_to_Multilingual $ 2 $ 41 $ 14 $ CRF $ In this formaldirectly expressing the choices which uniquely charism , the carrier of meaning is a choice tree ( called aeterize a given document in an homoge~cous class `` abstract tree ' ' in GF ) , a strongly typed object in of documents belonging to the same domain . 
Abstract $ 0 $ 28 $ 26 $ KP $ Since WordSmith Tools is Windows software , it has appealed to a large audience of applied linguists willing to do corpus-based research , to whom this platform is generally the only one that they know how to use . 
Approach_for_Chunk_Identification $ 3 $ 28 $ 1 $ KP $ Each chunk type belongs to I or B tags . 
TMethod $ 2 $ 78 $ 33 $ KP $ The binomial log-likelihood ratio test is simple to calculate . 
Introduction $ 1 $ 23 $ 15 $ KP $ The generator is freely available to the NLG research comnmnity ( see Section 5 below ) . 
_Instrumentalists_not_including_string_players $ 8 $ 97 $ 85 $ CRF $ Experiment 2 : sets of senses that have parallel translations in at least one out of the four target languages . 
Translations_selection_and_phrase $ 2 $ 51 $ 1 $ KP $ This method , however , results in too many ambiguities during the query translation and offers no path to select appropriate ones among the translations . 
_The_session_was_also_attended_by_observers_from_the_following_international_organizations : $ 7 $ 164 $ 113 $ PATTERN $ In this context , there is no clue for deciding whether Ashley Judd is a star or an athlete . 
_The_identity_of_the_speaker ,_denoted_as_the $ 6 $ 129 $ 19 $ CRF $ To that end , three different approaches were used : ( i ) the full/TERM model/TERM : all/DEF variables were used to determine the discriminant functions ;/O ( ii ) the forward/TERM model/TERM : starting/DEF from an empty model , variables were introduced in order to create a reduced model , with a small number of variables ;/O ( iii ) the backward/TERM model/TERM : starting/DEF from the full model , variables were eliminated to create a reduced model ./O 
Introduction $ 1 $ 10 $ 4 $ KP $ A dialogue/TERM move/TERM engine/TERM ( DME/TERM ) updates/DEF the information state on the basis of observed dialogue moves and selects appropriate moves to be performed ./O 
Multilinguality $ 5 $ 180 $ 0 $ KP $ The generation of dialogue scripts and result summaries is fully implemented in VERB~VIoBIL for German and English . 
Abstract $ 0 $ 2 $ 0 $ KP $ A wide range of natural language problems can be viewed as disambiguating between a small set of alternatives based upon the string context surrounding the ambiguity site . 
Evaluation $ 4 $ 104 $ 0 $ CRF $ To evaluate the approach , we took a sample of 20 million words of the BNC and extracted all sentences containing an occurrence of one of the 60 test verbs on average of 3000 citations of each . 
Abstract $ 0 $ 8 $ 5 $ KP $ Features of word senses and the significance of word contexts are analysed and possibility of searching based on word senses instead of mere words is examined . 
Introduction $ 1 $ 10 $ 4 $ KP $ noun phrases that do not contain other noun phrase descendants ( Church , 1988 ) . 
Tree_Metrics $ 3 $ 44 $ 4 $ PATTERN $ Therefore the tree/TERM distance/TERM can be defined as the/DEF cost of the sequence minimizing this sum ./O 
Overview_of_the_system $ 2 $ 17 $ 9 $ KP $ Si/TERM and Sf/TERM stand for the/DEF initial and the final state of chunk whose descriptor is Si ./O 
Word_Clustering $ 3 $ 57 $ 13 $ PATTERN $ Then as in ( Rissanen , 1996 ) , the SC value of w TM relative to a model I in which the presence or absence of w is independent from those of s ( i.e. , a Bernoulli model ) , is calculated as SC ( w TM : I ) = mH + ~ log ~ + log 7r , where m + denotes the number of l ' s in wm . 
Conclusions_&_Further_Work $ 6 $ 218 $ 9 $ PATTERN $ a information retrieval scenario , a semantic document annotation scenario such as described in ( Erdmann et al . , 2000 ) ) will allow us an application-specific evaluation of the ontology using standard measures such as precision and recall . 
Implications_for_NLG $ 5 $ 97 $ 0 $ KP $ For many NLG applications , the notion of compatibility defined above is a useful hard constraint ; even if violations of this constraint are sometimes acceptable , they are not essential . 
Summary_and_Future_Work $ 4 $ 208 $ 4 $ PATTERN $ If the model-based and language-specific features are aggregated as a single feature vector , the recall and precision of errors are 83% and 35% , respectively , which are the same if we just use language-specific features . 
The_lexicon_size_of_a_typical_large-vocabulary $ 9 $ 109 $ 30 $ CRF $ One way of dealing with multiple translations is to weight the alternative translations using either a statistical translation model trained on parallel or comparable corpora to estimate translation probability conditioned on the source language term . 
System_Overview $ 3 $ 54 $ 18 $ PATTERN $ One conjoined constituent is marked as the foot node , and the other is expanded into a spine-etree whose head is the anchor of the whole tree . 
Word_Clustering $ 3 $ 59 $ 15 $ CRF $ Let w m ' ' be the sequence of all wi ' s ( wi E w rn ) such that its corresponding si is 1 , where ms denotes the number of l ' s in s ~ . 
Introduction $ 1 $ 20 $ 11 $ CRF $ This is a result of the need to formalise the huge number of research results that appear in free-text form in online collections of journal abstracts and papers such as MEDLINE for databases such as Swissprot ( Ban : och and Apwefler , 1997 ) and also to search such collections for facts in an intelligent way . 
Joining_TM2_and_DTD $ 3 $ 84 $ 5 $ PATTERN $ . Each record/TERM in the database consists of four/DEF fields : the segment string , a counter for the occurrences of that string in the corpus , the tag and the attributes ( type , id and corresp ) ./O 
Introduction $ 1 $ 9 $ 2 $ KP $ Indeed , logical approaches may have a relevant impact at the level of semantic interpretation , where a logical representation of the meaning of a sentence is important and useful ( Mooney , 1999 ) . 
_General_Outline_of_the_Method $ 2 $ 20 $ 4 $ CRF $ The first stage is the learning process in which several classifiers are built from the training data . 
OUT $ -1 $ 62 $ 9 $ PATTERN $ The/DEF problem of identifying the words string in a character sequence is/O known as the segmentation/TERM / tokenization/TERM problem/TERM . 
Manual_Tagging_Process $ 2 $ 40 $ 18 $ PATTERN $ Written Chinese consists of rurming texts without word delimiters ; the first step is is to segment the text into Chinese word sequences . 
Overview $ 1 $ 4 $ 0 $ KP $ Lexical acquisition from large corpora has long been considered as a means for enriching vocabularies ( Boguraev and Pustejovsky , 1996 ) . 
Models_and_Modifications $ 2 $ 56 $ 43 $ CRF $ Since multiple modifier trees can adjoin at the same location , Psa ( 7 ) is also conditioned on a flag f which indicates whether ' 7 is the first modifier tree ( i.e. , the one closest to the head ) to adjoin at that location . 
Abstract $ 0 $ 63 $ 30 $ CRF $ Most importantly in SFG the only irreducible definition of meaning , or structure , is a set of contrasts between events , or observations . 
Current_Research_and_Future $ 3 $ 123 $ 6 $ PATTERN $ Further , our research shows that completeness is a problem . 
Examples_of_YAG_in_use $ 3 $ 70 $ 5 $ PATTERN $ M5/TERM is the/DEF proposition that the name of the discourse entity B2 is " Pluto " ./O 
PAR_as_anIL $ 4 $ 22 $ 11 $ CRF $ ( 1 ) The bottle . floated out La boteUa sali6 flotando ( the bottle exited floating ) ( 2 ) I blew out the candle Apagud la vela sopldndola ( I extinguish the candle blowing ) 4 . 1 In order to capture generalizations about motion actions , we have a generalized PAR schema for motion , and our hierarchy includes different types of motion actions such as inherently directed motion and manner of motion actions that inherit from the more general schema , as shown in Figure 4 . 
OUT $ -1 $ 77 $ 26 $ KP $ Figure 1 : Aggregation examples of ( Mellish et al . , 1998a ) which uses a joint relation to connect every two text spans that do not have a semantic relation other than objectattribute elaboration and conjunct/disjunct in between . 
Introduction $ 1 $ 15 $ 8 $ PATTERN $ An important component of the view developed is the observation that most methods use the same simple knowledge representation . 
Structural_compatibility $ 3 $ 59 $ 16 $ PATTERN $ Since the set {B ,C} is missing from this list , there is a grouping in figure 1 that is not realized in figure 2b , so these representations are not isomorphic . 
The_Complexity_of_Extracting_a $ 2 $ 31 $ 19 $ PATTERN $ The level/TERM of/TERM a/TERM fact/TERM , F , in a piece of text is defined by the/DEF following algorithm : F . Suppose {xl ,x~ , . . . ,Xn} are the nodes relevant to F . Let s be the partial network consisting of the set of nodes {xl , x~ , . . . , x~} interconnected by the set of arcs {tl , t2 , . . . , tk} ./O 
Current_approach $ 3 $ 39 $ 0 $ KP $ Our algorithm also focuses on inflectional languages . 
Unless_necessary ,_we_don ' t_list_relationship_in_the $ 3 $ 86 $ 28 $ CRF $ Let , _o=Po , Zn-1 be the set of ( n-1 ) -leveI graphs , suppose Pn~ ( , . ouZiu . . . . , Za4 ) , ( PnnXn . 1 ) ¢NIL , En/TERM (/TERM C-Pn/TERM )/TERM is the/DEF set of the edges between points in P~ ,/O Rn/TERM (/TERM C/TERM (/TERM P=x/TERM En/TERM )/TERM )/TERM is the/DEF set of relations between points in P= and edges in En ,/O then : v ) vi ) vii ) viii ) < P~ , E~ , Rn/TERM is a/DEF n-level compositional graph ;/O n-level/TERM concepts/TERM comprise n-level/DEF compositional graphs , n-level point-headed graphs , and n-level edge-headed graphs ./O 
Block-based_Chinese_dependency_analysis $ 2 $ 75 $ 15 $ PATTERN $ We define 11 kinds of blocks as explained below . 
System_Description $ 2 $ 14 $ 3 $ PATTERN $ GoDiS/TERM consists/DEF of a number of modules , an information state , and a number of resources hooked up to the information state ./O
Collaborative_Agents $ 1 $ 12 $ 8 $ PATTERN $ The large window in the background is the shared application , in this case , the Lotus eSuite TM email program . 
Introduction $ 1 $ 25 $ 13 $ CRF $ Article choice is particularly important for this application : many AAC users drop articles and resort to a sort of telegraphese , but this causes degradation in comprehension of synthetic speech and contributes to its perception as unnatural and robot-like . 
Joining_TM2_and_DTD $ 3 $ 93 $ 14 $ KP $ The <rs>/TERM tag/TERM can be considered to be the/DEF name of the varying element ./O 
_The_session_was_also_attended_by_observers_from_the_following_international_organizations : $ 7 $ 114 $ 63 $ PATTERN $ Universities ( 28 . 1 ) , celebrities ( 53 . 0 ) and countries ( 36 . 5 ) are the most productive NEs in their categories while international agencies ( 4 . 0 ) , film directors ( 4 . 4 ) and states ( 8 . 7 ) are the less productive ones . 
_The_co-reference_problem_in_summarization $ 4 $ 46 $ 22 $ CRF $ There are two types of situations in which multidocument summarization would be useful : ( 1 ) the user is faced with a collection of dis-similar documents and wishes to assess the information landscape contained in the collection , or ( 2 ) there is a collection of topicallyrelated documents , extracted from a larger more diverse collection as the result of a query , or a topically-cohesive cluster . 
Learning_Frameworks $ 2 $ 31 $ 11 $ CRF $ A different , distribution free inductive principle that is related to the pac model of learning is the basis for the account developed here . 
Previous_Work : _Richer_Features $ 1 $ 16 $ 4 $ CRF $ ( 1989 ) describe a language model that builds a decision tree that is allowed to ask questions about the history up to twenty words back . 
Given_an_input_space_X~*_of $ 3 $ 78 $ 47 $ PATTERN $ For example , the Mutual/TERM Information/TERM measures/DEF the strength of a correlation between co-occurring arguments ,/O and the Plausibility/TERM ( Cucchiarelli , Luzi and Velardi ( 1998 ) ) assigns/DEF a weight to a feature vector , depending upon the degree of ambiguity of its arguments and the frequency of its observations in a corpus ./O 
Psycholinguistic_production $ 3 $ 40 $ 5 $ CRF $ Roelofs [ 1996] , for instance , argues that if a number of nodes representing semantic features are the basis for lexical access , in lemma retrieval it becomes extremely difficult to control the activation spread in such a way that only the most specific lexical unit that combines these features gets selected . 
Clustering_Systematic_Polysemy $ 3 $ 81 $ 1 $ KP $ Our assumption is that , if a semantic concept is systematically related to another concept , words that have one sense under one concept ( sub ) tree are likely to have another sense under the other concept ( sub ) tree . 
OUT $ -1 $ 31 $ 31 $ CRF $ This scanning task is one of many jobs an analyst performs to support report writing for customers in other Government agencies .  
ADVF_Far_adverbs_ ( ~ ' l $ 7 $ 102 $ 17 $ PATTERN $ Text For an Input : S = blockl , block2 , . . . ,block , , the dependency parsing will generate a set of 3-tuple in the form of {governor , dependant , dependency-relation} , which represents dependency relations between blocks in the given sentence . 
Introduction : $ 1 $ 10 $ 5 $ PATTERN $ Because supervised training typically demands significant human involvement ( e . g . , annotating the parse trees of sentences by hand ) , building a new corpus is a labor-intensive task . 
Related_Work $ 5 $ 138 $ 20 $ PATTERN $ Our model does not use such intermediate topics , but accesses word cg-occurrence information directly aald represents a context as the accumulation of this information . 
Evaluation $ 3 $ 120 $ 29 $ KP $ MLE thresholding produced better results than the two statistical tests used . 
Related_work $ 6 $ 175 $ 26 $ KP $ For example , the GOLEM algorithm ( Muggleton and Feng , 1990 ) used relative/TERM least/TERM general/TERM generalisation/TERM ( rlgg/ACR ) . 
Implementation $ 4 $ 329 $ 158 $ KP $ If an audio media object occurs in a frame , the duration of all media objects in that frame is equal to the length of all the audio files in the segment . 
Introduction $ 1 $ 43 $ 38 $ CRF $ Most people don't know he is a real person who is grown now . 
Automatic_Argumentative $ 3 $ 95 $ 21 $ CRF $ We use a manually created lexicon for patterns for agents , and a manually clustered verb lexicon for the verbs . 
Introduction $ 1 $ 60 $ 52 $ PATTERN $ The parent class is a hypernym of its children classes . 
_Background $ 2 $ 35 $ 19 $ PATTERN $ The structure of Mandarin ( base ) syllables is ( CG ) V ( X ) , where ( CG/TERM ) the/DEF syllable onset C/TERM the/DEF initial consonant ,/O G/TERM is the/DEF optional medial glide ,/O V/TERM is the/DEF nuclear vowel ,/O and X/TERM is the/DEF coda ( which may be a glide , alveolar nasal or velar nasal ) ./O 
_Introduction $ 1 $ 22 $ 15 $ CRF $ Potentially , we can have parallel corpora in a myriad of languages , yet the downside is the scarcity of linguistic knowledge resources and processing tools for less widely represented/studied languages . 
OUT $ -1 $ 202 $ 131 $ CRF $ The first is the database of dependency microcontexts extracted from a large text corpus . 
Subcategorization_Frequency $ 3 $ 56 $ 0 $ KP $ 3.1 Methodology : For the second experiment , we coded the examples of the 64 verbs from each of the three corpora for transitivity . 
Error-driven_Learning $ 4 $ 147 $ 30 $ CRF $ This paper proposes a new error-driven HMMbased chunk tagger with context-dependent lexicon . 
_Preliminary_Evaluation $ 3 $ 108 $ 53 $ PATTERN $ The senses with the highest confidence scores are the senses that contribute the most to the maximization function for the set . 
ADVF_Far_adverbs_ ( ~ ' l $ 7 $ 130 $ 45 $ PATTERN $ The block-based/TERM dependency/TERM parsing/TERM strategy/TERM is a/DEF novel integration of phrase structure partial approach and dependency parsing approach ./O 
Building_Spoken_Dialo~te_Systems $ 4 $ 119 $ 14 $ CRF $ In the example below , monthphrase is the phrase category name and the remaining part is the network of word categories . 
Out-of-Vocabulary_Words $ 4 $ 140 $ 32 $ CRF $ He presents a demonstration of the magnitude of the OOV problem for a wide range of multilingual natural language corpora and shows that some tasks can require vocabularies larger than 100,000 words to reduce the OOV rate below 1 % . 
Results $ 3 $ 54 $ 5 $ KP $ In the double-pass method finding the most likely tag for each word was split in finding chunk boundaries and assigning types to the chunks . 
Abstract $ 0 $ 2 $ 1 $ KP $ Multi-document summarization differs from single in that the issues of compression , speed , redundancy and passage selection are critical in the formation of useful summaries . 
_Tagging_NuLL-marker_CDM_pairs_ ( via $ 8 $ 104 $ 34 $ CRF $ If D~ satisfies a stopping criterion , the tree for Dr is a leaf associated with the most frequent class in D~ . 
Exploring_the_relationship_between $ 2 $ 42 $ 27 $ PATTERN $ For example , one rule describes dialogue contributions whose general function was what we call presentation , to advance the description of the house by introducing a single new object . . 
Conclusion_and_Future_Work $ 6 $ 211 $ 6 $ PATTERN $ Another factor that affects the onfission of information is the trade off between accuracy and conciseness . 
Abstract $ 0 $ 139 $ 15 $ CRF $ Rather , I intend to put forth the conjecture that syntax acquisition is extremely sensirive to the distribution of ambiguity , and , given this extreme sensitivity , suggest that simulation studies need to be conducted in conjunction with a broader analysis which abstracts away from whatever linguistic particulars are necessary to bring about the sentences required to build the input sample that feeds the simulated learner . 
Discussion $ 7 $ 170 $ 6 $ KP $ The verb rules defined here are less general then the basic verb groups ( Osolsob~ , 1999 ) . 
_KNOWLEDGE_EXTRACTION $ 3 $ 155 $ 97 $ PATTERN $ However the recall rate is not important , since the whole knowledge extraction process is a recurrent process . 
Introduction $ 1 $ 45 $ 36 $ KP $ A hypotactic/TERM construction/sentence/TERM : a/DEF sentence that has a main clause and a dependent clause , connected by a cue phrase ./O 
Introduction $ 1 $ 17 $ 4 $ KP $ A sentence planner is then used to select an appropriate syntactic structure .
Significance_of_word_contexts $ 2 $ 70 $ 22 $ PATTERN $ The investigation of word contexts is the most important , essential , unique and indispensable means of understanding the sense of words and texts . 
Focus_and_word_order $ 1 $ 11 $ 1 $ KP $ In many languages , intonation can reflect pragmatically motivated conceptual decisions . 
The_TransType_model $ 2 $ 79 $ 64 $ CRF $ It relies on the fact that a small number of words account for most of the tokens in a text . 
Clustering_into $ 5 $ 91 $ 22 $ CRF $ Figure 6 depicts a 3D representation of results obtained from profiling VBtags with six other major syntactic categories ; figure 7 shows the main syntactic behavioural features found for the co-occurrence of some of the major syntactic classes ranging over the chosen window of ten words . 
OUT $ -1 $ 82 $ 82 $ PATTERN $ Underpinning much work in summarization is the view that summaries are time savers . 
Experiments $ 5 $ 101 $ 7 $ PATTERN $ " Precision/TERM " is the/DEF percentage of correct answers among the answers proposed by the system ./O 
Dialogue_Management $ 2 $ 46 $ 2 $ CRF $ Because we consider the model of natural human reasoning as one of the important components in attaining naturalness of dialogue as a whole , we will discuss our model of reasoning in some detail . 
Generation_of_Crisp_Descriptions $ 3 $ 67 $ 13 $ PATTERN $ ) • 1Note that C contains r , unlike Dale and Reiter ' s ' contrast set ' C , which consists of those elements of the domain from which r is set apart . 
Abstract $ 0 $ 4 $ 3 $ CRF $ By coordinating thesaurus semantic/TERM categories/TERM (/TERM SEMCATs/ACR )/TERM of the long run words to the semantic categories of paragraphs , we conclude that for paragraphs containing both long runs and short runs , the SEMCAT weight of long runs of content words is a strong predictor of the semantic coherence of the paragraph . 
Abstract $ 0 $ 15 $ 14 $ KP $ The size of a topic continuity is empirically determined to be about 50 words with some variation . 
Overall_System_Architecture $ 3 $ 48 $ 0 $ PATTERN $ The compound/TERM noun/TERM indexing/TERM system/TERM proposed in this paper consists/DEF of two major modules : one for automatically extracting compound noun indexing rules ( in Figure 1 ) and the other for indexing documents , filtering the automatically generated compound nouns , and weighting the indexed compound nouns ( in Figure 2 ) ./O 
Corpus_comparison_based_on $ 6 $ 113 $ 4 $ PATTERN $ When a feature F is given , the conditional/TERM entropy/TERM for NE classes H ( CIF ) is defined by p/DEF ( ~ , f ) logs p ( cll ) H ( C]F ) cEC fEF ./O 
Sample_Selection $ 2 $ 44 $ 16 $ CRF $ L/TERM is a/DEF set of labeled training examples ./O 
Conclusion $ 5 $ 160 $ 5 $ KP $ The EMCL can be defined monolingually , multilinguality being obtained through NLG . 
Hyperonyms_in_NLG_systems $ 4 $ 60 $ 1 $ CRF $ In these systems , the hyperonym problem as one aspect of the general task of lexical choice arises only in systems that employ a sufficiently rich model of the lexicon and tile concept-lexicon link . 
Determining_a_User ' s_Line_of $ 3 $ 65 $ 8 $ CRF $ Hence , presenting them to the user for selection is a reasonable course of action . 
The_Complexity_of_Extracting_a $ 2 $ 55 $ 43 $ CRF $ The type of attack could appear as a level-0 fact as in `` the Medellin bombing ' ' ( assuming that the network is built at the phrase level ) because in this case both the attack designator ( bombing ) and the modifier ( Medellin ) occur in the same node . 
The_Verbmobil_treebanks $ 2 $ 51 $ 17 $ CRF $ 1 , one needs to search for trees containing a node nl with label PX and grammatical function 0A-MOD , a node n2 with label VF that dominates nl , a node n3 with label MF and a node n4 with label NX and grammatical function 0A that is immediately dominated by n3 . 
Abstract $ 0 $ 2 $ 0 $ KP $ Morphology/TERM induction/TERM is a/DEF subproblem of important tasks like automatic learning of machine-readable dictionaries and grammar induction ./O 
Unless_necessary ,_we_don ' t_list_relationship_in_the $ 3 $ 83 $ 25 $ CRF $ Suppose Pi~-Po , Ei/TERM (/TERM ~-Po/TERM )/TERM is the/DEF set of the edges between points in P1 ,/O Ri/TERM (/TERM ~/TERM (/TERM PlX/TERM El/TERM )/TERM )/TERM is the/DEF set of relations between points in PI and edges in Et s ,/O then : s Here , Edges are also points . 
Corpus_comparison_based_on $ 6 $ 150 $ 41 $ CRF $ These values are calculated for pairs of an NE class and features , and averaged for the n-fold experiments . 
Intelligent_Multimedia_Presentation $ 3 $ 70 $ 2 $ PATTERN $ The motivation for this is obvious : when a summarization filter ( which is a program under our control ) is generating a media object , we can often provide sufficient recta-information about that object to generate a short caption and some running text . 
Topic_Analysis $ 4 $ 85 $ 18 $ PATTERN $ 10 ) U5 ( 0 . 06 ) 15 In Hung long , where nauspaporo have alleged Japan has been nailing baler-cost semiconductors , some electronicsmanu~acturnrn share . . . 16 " That is a very short-term vies . 
Representing_cross-document $ 3 $ 91 $ 9 $ PATTERN $ Definition An extractive/TERM summary/TERM S/TERM of a cube C is a/DEF set of document units , S c C ,/O see Figure 3 ( d ) . 
Introduction $ 1 $ 5 $ 2 $ PATTERN $ The task of VERBMOBIL is the multi-lingual ( German , English , Japanese ) speaker-independent translation of spontaneous speech input that enables users to converse about the scheduling of a business appointment including travel , accommodation , and leisure time planning in a multi-lingual dialogue . 
Performance $ 4 $ 173 $ 10 $ PATTERN $ Time/TERM is the/DEF total time for the query in seconds ./O 
Stochastic_Surface_Realization $ 2 $ 63 $ 5 $ CRF $ We replace the gefieration grammar with a simple statistical language model to generate more complex utterances . 
Conclusions_and_Discussions $ 3 $ 61 $ 7 $ PATTERN $ Of course , new techniques to improved the accuracy of statistical model are the constant aim of our research . 
Word_Domain_Disambiguation $ 3 $ 44 $ 0 $ KP $ In this section we present two baseline algorithms for word/DEF domain disambiguation and/O we propose some variants of them to deal with WDD/TERM in the context of parallel texts . 
OUT $ -1 $ 13 $ 9 $ KP $ A writer expects that the checking tool should not only detect errors but also propose a CL conformable expression . 
Applications $ 3 $ 67 $ 7 $ PATTERN $ The same technique has more recently been applied to compare corpora analysed at the semantic level in a systems engineering domain and this is the main focus of this section . 
Acquisition_Process $ 4 $ 135 $ 36 $ PATTERN $ If this stem exists , we need to do find out whether or not the dictionary entry describes the same concept as contained in the ontology . 
Principles_and_Parameters $ 1 $ 28 $ 21 $ PATTERN $ The motto of such a learner is : Don't learn . from ambiguous input and learning efficiency can be measured by the number of sentences the learner has to wait for usable , unambiguous inputs to occur in the input stream . 
_Pronominalise_the_Cb_only_after_a_Continue $ 4 $ 182 $ 143 $ PATTERN $ The implementation of Centering/TERM reported here is a/DEF special case of text planning by constraint satisfaction , where the user has control over the different constraints ,/O and this approach means that different strategies for e . g . 
Indexing_and_Retrieval $ 5 $ 233 $ 0 $ KP $ The indexing/TERM process/TERM takes/DEF a group of document files and produces a new index ./O 
Introduction $ 1 $ 14 $ 8 $ PATTERN $ These uncertainty measures are useful in situations where both the classification of an sample and the system's confidence in that classification are needed . 
_Otherwise ,_add_to_the_current_context_new $ 6 $ 156 $ 73 $ PATTERN $ Still , the probability of node 1 is quite low ( i.e. , there is a high belief in its negation ) . 
OUT $ -1 $ 22 $ 19 $ PATTERN $ There is a rich variety of colour descriptions including basic colours , intervals , changes , etc . 
Introduction $ 1 $ 17 $ 6 $ PATTERN $ When analysing the content of a document , terms are the basic processed units -usually they are words of natural language . 
CommandTalk $ 2 $ 22 $ 0 $ KP $ CommandTalk/TERM is a/DEF spoken-language interface to the ModSAF ( Modular Semi-Automated Forces ) battlefield simulator ,/O developed with the goal of allowing military commanders to interact with simulated forces in a manner as similar as possible to the way they would command actual forces . 
Experimental_Results $ 4 $ 131 $ 12 $ PATTERN $ The alignment form D is the best . 
BOV_Stem_NE_Defaults_Qspecific_41 $ 5 $ 74 $ 44 $ PATTERN $ As already noted , 3 Question Type Default Eliminate Who title dateline What 1st story line ( none ) When dateline ( none ) Where dateline title Why 1st story line title , dateline Figure 3 : Default and eliminable sentences in the " Default " strategy " why " questions are the most difficult for bag-of-words . 
OUT $ -1 $ 11 $ 7 $ CRF $ These rules are given as recommendations or prohibitions for both the lexicon and the grammar . 
Impact_of_Missing_Translations $ 8 $ 127 $ 3 $ CRF $ To test the conjecture , for each English query term , a native speaker in Chinese or Spanish manually checked whether the bilingual lexicon contains a correct translation for the term in the context of the query . 
Error-driven_Learning $ 4 $ 120 $ 3 $ CRF $ the lexicon : F~ ( e i ) = F : rr°r ( e i ) o+Ao Here , F/TERM (/TERM el/TERM )/TERM is the/DEF chunking error number of the lexical entry e i for the old lexicon r~ Error / x and/O r~/TERM ,/TERM +~/TERM te/TERM i/TERM )/TERM is the/DEF chunking error number of the lexical entry e i for the new lexicon + AO where/O e~/TERM A~/TERM is the/DEF list of new lexical entries added to the old lexicon ~/O ) . 
Abstract $ 0 $ 21 $ 20 $ PATTERN $ If each corpus generates a separate set of probabilities , which probabilities are the correct ones to use as a model of human language processing?
The_query_language $ 3 $ 85 $ 28 $ PATTERN $ ( ILl , 79 , 73 , £ , # , rl , a ) is a query model with categories C , edge labels E and terminals Tiff/TERM 1/TERM is a/DEF finite set with Lt n ( C U E U T ) = O , the set of nodes ./O 
_Rare_w~_contains_a_hyphen $ 9 $ 88 $ 33 $ PATTERN $ For example , the number 244 in the ( NN , JJ ) position is the number of words that were NNs but were incorrectly assigned the JJ category . 
Interclausal_Coherence $ 4 $ 92 $ 2 $ CRF $ This problem is not restricted to prepositions : ( Knott and Sanders , 1998 ) actually build a multi-taxonomy of cuephrases in which elements may give rise to several relations , depending on context . 
CoreLex $ 2 $ 25 $ 1 $ KP $ With this definition , we can construct classes of systematically polysemous words as shown in the CoreLex approach ( Buitelaar 1998a ) ( Buitelaar 1998b ) . 
Models_and_Modifications $ 2 $ 79 $ 66 $ CRF $ *3 of the 400 sentences were not parsed due to timeouts and/or pruning problems . 
_Rare_w~_contains_a_hyphen $ 9 $ 160 $ 105 $ PATTERN $ The second feature template has the form : The last verb is v and the current word is w and w has been tagged as a particle and the current tag is t . The last verb is the pseudo-symbol NA if there is no verb in the previous three positions . 
Introduction $ 1 $ 14 $ 6 $ CRF $ In other words , they provide the/DEF conditional probability of a word given with the previous word sequence ,/O P/TERM (/TERM wilw~-l/TERM )/TERM , which shows the prediction of a word in a given context . 
Results $ 3 $ 95 $ 38 $ KP $ A reference corpus that is five times as large as the study corpus yields a larger number of keywords than a smaller reference corpus . 
Introduction $ 1 $ 62 $ 53 $ KP $ Finally , VALDIA is described in more detail and then the paper is closed by a discussion of relevant results and papers . 
Introduction $ 1 $ 30 $ 22 $ CRF $ Figure 2 : Annotation Scheme for Argumentative Zones Our hypothesis is that a segmentation based on regularities of scientific argumentation and on attribution of intellectual ownership is one of the most stable and generalizable dimensions which contribute to the structure of scientific texts . 
Learning_Algorithms_Tested $ 2 $ 57 $ 37 $ CRF $ LazyBoosting/TERM ( Escudero et al . , 2000a ) is a/DEF simple modification of the AdaBoost ./O MH/TERM algorithm/TERM , which consists/DEF in reducing the feature space that is explored when learning each weak classifier ./O 
Introduction $ 1 $ 16 $ 11 $ CRF $ Each participant is to return a ranked list of the five best answer strings for each question , where each answer/TERM string/TERM is a/DEF string of 50 bytes ( or 250 bytes ) that contains an answer to the question ./O 
Related_Work $ 5 $ 95 $ 5 $ KP $ The discourse structure determined accentuation , with deaccenting of discourse-old entities realized ( by lexically identical morphs ) in the current or previous discourse segment . 
Implementation $ 4 $ 308 $ 137 $ PATTERN $ The motivation for this is obvious : when a summarization filter ( which is a program under our control ) is generating a media object , we can often provide sufficient meta-information about that object to generate a short caption and some running text . 
Tree_Generalization_using_Tree-cut $ 2 $ 59 $ 13 $ CRF $ Therefore , in general , m when clusters C1 . . Cm are merged and generalized to C according to the thesaurus tree , the estimation of a probability model becomes less accurate . 
Conclusions $ 8 $ 253 $ 4 $ CRF $ It has the following novel features : 1 ) it represents topics by means of word dusters and employs a finite mixture model ( STM ) to represent a word distribution within a text ; 2 ) it constructs topics on the basis of corpus data before conducting topic analysis ; 3 ) it segments a text by detecting significant differences between STMs ; and 4 ) it identifies topics by estimating parameters 1°Here , k was set to 5 because the average length of a text was about 10 sentences . 
Abstract $ 0 $ 94 $ 1 $ PATTERN $ The second formalism that we introduce is the multi-document graph . 
Dialogue_Management $ 2 $ 51 $ 7 $ PATTERN $ The reasoning/TERM model/TERM consists/DEF of two functionally linked parts : 1 ) a model of human motivational sphere ; 2 ) reasoning schemes ./O 
Only_the_relevant_attributes_of_each_semantic_role $ 5 $ 131 $ 40 $ CRF $ In syntactic generation the interlingua representation is converted by ' transformational rules ' into an ordered surface-structure tree , with appropriate labeling of the leaves with target language grammatical functions and features . 
OUT $ -1 $ 156 $ 138 $ PATTERN $ But when the words " story " or " this " appear , the question seems to be referring to the story in its entirety and the dateline is the best answer . 
Abstract $ 0 $ 2 $ 1 $ KP $ The abstracts are generated by a process of conceptual identification , topic extraction and re-generation . 
Discussion $ 6 $ 268 $ 90 $ CRF $ However , many prior techniques/DEF used in natural language information retrieval (/O e . g . , head/modifier/TERM pairs/TERM ) can be expressed within the ItEXTOR framework , and furthermore the system provides a playground for experimenting with new techniques . 
Introduction $ 1 $ 7 $ 2 $ PATTERN $ A difference/TERM coefficient/TERM defined by Yule ( 1944 ) showed/DEF the relative frequency of a word in the two corpora ./O 
Experimental_Setup $ 5 $ 156 $ 10 $ PATTERN $ We asked 2 native Japanese who have an intermediate level understanding of Chinese language and who are the fluent users of the Internet search engines , to formulate 5 queries each in natural Japanese . 
Related_Work $ 5 $ 129 $ 11 $ PATTERN $ As for the second point , we doubt the appropriateness to use the word ' s distribution as a measure of combination of two models . 
PAR_as_anIL $ 4 $ 18 $ 7 $ PATTERN $ English and other Germanic languages are considered satellite-framed/TERM languages/TERM , expressing/DEF the path in the satellite ;/O Spanish/TERM , among other Romance languages , is a/DEF verb-framed language and expresses the path in the main verb ./O 
Introduction $ 1 $ 8 $ 2 $ KP $ The CoNLL-2000 shared task attempts to fill this gap . 
OUT $ -1 $ 110 $ 92 $ KP $ Any sentence that contains a proper noun whose head noun matches <x> will be highly rewarded . 
From_Prepositional_Phrases_to $ 3 $ 80 $ 23 $ PATTERN $ The degree expression SPACE/TERM , with its associated negative POL--MARKER , ( Staab and Hahn , 1997 ) is the/DEF trigger for recognizing the evaluative status of the matrix clause ./O 
Discussions $ 6 $ 79 $ 10 $ CRF $ We can see that the redundant ratio obviously decreases by using the revisional distance , and the result that has the lowest redundant ratio corresponds of the minimum value of the objective function . 
OUT $ -1 $ 65 $ 39 $ CRF $ An alignment algorithm using substitution , insertion and deletion of tokens as operations attempts to match the generated string with the reference string . 
Abstract $ 0 $ 61 $ 0 $ CRF $ scription as a set of constraints -each constraint/TERM is an/DEF atomic formula with free variables that specifies the requirement that some lexical meaning contributes to the description ;/O the variables/TERM are placeholders/DEF for the discourse entities that the description identifies ./O 
Abstract $ 0 $ 121 $ 17 $ KP $ Such a person name cannot be recognized in person name extraction because it does not begin with a surname or first character of transliterated person names . 
OUT $ -1 $ 165 $ 123 $ KP $ classifier is of its classification . 
Abstract $ 0 $ 90 $ 23 $ KP $ The sorted templates constitute the text plan . 
Highlight $ 3 $ 140 $ 89 $ CRF $ This contains a set of all the sorts and words found in the file . 
_System_Configuration $ 2 $ 44 $ 20 $ PATTERN $ Assigned to each individual term is a different weight so as to reshape a new search emphasis . 
ADAM : _Architectural_Principles $ 3 $ 80 $ 57 $ CRF $ We believe that this approach represents a further value of the ADAM Corpus . 
Proper_name_recognition $ 4 $ 115 $ 41 $ PATTERN $ Likewise the unknown word clinton is ( incorrectly ) interpreted as a common noun in ( 11 ) , as it is the last item of a noun phrase introduced by a determiner , but it becomes a proper name if another noun follows . 
Issues_and_proposals $ 3 $ 76 $ 33 $ CRF $ A better analysis would be to mark a question-answer relation between utterances 2 and 3 , and a motivation relation between utterance 1 and the unit consisting of utterances 2 and 3 . 
Introduction $ 1 $ 38 $ 31 $ KP $ using sortal constraints such as company and location , and restrictive constraints such as subject_of or same sentence . 
The_REXTOR_System $ 5 $ 170 $ 48 $ CRF $ Also , the sequence of adjectives is saved as the 0th bound variable , and the sequence of nouns is saved as the 1st bound variable . 
The_TABULATE_ILP_Method $ 3 $ 94 $ 28 $ CRF $ The size/TERM of/TERM a/TERM theory/TERM is the/DEF sum of the sizes of its clauses ./O 
Introduction $ 1 $ 47 $ 38 $ KP $ TM3 simply hosts the whole collection of aligned bilingual Types of documents in the corpus beyond our interest ( Trados Translator ' s Work . . . . . . 
Results $ 4 $ 70 $ 1 $ KP $ The word list captions scored 74.6% on our crossing measure , while the sentence captions scored 89.5% . 
ADAM : _Architectural_Principles $ 3 $ 88 $ 65 $ PATTERN $ ATLAS/TERM offers/DEF a threelayers solution to the problem of integrating different data storage formats by providing a logical level which consists of the language formalism and the API ./O 
Conclusion $ 4 $ 56 $ 2 $ CRF $ We are at present checking that the behavior also holds for other quality measures as the precision and recall of parses of sentences that express strong equivalence between the model and the data . 
Abstract $ 0 $ 3 $ 1 $ KP $ Reading/TERM comprehension/TERM tests/TERM are specifically/DEF designed to evaluate human reading skills ,/O and these require vast amounts of world knowledge and common-sense reasoning capabilities . 
Results $ 3 $ 46 $ 7 $ KP $ In the gisting exercise , each user was asked to rate decision points in a translation on a 1-5 scale . 
Architecture_of_WIT-Based_Spoken $ 3 $ 68 $ 34 $ PATTERN $ The domain-dependent/TERM knowledge/TERM used in this module consists of a/DEF unification-based lexicon and phrase structure rules ./O 
Abstract $ 0 $ 3 $ 2 $ CRF $ However , an important area of research that has not been given the attention it deserves is a formal analysis of the parameters affecting the performance of the learning task faced by these systems . 
Memory_based_Learning $ 4 $ 60 $ 11 $ PATTERN $ Given one of the N most probable chunk sequences extracted by the error-driven HMMbased chunk tagger , we can extract a set of chunk/TERM patterns/TERM , each of them with/DEF the format : XP 1 n n+l r~+l = poroPlrn Pn+l , where is the structural relation between Pi and Pi+l ./O 
Unfolding_and_Specialization $ 2 $ 38 $ 11 $ PATTERN $ The set of all such best specializations defines the set of candidate successor grammars . 
Examples $ 2 $ 26 $ 4 $ KP $ U2 : screen saver . 
_Semantic_classes_of_the_head_of_the_NP : _If $ 8 $ 81 $ 15 $ PATTERN $ The first , IB1/TERM is a/DEF k-nearest neighbour algorithm ./O 
Results $ 3 $ 69 $ 12 $ CRF $ Obviously , a total of 13,950 keywords could never have been obtained since die maximum possible number of 10 keywords in the letters corpus is 2,415 , which is the total number of types . 
Reading_Comprehension_Tests $ 2 $ 47 $ 29 $ CRF $ A mummy/TERM is a/DEF body wrapped in sheets ./O 
Future_Work $ 7 $ 178 $ 0 $ PATTERN $ The biggest remaining step is a more careful evaluation of different sub-systems and preference strategies to more efficiently process very ambiguous and complex inputs , without substantially sacrificing translation quality . 
Issues_and_proposals $ 3 $ 159 $ 116 $ PATTERN $ In our annotation scheme the presentational relations are split from the subject-matter relations and annotators are instructed to consider for each set of spans whether there is a subject-matter relation , and also whether there is a presentational relation . 
Memory-Based_Language $ 1 $ 14 $ 8 $ CRF $ IGTREE/TERM is a/DEF variant in which an oblivious decision tree is created with features as tests , and in which tests are ordered according to information gain of the associated features ./O 
Introduction $ 1 $ 21 $ 7 $ CRF $ The learning system is equipped with a UG/TERM and associated parameters , encoded/DEF as a Unification-Based Generalised Categorial Grammar ,/O and a learning/TERM algorithm/TERM that fixes/DEF the values of the parameters to a particular language ./O 
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 87 $ 52 $ CRF $ Domain/TERM dependency/TERM of/TERM words/TERM is a/DEF measure showing how greatly each word features a given set of data ./O 
Hyperonymy_in_lexical_semantics $ 5 $ 121 $ 24 $ CRF $ There is a general rule that requires speakers to use this term in order to obtain an unmarked utterance in a given context : : . unless . this would result in an ' abnormal communication ' , in which case the speaker should deviate from neutral level , but only to the minimum degree required to ensure normality . 
_Pronominalise_the_Cb_only_after_a_Continue $ 4 $ 65 $ 26 $ PATTERN $ We noted above that there is evidence that a RETAIN-SHIFT sequence is the preferred way of introducing a new transition following CONTINUE , another CONTINUE would be cheap as well . 
Introduction $ 1 $ 12 $ 5 $ KP $ Other KA techniques used in the past for building NLG systems include letting domain experts specify content rules in pseudo-code ( Goldberg et al . , 1994 ) and ethnographic techniques such as observing doctors and patients in real consultations ( Forsythe , 1995 ) . 
Evaluation $ 7 $ 199 $ 15 $ CRF $ This is the major reason of the failure as shown in Figure 4 .  
OUT $ -1 $ 101 $ 50 $ KP $ A good/TERM embedding/TERM is one/DEF satisfying all the following conditions : strative or a bridging description (/O as defined in ( Poesio et al . , 1997 ) ) . 
EXOT $ 9 $ 94 $ 22 $ PATTERN $ Therefore any frequency based measures defined by Boyd et al . 
Introduction $ 1 $ 38 $ 34 $ CRF $ Fiof prospective arguments on two models : ( 1 ) a normally , we illustrate the operation of our mechanism/TERM mative/TERM model/TERM , which represents/DEF NAG ' s beliefs ,/O and with an example , discuss results from our preliminary ( 2 ) a user/TERM model/TERM , which represents/DEF a user ' s presumed beliefs ./O 
OUT $ -1 $ 147 $ 96 $ KP $ ( Mellish et al . , 1998a ) summarises the genetic/TERM algorithm/TERM roughly as follows : quences/DEF by loosely following sequences of facts where consecutive facts mention the same entity ./O 
Construction_of_Features $ 3 $ 75 $ 7 $ CRF $ Though the analysis complexity can be reduced by segmenting a sentence , there is a mis-segmentation risk that causes parsing failures . 
Results $ 3 $ 81 $ 29 $ PATTERN $ ' */TERM ' denotes significantly/DEF better accuracy of RBM or RIPPER over IBi-IG with p 0.05 ./O 
_Otherwise ,_add_to_the_current_context_new $ 6 $ 95 $ 12 $ CRF $ Inference to the best explanation The assertion of the goal G supports a proposition Q which is firmly believed ( i.e. , P ( Q ) = High , where Q/TERM is a/DEF premise or inferred from premises )/O , but which would be unexplained ( improbable ) without supposing the truth of the goal . 
Accommodation_in_GoDiS $ 3 $ 76 $ 25 $ PATTERN $ Furthermore , as this is the first indication of what the customer wants , the travel agent cannot have a plan with detailed questions . 
Abstract $ 0 $ 95 $ 2 $ PATTERN $ Each graph consists of smaller subgraphs for each individual document ( Figure 4 ) . 
_Modeling $ 4 $ 111 $ 1 $ CRF $ We have instead provided a prose description of the process for a very few examples and many questions of just what constitutes a displacement or how one might know that a relation reached in the traversal Should be unpacked remain unanswered . 
OUT $ -1 $ 146 $ 75 $ KP $ [large , ( A , Adv , D ) , very] is an example of an SDR . 
Sample_Dialogues $ 3 $ 112 $ 5 $ KP $ ( ASR output : I would like to the zoo historic sites in stanhope historic ) $2 : Did you say you are interested in going to a zoo?
Abstract $ 0 $ 33 $ 8 $ KP $ In section 3 Eckert and Strube ' s algorithm is introduced and in 4 the Danish personal and demonstrative prononn~ are described with focus on discourse deictics in dialogues . 
Learning_validation_and_results $ 4 $ 148 $ 10 $ PATTERN $ The theoretical/TERM generality/TERM of a generalized clause is the/DEF number of not generalized clauses ( E + ) that this clause can cover ./O 
The_MATE_Markup_Framework $ 3 $ 88 $ 0 $ KP $ The MATE/TERM markup/TERM framework/TERM is a/DEF conceptual model which basically prescribes ( i ) how files are structured , for instance to enable multi-level annotation , ( ii ) how tag sets arc ; represented in terms of elements and attributes , and ( iii ) how to provide essential information on markup , semantics , coding purpose etc ./O 
Introduction $ 1 $ 8 $ 1 $ PATTERN $ There are many definitions for the compound noun which cause ambiguities as to whether a given continuous noun sequence is a compound noun or not . 
Proposed_Architecture $ 1 $ 50 $ 28 $ PATTERN $ Implemented in a combination of C++ , Java and Lisp , the new version represents a service-oriented architecture . 
Enriching_the_Feature_Set_with $ 2 $ 89 $ 19 $ PATTERN $ c/TERM (/TERM x~/TERM ,wz/TERM )/TERM represents the/DEF count of the event that x and y occur adjacent and in this order in the training corpus ./O 
_The_Problem $ 1 $ 54 $ 48 $ PATTERN $ Consider the partially saturated relation below that is the denotation of the relative clause of la at the point when the downstairs S has been parsed ( " which the three companies will equally shoulder " ) . 
Methodology $ 2 $ 29 $ 1 $ CRF $ In order to set the terms of the problem , we find it useful to partition the set of synonymy relations defined in WordNet into three classes : . 
MALIN $ 4 $ 253 $ 147 $ KP $ The Domain/TERM Knowledge/TERM Manager/TERM is functional/DEF utilising a Spatial Reasoner for one sub-area of OstergStland and a Temporal Reasoner ./O 
OUT $ -1 $ 181 $ 176 $ KP $ For each question type , we uniformly use the same , identical set of features . 
Theoretical_Ideas , $ 2 $ 95 $ 70 $ PATTERN $ For instance , if the command is " go to the crew hatch and open it " and the crew hatch is already open , the interface has the option of informing the user that there is a problem without-first carrying out the " go to " action . 
Only_the_relevant_attributes_of_each_semantic_role $ 5 $ 100 $ 9 $ CRF $ Finally , the semantic role MODIFIER has the following attributes : Cat that contains the syntactic category of the constituent ; Identifier with the value of the discourse marker ; Prep with the preposition of the constituent and ENTITY , which is the object of the PP and contains the same attributes as the THEME . 
Related_Work $ 5 $ 135 $ 17 $ CRF $ Reductions in perplexity relative to a bigrmn model were 10.5 % for the entire text and 33.5 % for the target vocabulary . 
Abstract $ 0 $ 4 $ 2 $ KP $ We find that GA feature selection always significantly outperforms the MBLP variant without selection and that feature ordering and weighting with CA significantly outperforms a situation where no weighting is used . 
OUT $ -1 $ 108 $ 54 $ KP $ The compound noun maple syrup ( i.e. 
Introduction $ 1 $ 48 $ 35 $ PATTERN $ In Figure 3 , it was not necessary for the application to specify that the conjunction of two noun phrases is a parallel noun phrase , nor that component/TERM noun/TERM phrases/TERM ( proper/DEF nouns , pronouns , and possessives )/O should not , contain an article . 
Informational_content_of_sentences $ 2 $ 24 $ 0 $ CRF $ Cluster-based/TERM sentence/TERM utility/TERM ( CBSU/TERM , or utility/TERM ) refers to the/DEF degree of relevance ( from 0 to 10 ) of a `` particular sentence to the general topic of the entire cluster (/O for a discussion of what is a topic , see [ Allan et al . 
Example_Dialogue $ 3 $ 35 $ 12 $ PATTERN $ None of the above ( U5 ) U : list ( U6 ) S : By " from bob " do you mean : 1 . source folder 2 . sender ( UT ) U : sender ( US ) S " 2christmas " is a way to express : -1 . yesterday ( date relative yesterday ) 2 . tomorrow ( date relative tomorrow ) 3 . today ( date relative today ) 0 . 
Setting $ 3 $ 111 $ 32 $ PATTERN $ The symbol " +/TERM " stands for set/DEF union ,/O therefore A+B-B means that the training set is A union B and the test set is B . 
Introduction $ 1 $ 13 $ 10 $ CRF $ Each story has the form of a newspaper article , including a title and dateline .
Determination_Schemes_of $ 4 $ 119 $ 4 $ PATTERN $ Coverage/TERM is the/DEF ratio of the number of actually segmented sentences to the number of segmentation target sentences that are longer than ot words ,/O where o~/TERM is a/DEF fixed constant distinguishing long sentences from short ones ./O 
The_CGS_system $ 4 $ 68 $ 0 $ KP $ The Caption/TERM Generation/TERM System/TERM ( CGS/TERM ) generates/DEF explanatory captions of graphical presentations ( 2D charts and graphs ) ./O 
Abstract $ 0 $ 164 $ 17 $ PATTERN $ Relating to the highest unit providing antecedents works only when there is a mini segment boundary every time an anaphoric expression is used . 
Introduction $ 1 $ 35 $ 22 $ CRF $ 2 An Overview of YAG/TERM YAG/TERM ( Yet/TERM Another/TERM Generator/TERM ) ( Channarukul , 1999 ; McRoy et al . , 1999 ) is a/DEF template-based textrealization system that generates text in real-time ./O 
_Annotation_Guidelines $ 4 $ 88 $ 0 $ KP $ In general , Chinese/TERM phrases/TERM can roughly be classified/DEF into five categories , i.e. , subpredicate , verb-object , modifier-center , verbcomplement , and coordinate ./O 
Approach $ 2 $ 39 $ 12 $ KP $ Each leaf node has an associated/TERM goal/TERM , which , when realized , provides/DEF content for that node ./O 
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 177 $ 142 $ PATTERN $ Let $1 : -' , S , , be all the other training documents ( where m/TERM is the/DEF number of training documents which does not belong to the target event )/O and Sx/TERM be a/DEF test document which should be classified as to whether or not it discusses the target event ./O 
Statistical_Semantic_Parsing $ 4 $ 113 $ 14 $ PATTERN $ S+/TERM and S/TERM are the/DEF sets of good and bad states respectively ./O 
Statistical_Semantic_Parsing $ 4 $ 119 $ 20 $ PATTERN $ The equation states that the probability that a given query is a correct meaning for I is the same as the probability that the final state ( reached by parsing l ) is a good state . 
Introduction $ 1 $ 39 $ 33 $ KP $ The Bayesian Times reporting Mr Body took Mr Green ' s girlfriend implies Mr Green and Mr Body possibly were enemies , which implies Mr Green possibly had a motive to murder Mr Body . 
HMM-based_Chunk_Tagger_with $ 2 $ 20 $ 11 $ PATTERN $ The second item is the summation of log probabilities of all the tags . 
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 162 $ 127 $ PATTERN $ ' Accuracy ' in Table 1 is the total average ratio . 
Construction_of_Features $ 3 $ 87 $ 19 $ PATTERN $ 166 words in a sentence , and posi_v/TERM represents the/DEF region in which a word lies ./O 
_Annotation_Guidelines_I : $ 3 $ 97 $ 51 $ PATTERN $ both Nd and Ne are elaboration of N ) , then the basic category is the default inheritance category for the mother . 
Experiment $ 3 $ 70 $ 2 $ KP $ The verbs were chosen at random , subject to the constraint that they occurred frequently enough in corpus data 4 and when applicable , represented different sub-classes of each examined Levin class . 
Tagged_Text $ 3 $ 115 $ 101 $ PATTERN $ • PP/TERM rules/TERM for/TERM word-sense/TERM disambiguation/TERM : For/DEF some nouns ( propernouns ) which are the object of a preposition , the intersection of the semtype value sets of the preposition word and its object determines their semtype ./O 
Selection_of_candidate_strings $ 2 $ 38 $ 14 $ PATTERN $ The existence of a single-character string is the necessary but not sufficient condition for a new word . 
Learning_Algorithms_Tested $ 2 $ 50 $ 9 $ PATTERN $ For k ' s greater than 1 , the resulting sense is the weighted majority sense of the k nearest neighbours --where each example votes its sense with a strength proportional to its closeness to the test example . 
The_MATE_Approach $ 2 $ 52 $ 16 $ CRF $ This list of information items which we call a coding module , is the core concept of the MATE markup framework and extends and formalises the concept of a coding scheme . 
Abstract $ 0 $ 3 $ 2 $ KP $ We have tested Quarc on reading comprehension tests typically given to children in grades 3-6 . 
Aspect_in_Lexical_Conceptual $ 3 $ 101 $ 1 $ PATTERN $ The LCS/TERM framework/TERM consists/DEF of primitives ( GO , BE , STAY , etc . ) ./O 
Building_Spoken_Dialo~te_Systems $ 4 $ 118 $ 13 $ CRF $ Each definition/TERM is a/DEF pair comprising a phrase category name and a network of word categories ./O 
Maximum_Entropy_models $ 2 $ 41 $ 37 $ PATTERN $ Trying new feature combinations , by adding them manually and testing the new configuration is a time consuming and not very interesting activity . 
Current_approach $ 3 $ 103 $ 64 $ PATTERN $ " Yet , it is true that " -es " is a valid suffix for the words " flashes , " " catches , " " kisses , " and many other words where the " -es " is preceded by a voiceless sibilant . 
Information_Structures $ 3 $ 109 $ 32 $ PATTERN $ R1 between C1 and C2 should be interpreted as " C1 is the RI of C2 " . 
Introduction $ 1 $ 32 $ 25 $ PATTERN $ Plain text documents are provided to a/DEF lexical analyzer and a noun-recognizer (/O XEROX/TERM MULTEXT/TERM ) , whose output is the document text tagged with parts of speech to be fed to the parser . 
_Proposed_method $ 2 $ 43 $ 13 $ KP $ WordNet/TERM is a/DEF lexical ontology a variant on semantic networks with more of a hierarchical structure ,/O even though some of the nodes can have multiple parents that was manually constructed for the English language . 
The_Structure_of_a_Relational $ 3 $ 77 $ 3 $ PATTERN $ Each record ( row ) in the entity file defines a unique entity . 
Abstract $ 0 $ 147 $ 4 $ CRF $ But if computational modeling is going to eventually lay claim to a model which accurately mirrors the human process of language acquisition , years of fine grinding are necessary . 
Introduction $ 1 $ 9 $ 3 $ KP $ In English BNP/ACR ( base/TERM noun/TERM phrase/TERM ) is defined as simple/DEF and non-nesting noun phrases ,/O i.e. 
The_Generation_System $ 3 $ 107 $ 65 $ CRF $ The result of the linearization/TERM phase/TERM is a/DEF word lattice specifying the sequence of words that make up the resulting sentence and the points of ambiguity where different generation paths are taken ./O 
HMM-based_Chunk_Tagger $ 1 $ 40 $ 7 $ PATTERN $ ) The second item in the above equation is the mutual information between the tag sequence Tin and the given token sequence G~ . 
Abstract $ 0 $ 163 $ 16 $ KP $ This algorithm is only a useful approximation towards a complete account of a text ' s discourse structure . 
Modeling_various_degrees_of $ 5 $ 82 $ 19 $ PATTERN $ The ACTIONS/TERM field/TERM is a/DEF stack of ( domain ) actions which the user has been instructed to perform but has not yet performed ./O The LU/TERM field/TERM contains/DEF information about the latest utterance ./O 
Bridging_Natural_Language_and $ 4 $ 119 $ 40 $ PATTERN $ For example , indexing adjacent word pairs consists of indexing adjacent words with the adjacent relation . 
Conclusion $ 5 $ 194 $ 10 $ CRF $ The last phase of the project will deal with the real use of the N-V pairs obtained by the machine learning method within one information retrieval system and the evaluation of the improvement of its performances .
Perfect_Sampling $ 3 $ 44 $ 0 $ KP $ In this paper , we propose the/DEF application of another sampling technique in the parameter estimation process of the WSME model which/O was introduced by Propp and Wilson ( Propp and Wilson , 1996 ) : the Perfect/TERM Sampling/TERM ( PS/ACR ) . 
OUT $ -1 $ 0 $ 0 $ KP $ NJFun/TERM : A/DEF Reinforcement Learning Spoken Dialogue System Diane/O Litman , Satinder Singh , Michael Kearns and Marilyn Walker AT&T Labs -Research 180 Park Avenue Florham Park , NJ 07932 USA {diane ,bavej a ,mkearns ,walker} @research . att . com
Abstract $ 0 $ 5 $ 3 $ CRF $ A preliminary evaluation of the proposed method yielded results of up to 79 % accuracy rate for the English data on 81 . 8 % of the SemCor manually tagged data . 
Topic_Analysis $ 4 $ 99 $ 32 $ PATTERN $ The Shannon/TERM information/TERM of/TERM word/TERM w/TERM in text t is defined as I/DEF ( w ) = -N ( w ) logP ( w ) ,/O where N/TERM (/TERM w/TERM )/TERM denotes the/DEF frequency of w in t ,/O and P/TERM (/TERM w/TERM )/TERM the/DEF probability of the occurrence of w as estimated from corpus data ./O 
Reference_Resolution $ 4 $ 99 $ 0 $ PATTERN $ What follows here is a discussion of what is needed to resolve the 273 references made in the example Spanish text . 
Building_a_reusable_lexical_chooser $ 2 $ 18 $ 1 $ KP $ There are two main reasons why such a lexical chooser has not been developed in the past : 1 . 
The_MATE_Markup_Framework $ 3 $ 177 $ 89 $ CRF $ Example : The declaration Occursln : href ( lxanscription , u ) allows an attribute used as , e . g . , Occursln= ' ' base~_123 ' ' , where base is a coding file using the transcription module and u_123 is the value of the id attribute of a t~ element in that file . 
POS_Assignment $ 3 $ 85 $ 25 $ PATTERN $ For example , N ( vl2 ( c ) ) Pv12 ( c ) = N ( c ) where N/TERM (/TERM v12/TERM (/TERM c/TERM )/TERM )/TERM is the/DEF number of occurrences of a character in the first position of a two-character verb while/O N/TERM (/TERM c/TERM )/TERM is the/DEF total number of occurrences of this character in the dictionary headwords ./O 
October_2000 $ 7 $ 11 $ 9 $ KP $ The SIGLEX ' 00 Program Committee enabled us to work within a very brief time frame , by quickly turning around reviews for the substantial number of submissions to the conference . 
_The_Problem $ 1 $ 64 $ 58 $ PATTERN $ We have the option to view ( i ) as a composite object with a first class object representing each of its variable bindings in its own right , as in ( iii ) which is the unreduced binding of the amount of money to the amount variable of the object we named Cap-1 in ( i ) . 
Abstract $ 0 $ 79 $ 48 $ CRF $ From the perspective of discourse analysis , the study of discourse markers basically involves four distinct but fundamental issues : 1 ) the occurrence and the frequency of occurrence of discourse markers ( Moser and Moore 1995 ) , 2 ) determining whether a candidate linguistic item is a discourse marker ( identification / disambiguation ) ( Hirschberg and Litman 1993 ; Siegel and McKeown 1994 ) , 3 ) determination or selection of the discourse function of an identified discourse marker ( Moser and Moore 1995 ) , and 4 ) the coverage capabilities ( in terms of levels of embedding ) among rhetorical relations , as well as among individual discourse markers . 
Conclusion $ 7 $ 200 $ 11 $ KP $ by referential means . 
Results $ 3 $ 86 $ 34 $ KP $ Table 1 ) , RBM ' s classification is very speedy . 
User_Interface $ 5 $ 104 $ 13 $ CRF $ As can be seen from Figure 5 , there is a check box along with each retrieved record . 
Centroid-based_summarization $ 4 $ 78 $ 6 $ PATTERN $ 101 The output consists of a sequence of In * r] sentences from the original documents in the same order as the input documents . 
Implementing_Embedded_MT $ 2 $ 112 $ 55 $ CRF $ Another part is the analysis of not-translated words . 
_Pronominalise_the_Cb_only_after_a_Continue $ 4 $ 83 $ 44 $ CRF $ That is , the Text/TERM Planner/TERM would plan/DEF the content of Un+l by aiming to realise a proposition in the knowledge base which mentions an entity which is salient in Un ./O 
Abstract $ 0 $ 206 $ 9 $ KP $ The nouns that refer to concrete objects and verbal actions are similar to adjectives when they represent a level in context . 
Results $ 3 $ 114 $ 75 $ KP $ The average recall over all users and all • texts is 66 . 7% for Y and 75% for N . These averages create for the Y and N chart the respective cutoff boundaries for " YES " ( text output is acceptable for filtering ) and " NO " ( it is not ) . 
OUT $ -1 $ 78 $ 32 $ KP $ Only 26 OCR-ed " words " are found in the NIT lexicon , i.e. , recognized as valid Spanish words . 
Tagged_Text $ 3 $ 23 $ 9 $ CRF $ The Partial/TERM Parser/TERM Module/TERM then takes/DEF this updated text and breaks it into phrases while attempting to lexically disambiguate the text ./O 
Only_the_relevant_attributes_of_each_semantic_role $ 5 $ 179 $ 88 $ PATTERN $ So the AGENT of this CLAUSE is the PRONOUN she and it has a link to the ENTITY Ann ( the chosen antecedent ) .  
Classifiers $ 2 $ 174 $ 10 $ PATTERN $ ( x _/~ , ) log l , I +2 log p ( w c ) g , ( x ) = ( x-lee ) r Z ,-~ ( x-/~ , ) log] Z~ ] +2log p ( w , ) Pc/TERM and ,ue/TERM are the/DEF mean vectors of the class wc and we ,/O respectively , C/TERM (/TERM Wc/TERM ,/TERM We/TERM )/TERM are the/DEF covariance matrices of the class wc and we ,/O respectively , and 1-I is the determinant . 
Dialogue_examples $ 4 $ 240 $ 33 $ PATTERN $ The example represents a dialogue where the computer is implementing the tactic of threatening . 
_Methods $ 2 $ 56 $ 9 $ CRF $ The pre-recorded dialogues were copied from CD and digitised for analysis at 22 kHz using Entropic ' s ESPS/ Waves + speech analysis software running on a Sun workstation in the Phonetics Laboratory of the UniversiW of Melbourne . 
Learning_Frameworks $ 2 $ 23 $ 3 $ PATTERN $ Pr ( s ) = Pr ( wl , W2 , . . . Wn ) ---= H~=lPr ( wilwl , . . . wi-1 ) = H~=lPr ( wilhi ) where hi/TERM is the/DEF relevant history when predicting wi ,/O and s/TERM is any/DEF sequence of tokens , words , part-of-speech ( pos ) tags or other terms ./O 
Comparing_Three_Treebank $ 4 $ 67 $ 0 $ KP $ Grammars In this section , we describe our methodology for comparing Treebank grammars and the experimental results . 
Architecture_components $ 2 $ 28 $ 19 $ PATTERN $ The second stage consists of a combination of the outputs of the five base chunkers , using another WPDV model . 
Introduction $ 1 $ 13 $ 6 $ PATTERN $ The prosodic/TERM information/TERM consists/DEF of ToBI labeling of accents and breaks (/O Silverman et al . , 1992 ) . 
Generation_of_Multiple_Quantifiers $ 5 $ 185 $ 0 $ CRF $ When there are two distinct roles across the propositions , the algorithm tries to use a universal quantifier for one role and an existential quantifier for another . 
Conclusion $ 9 $ 184 $ 2 $ PATTERN $ We believe there is a stronger role that NL generation can play in the narrative aspects of our briefings , which currently rely for the most part on canned text . 
Global_View_on_the_DE $ 2 $ 20 $ 3 $ PATTERN $ The generation/TERM process/TERM consists/DEF of a series of structure mappings between adjacent strata until the SMorph stratum is reached ./O 
OUT $ -1 $ 173 $ 168 $ PATTERN $ The test/TERM set/TERM consists/DEF of 30 stories from grade 3 and 30 stories from grade 4 ./O 
Introduction $ 1 $ 28 $ 20 $ CRF $ 2 An Overview of HowNet HowNet/TERM is a/DEF bilingual general knowledge-base describing relations between concepts and relations between the attributes of concepts ./O 
Introduction $ 1 $ 12 $ 8 $ KP $ The PCFG/TERM obtained in this way consists/DEF of rules that include information about the context where the rule is applied ./O 
ALLiS $ 2 $ 24 $ 19 $ KP $ Using XML properties , the grammar has easily access to all the levels/TERM of/TERM the/TERM document/TERM ( word/DEF , tag , phrase , and higher structures )/O . 
Abstract $ 0 $ 7 $ 6 $ KP $ Our conclusions show the importance of the application area in the design of NLP tools . 
Chunk_Types $ 3 $ 30 $ 13 $ KP $ Our NP chunks are very similar to the ones of Ramshaw and Marcus ( 1995 ) . 
OUT $ -1 $ 0 $ 0 $ KP $ Using Summarization for Automatic Briefing Generation Inderjeet Mani . 
Related_Work $ 5 $ 159 $ 1 $ CRF $ For example , revision/TERM ( Robin , 1994 ) is a/DEF technique for building semantic inputs incrementally ./O 
Previous_Schemes_for_Grunt $ 2 $ 49 $ 21 $ PATTERN $ Second , there is the assumption that the set of conversational grunts is small . 
Generating_naive_rules $ 2 $ 34 $ 1 $ KP $ The details of integrating induction into chart parsing have been described in ( Cussens and Pulman , 2000 ) , here we give just a brief account . 
_Topic_Deviation $ 3 $ 158 $ 12 $ PATTERN $ 5 ) Informative , neutral and destructive phrasal terms are defined by means of MI ( oce ,rel ) . 
Advantages_of_automatic $ 3 $ 127 $ 25 $ KP $ NLG can be used for checking a CL , which is helpful even if the CL is intended for a human writer because it may avoid the discovery of various cases of incoherence by the writer . 
Abstract $ 0 $ 12 $ 8 $ KP $ Language/TERM models/TERM are important/DEF post-processing modules to improve recognition accuracy of a wide variety of input ,/O namely speech recognition ( Balh et al . , 1983 ) , handwritten recognition ( Elliman and Lancaster , 1990 ) and printed character recognition ( Sun , 1991 ) , for many human languages . 
Feature_merging_and_overfitting $ 3 $ 71 $ 14 $ PATTERN $ The merged elements in this case are the lexical items offered and allow . 
_INTRODUCTION $ 1 $ 33 $ 27 $ PATTERN $ However an organization ' s full names usually occur at its first mention , unless it is a well-known organization . 
Evaluation $ 5 $ 139 $ 6 $ KP $ This is understandable , in the sense that the larger the document , the smaller proportion of fixed sections it will contain . 
Related_Work $ 6 $ 173 $ 13 $ CRF $ In this paper , we have summarized the evidence for this view of human conversation , and shown how it informs the generation of communicative action in our artificial/DEF embodied conversational agent ,/O REA/TERM . 
SYSTEM : _U_S-air_flight_63_57_departs_la $ 9 $ 71 $ 64 $ PATTERN $ Another example is the following : ( i ) there don ' t seem to be any nonstop flights from san francisco to newark new jersey on united which serve breakfast and depart after nine A M on february tenth . 
Evaluation $ 3 $ 200 $ 11 $ KP $ Also , the skip ratio is 65% , which is much higher than the skip ratio of 0 . 1% if we did not use the classifier . 
_Otherwise ,_add_to_the_current_context_new $ 6 $ 111 $ 28 $ PATTERN $ However , in the current implementation , each Qi consists of one proposition only . 
_Relations_that_are_relevant_and_correct_in $ 3 $ 79 $ 45 $ PATTERN $ The final score assigned to each polysemous term tl is the highest scorePolyCQi ,s . 
How_to_generate_technical $ 2 $ 75 $ 24 $ CRF $ Similarly , the syntactic constructions and the discourse structures of this component should correspond to the set of allowed constructions / structures in the CL . 
Portable_Information_Extraction $ 4 $ 77 $ 0 $ CRF $ For our Phase I feasibility demonstration , we chose a minimal scenario template for meeting and negotiation events consisting of one or more participant slots plus optional date and location slots . 
Background $ 2 $ 122 $ 102 $ KP $ The system can respond to the inform with a sub-dialogue : s Dialogue ( sys ) --~ ( request ( sys ) + Inform ( usr ) ) * Inform ( usr ) --+inform ( usr ) + [Dialogue ( sys ) ] The dialogue history reflects all previous negotiations ( here : task theatreinfo ) . 
BOV_Stem_NE_Defaults_Qspecific_41 $ 5 $ 53 $ 23 $ PATTERN $ The first of these is the " bag/TERM of/TERM verbs/TERM " ( BOV/ACR ) technique . 
Sample_Selection $ 2 $ 45 $ 17 $ PATTERN $ C/TERM is the/DEF current hypothesis ./O 
Previous_Schemes_for_Grunt $ 2 $ 47 $ 19 $ PATTERN $ First , there is the assumption that each grunt has some fixed meaning and some fixed functional role ( filler , back-channel , etc ) . 
_Head_countability_preferences_of_the_head $ 7 $ 61 $ 1 $ KP $ We anticipate that this is a useful feature because singular indefinite countable nouns normally take the article a/n , whereas singular indefinite uncountable nouns normally take no article : a dog vs water . 
Abstract $ 0 $ 161 $ 14 $ CRF $ When clauses are considered to be the elementary unit size , the predictions are correct in up to 81 % of the cases in which the algorithm makes a prediction -under the pre-condition that intra-sentential units axe handled first . 
See_the_report_entitled_ $ 3 $ 19 $ 9 $ PATTERN $ We describe preliminary work developing measures on system-internal components that assess : ( i ) the flow of words relevant to the filtering task and domain through the steps of document processing in our embedded MT system , and ( ii ) the level of " noise/TERM " i.e. , processing/DEF errors , passing through the system ./O 
Abstract $ 0 $ 216 $ 74 $ CRF $ Screen copies of the IG interface during an authoring process of a VIDAL notice are given on figures 1 and at a given stage . 
The_MATE_Approach $ 2 $ 54 $ 18 $ CRF $ Roughly 20 speaking , a coding module includes or describes everything that is needed in order to perform a certain kind of markup of spoken language corpora . 
Maximum_entropy-based_parse $ 2 $ 40 $ 11 $ CRF $ The statistical features used for parse selection should contain information pertinent to sentence structure , as it is the information encoded in these features which will be brought to bear in prefering one parse over another . 
_Measurements $ 3 $ 85 $ 18 $ CRF $ The first collection to be seen moving up the headline at a remove of two nodes ( the main verb and the vp ) is the conjunction of companies . 
The_machine_learning_method $ 3 $ 101 $ 42 $ PATTERN $ Here is the form of the positive examples : POSITiVE ( category_before_N , category_after . N , category_before_V , V_type , distance , position ) . 
Empirical_Evaluation $ 4 $ 150 $ 39 $ CRF $ Instead , kNN/TERM performs/DEF online scoring to find the training patterns that are nearest to a test pattern and makes the decision based on the statistical presumption that patterns in the same category have similar feature representations ./O 
OUT $ -1 $ 122 $ 71 $ CRF $ ILEX/TERM is an/DEF adaptive hypertext generation system , providing natural language descriptions for museum objects ./O 
Complexity_Formulas $ 4 $ 57 $ 1 $ KP $ This is common in information theory ( Ash , 1965 ) ; that is , when the user makes a statement , it must be encoded , and the number of bits needed to encode the statement is a measure of its information content . 
Abstract $ 0 $ 14 $ 12 $ PATTERN $ Consistency/TERM was measured/DEF by two means ./O 
The_Classifiers $ 3 $ 98 $ 61 $ KP $ Learning/TERM : Once/DEF the search ends , the weight vectors w~ and w~ are updated accordingly ./O 
Conceptualizing_Events $ 2 $ 142 $ 112 $ PATTERN $ In addition to the cascaded processes there is a concept lexicon , accessible via a concept matcher : these modules , which are called by the construction/TERM process/TERM , find/DEF best matches for structures that can either be subsumed by a more complex concept or may represent still incomplete concepts ./O 
Introduction $ 1 $ 13 $ 9 $ CRF $ So TRANSTYPE/TERM is a/DEF specialized text editor with an embedded Machine translation engine as one of its components ./O 
Introduction $ 1 $ 11 $ 4 $ CRF $ Classical/TERM dialogue/TERM systems/TERM like UC ( Wilensky et al . , 1984 ) utilized/DEF a formal language to represent knowledge , which requires the heavy cost of construction and maintenance and makes the scaling up quite difficult ./O 
_Instrumentalists_not_including_string_players $ 8 $ 87 $ 75 $ CRF $ In EWN , each monolingual database is linked , via CrossLanguage equivalence relations , to the InterLingual/TERM Index/TERM (/TERM ILI/TERM )/TERM which is the/DEF superset of all concepts occurring in all languages ./O . 
Methodology $ 2 $ 45 $ 6 $ PATTERN $ The reason for choosing it is that newspaper text is the most typical kind of reference corpus used by applied linguists , mainly because it is easy to get . 
System_Overview $ 3 $ 104 $ 68 $ CRF $ ( C3 ) Target/TERM grammar/TERM : Each/DEF tree in the set falls into one of the three types as specified in Section 3 . 1 ./O 
OUT $ -1 $ 12 $ 12 $ CRF $ In addition to these three themes , this year ' s workshop includes a special-theme session on • Principles for Evaluation of Dialogue Systems . 
Joining_TM2_and_DTD $ 3 $ 79 $ 0 $ KP $ TM2 specifically stores a type of translation segment class , which we have tagged <segl> , <seg2> . . . <segn> , <title> and <rs> , and which is relevant to the DTD . 
Abstract $ 0 $ 113 $ 18 $ KP $ In the Wall Street Journal texts , for example , the top two grammatical relations are Or ( object of transitive verb ) and PPN ( prepositional phrase complement of a NP ) . 
Abstract $ 0 $ 7 $ 6 $ PATTERN $ Since the central barrier to developing such a system today is the incompleteness of the knowledge base , we outline a strategy starting with the implementation of a series of form-based resolution algorithms that are applied directly to the referring expressions of the input text . 
Transformation_rule_lists $ 2 $ 35 $ 10 $ CRF $ An often-used method is the difference in performance resulting from applying the rule . 
Setting $ 3 $ 89 $ 10 $ CRF $ Table 1 contains information about the number of examples , the number of senses , and the percentage of the most/TERM frequent/TERM sense/TERM ( MFS/ACR ) of these reference words , grouped by nouns , verbs , and all 21 words . 
Generation_of_Multiple_Quantifiers $ 5 $ 192 $ 7 $ PATTERN $ In " Each patient is given a high severity rating " , performing universal quantification on the patients ( ARG3 ) is a separate decision from the existential quantification of the severity ratings ( ARG2 ) . 
OUT $ -1 $ 62 $ 44 $ KP $ The rules are applied to each sentence in the story , as well as the title of the story , with the exception that the title is not considered for WHY questions . 
Introduction $ 1 $ 9 $ 5 $ KP $ kNN and SVM have been reported as the top performing methods for English text categorization ( Yang and Liu , 1999 ) . 
Topic_Analysis $ 4 $ 91 $ 24 $ PATTERN $ The measures proposed include lapse supplementary budget and record public works spending in the firso half of ohe financial year . 
OUT $ -1 $ 136 $ 99 $ CRF $ As a result , languages whose syntactic structures deeply differ from the English ones may 30 present an additional level of complexity that makes mapping to/from UNL impossible or unrealistic . 
The_Generation_System $ 3 $ 110 $ 68 $ PATTERN $ The keyword SEQ/TERM specifies/DEF that what follows it is a list of words in their correct linear order ./O 
Given_an_input_space_X~*_of $ 3 $ 51 $ 20 $ CRF $ The parameters e and S have the following meaning : e/TERM is the/DEF probability that the learner produces a generalization of the sample that does not coincide with the target concept ,/O while S/TERM is the/DEF probability , given D , that a particularly unrepresentative ( or noisy ) training sample is drawn ./O 
_The_Problem $ 1 $ 55 $ 49 $ PATTERN $ We assume for present purposes that shoulder denotes a model-level category we can gloss as contributes-to-capitalization . 
Experimental_Design $ 3 $ 180 $ 14 $ PATTERN $ These measures may depend on none , one , or all of the collection of ground truth summaries {gj} . 
Construction_of_Features $ 3 $ 84 $ 16 $ PATTERN $ The position value posi_v of the ith word wi is calculated as pos _v = r × R] , where n/TERM is the/DEF number of words and/O R/TERM represents/DEF the number of regions in the sentence ./O 
_General_Outline_of_the_Method $ 2 $ 103 $ 87 $ CRF $ Their differences lie in that • MBL/TERM is a/DEF lazy learning algorithm that keeps all training data in memory ./O
Abstract $ 0 $ 6 $ 1 $ CRF $ We present such a component : a fast and robust morphological generator for English based on finite-state techniques that generates a word form given a specification of the lemma , part-of-speech , and the type of inflection required . 
Architecture_of_WIT-Based_Spoken $ 3 $ 78 $ 44 $ PATTERN $ Precisely speaking , the participant having the initiative is the one the system assumes has it in the dialogue . 
Content_Planning $ 1 $ 45 $ 20 $ CRF $ Hence , we built a two-stage statistical model of human-human dialogues using the CMU corpus . 
Topic_Analysis $ 4 $ 132 $ 65 $ PATTERN $ In the text in Figure 3 , the topic represented by seed-words ' trade-export-tariff-import ' is the main topic , and ' Japan-Japanese , ' ' Hong Kong , ' etc . , are subtopics . 
Dichotomizer_Ensembles $ 2 $ 53 $ 10 $ PATTERN $ Assuming a normal distribution of errors and bit values in every 2 bits-block , there is a 25% chance that both bits in a 2-bit block are wrong . 
Tree_Structures $ 2 $ 22 $ 6 $ KP $ The syntactic structure corresponding to the sentence " Hennessy will be a hard act to follow " is presented in Figure 1 as an example ( the syllable level has been omitted for clarity ) . 
Levels_of_Annotation $ 4 $ 135 $ 41 $ CRF $ Informally speaking , a dialogue/TERM act/TERM tag/TERM is a/DEF label belonging to a tag set which refers to a given iUocutionary dimension that may be performed by uttering a sentence ./O 
Introduction $ 1 $ 24 $ 16 $ KP $ Clearly , the performances of WSD systems are related to a variety of parameters , but the formal nature of these dependencies is not fully understood . 
OUT $ -1 $ 94 $ 76 $ KP $ Rule #2 rewards sentences that contain a recognized NAME , and rule #3 rewards sentences that contain the word " name " . 
Overview_of_the_Approach $ 2 $ 34 $ 14 $ PATTERN $ Finally , an nn . qpeci~ed object required as an argument to a predicate can appear elsewhere in the sentence , requiring the use of the predicate const ( X ,C ) to bind the variable X to the constant C . Some other database queries ( or training examples ) for the U . S . Geography domain are shown below : What is the capital of Texas?
DTD_abstraction $ 2 $ 52 $ 0 $ CRF $ SGML/TERM mark-up/TERM determines/DEF the logical structure of a document and its syntax in the form of a context-free grammar ./O 
OUT $ -1 $ 77 $ 76 $ PATTERN $ In contrast , if the subject is a comparison between two entities ( e . g . , v ( ed > v ( e_ , ) ) , the value of the subject for an objective o is [vo ( e9 Vo ( e , ) ] , and it is positive when it is greater than 0 ( negative otherwise ) . 
HMM_for_Cross-lingual_IR $ 3 $ 29 $ 0 $ KP $ For CLIR we extend the query generation process so that a document Dy written in language y can generate a query Qx in language x . 
The_Generation_System $ 3 $ 85 $ 43 $ PATTERN $ Nitrogen ' s input , Abstract/TERM Meaning/TERM Representation/TERM ( AMR/ACR ) , is a/DEF labeled directed graph written using the syntax for the PENMAN Sentence Plan Language (/O Penman 1989 ) . 
Current_approach $ 3 $ 84 $ 45 $ PATTERN $ The product f~w=~wTVk/TERM is the/DEF projection of ~T into the k-dimensional latent semantic space ./O 
OUT $ -1 $ 126 $ 89 $ KP $ The BP decoder , for example , is able to produce outputs whose literal meaning is preserved in most cases ( Martins et al . , 1998b ) , using handcoded UNL expressions . 
Results $ 8 $ 164 $ 3 $ CRF $ Concerning tense , our `` gold standard ' ' is the set of human translations , generated tense past present human past 134 17 translation present 17 27 Table 2 : Preliminary Tense Results previously constructed for these sentences . 
Results $ 3 $ 78 $ 24 $ PATTERN $ The next most affected phrase type is the ADJP , which can often be joined with or removed from the NP . 
Introduction $ 1 $ 18 $ 9 $ KP $ The markup will help disclose the underlying logical structure of documents . 
Grammar_Induction $ 3 $ 61 $ 0 $ KP $ The degree of difficulty of the task of learning a grammar from data depends on the quantity and quality of the training supervision . 
Embedding_Translation_in_an $ 6 $ 143 $ 41 $ CRF $ Also , at higher false rejection rates , the task performance is better for trigram-based translation model than the phrase trigram-based translation model since the precision of lexical choice is better than that of the phrase trigram-based model as shown in Table 3 . 
OUT $ -1 $ 66 $ 20 $ KP $ " The latter " words " may include any of the following : wordstrings with OCR-induced spelling changes ( valid or invalid for the specific language ) , wordstrings duplicating misspellings in the source document , and words accurately OCR-ed . 
Introduction $ 1 $ 32 $ 25 $ KP $ We use a phrase-based VNSA target language model to retrieve the most likely translation from the lattice . 
MALIN $ 4 $ 224 $ 118 $ PATTERN $ The answer U7 is a valid response to $6 and produces a new OPM , see figure 14 . 
_Pronominalise_the_Cb_only_after_a_Continue $ 4 $ 163 $ 124 $ PATTERN $ Of course we are not satisfied that this metric is the best ; an advantage of the generation approach is that different evaluation methods can easily be compared . 
OUT $ -1 $ 154 $ 100 $ PATTERN $ There is a number of important areas of research that ExtrAns and WebExtrAns , and by extension any AE system , has to focus on . 
Abstract $ 0 $ 4 $ 1 $ PATTERN $ An information/TERM structure/TERM consists/DEF of two components : HowNet definitions and dependency relations ./O 
_The_center_is_the_entity_which_is_most_likely $ 3 $ 28 $ 0 $ KP $ to be pronominalised : Grosz et al ' s " Rule 1 " in its weakest form states that if any entity is referred to by a pronoun , the Cb must be . 
The_machine_learning_method $ 3 $ 71 $ 12 $ CRF $ For our experiment , 3A semantic operation that converts an argument to the type which is expected by a function , where it would otherwise result in a type error . 
_Generation_of_the_surface_phrase_from_the $ 4 $ 124 $ 87 $ PATTERN $ LI : A summary describes about a trip by the Malay Railway , but the fare is not referred in it . 
OUT $ -1 $ 136 $ 94 $ CRF $ The second experiment compares the performances achieved by TBLDT and C4.5 training on samples selected by active learning . 
Comparing_Taggers $ 1 $ 8 $ 1 $ KP $ As a contextual clue , we might for instance assume that it is unlikely that a verb will follow an article . 
About_Theory_Refinement $ 2 $ 12 $ 4 $ PATTERN $ ( Mooney , 1993 ) defines it as : Theory/DEF refinement systems developed in Machine Learning automatically modify a Knowledge Base to render it consistent with a set of classified training examples ./O 
Related_Work $ 5 $ 122 $ 4 $ PATTERN $ Our model is the most related to Coccaro and Jurafsky ( 1998 ) , in that a reduced vector space approach was taken and context is represented by the accumulation of word cooccurrence vectors . 
_Features $ 3 $ 46 $ 23 $ CRF $ The mention/TERM is a/DEF child of a relative clause ./O 
Enriching_the_Feature_Set_with $ 2 $ 71 $ 1 $ PATTERN $ These measures are based on empirical counts of word occurrences and co-occurrences . 
Prosody_Prediction $ 4 $ 62 $ 0 $ KP $ The tree representations and the metrics can now be used to predict the prosody of a sentence . 
Experimental_Results $ 7 $ 169 $ 2 $ CRF $ We thus utilized Reuters news articles referred to as ' Reuters-21578 , ' which has been widely used in text classification v . We used a prepared SAn exception is the method proposed in ( McCallure and Nigam , 1999 ) , which , instead of labeled texts , uses unlabeled texts , pre-determined categories , and keywords defined by humans for each category . 
OUT $ -1 $ 150 $ 143 $ PATTERN $ GNote that , in c&~e the object marked for high interest is the person , a more abbreviated sentence construct iotl is appropriate : ' . losEP ARcos is in your neighbourh~md ' . 
Danish_Data $ 4 $ 123 $ 49 $ CRF $ Especially noticeable is the Danish use of discourse deictics in cases where elliptical constructions are normal in English . 
The_Acquisition_of_Word_Order $ 3 $ 111 $ 0 $ CRF $ We are investigating the acquisition of word order , which reflects the underlying order in which constituents occur in different languages . 
Systems_Foundations $ 3 $ 30 $ 6 $ KP $ MIMIC/TERM currently utilizes/DEF templatedriven text generation , and passes on text strings to a stand-alone TTS system ./O 
Statistics_Based_Hybrid_Approach_to $ 2 $ 27 $ 9 $ PATTERN $ The task is to find RC/TERM , a/DEF most possible sequence of duples formed by base phrase tags and boundary tags ,/O among the POS sequence T . RC = ( <ro , co > . . . . . . . . <rn , Cn> ) , in whil~h ri ( l <i< =n ) indicates the boundary tags , ci represents the base phrase tags . 
Abstract $ 0 $ 8 $ 2 $ KP $ idf , but the fact that there are so many variants of this formula in the literature suggests that there remains considerable uncertainty about these assumptions . 
Tree_Generalization_using_Tree-cut $ 2 $ 79 $ 33 $ PATTERN $ The best model is the one with the tree-cut [AIRCRAFT , ball , kite , puzzle] indicated by a thick curve in the figure . 
The_Feasibility_of_the_STL $ 3 $ 58 $ 16 $ PATTERN $ For instance , if a sentence does not have a relative clause , it will not express parameters that concern only relative clauses ; if it is a declarative sentence , it won ' t express the properties peculiar to questions ; and so on . 
The_MATE_Markup_Framework $ 3 $ 89 $ 1 $ PATTERN $ When a coding module has been applied to a corpus , the result is a coding file . 
Abstract $ 0 $ 17 $ 15 $ PATTERN $ Another view of this is the DARPA Translingual/TERM Information/TERM Detection/TERM , Extraction/TERM and Summarisation/TERM effort ( TIDES/ACR ) . 
Implementing_Embedded_MT $ 2 $ 94 $ 37 $ PATTERN $ Much 20 current CyberTrans work consists of developing and transitioning tools which can accurately detect and remediate errors , converting documents into a standard ( normalised ) form . 
Summarization_Filters $ 4 $ 93 $ 0 $ KP $ As mentioned above , create goals are satisfied by summarization filters , which create new media objects summarizing information sources . 
Word_Clustering $ 3 $ 50 $ 6 $ CRF $ 36 MDL/ACR ( Minimum/TERM Description/TERM Length/TERM ) principle/TERM is a/DEF model selection criterion which asserts that , for a given data sequence , the lower a model ' s SC value , the greater its likelihood of being a model which would have actually generated the data ./O 
Introduction $ 1 $ 77 $ 73 $ CRF $ We will also discuss how the backtracking can be partly avoided taking into account some properties of the algorithm , and using a minimum constraint propagation technique , 233 3 . 3 The semi-reeursivealgorithm • ( Danlos , 1996 ) emphasizes on the problems tied to the use of a recursive depth-first algorithm in the area of text generation . 
The_formula_is_valid_when_J_>_R_ ( that_is ,_the_judges $ 7 $ 97 $ 11 $ CRF $ Next , five judges had to indicate for each sentence which other sentence ( s ) , if any , it subsumes s . 5 . 1 CBSU : interjudge agreement Using the techniques described in Section 0 , we computed the cross-judge agreement ( J ) for the 6 clusters for various r ( Figure 3 ) . 
Conclusions_and_Future_Work $ 5 $ 144 $ 10 $ PATTERN $ To extract the corresponding syntax information of English Chinese bilingual corpus by shallow parsing is a direction for future work , also . 
OUT $ -1 $ 72 $ 46 $ CRF $ This is particularly worrisome in our case , since in our evaluation scenario the generated sentence is a permutation of the tokens in the reference string . 
Introduction $ 1 $ 21 $ 12 $ PATTERN $ The method was evaluated on the LFG grammar for French developed within the PARGRAM project ( Butt et al . , 1999 ) , but it is applicable to any unification grammar with a phrase-structure backbone where the reference treebank contains all possible analyses for each training example , along with an indication of which one is the correct one . 
Introduction $ 1 $ 10 $ 1 $ PATTERN $ Resolving the ambiguity of words is a central problem for large scale language understanding applications and their associate tasks ( Ide and V4ronis , 1998 ) , e . g . , machine translation , information retrieval , reference resolution , parsing , etc . 
Evaluation_Measures $ 2 $ 120 $ 95 $ PATTERN $ Since indicative summaries alert users to document content , any measure that evaluates the quality of an indicative summary ought to consider the similarity of the content of the summary to the content of the full document . 
Introduction $ 1 $ 11 $ 2 $ KP $ SGML is well established as the coding scheme underlying most Translation/TERM Memory/TERM based/TERM systems/TERM ( TMBS/ACR ) , and has been proposed as the cod-it~g scheme for the interchange of existing Translation Memory databases Translation Meinories eXchange , TMX ( Melby , 1998 ) . 
The_semantic_behavior_of_the $ 5 $ 100 $ 7 $ PATTERN $ That is , " MIKETTEI NO ( of indecision ) , " represents the situation of a problem . 
OUT $ -1 $ 60 $ 34 $ KP $ We employ two metrics that measure the accuracy of a generated string . 
The_Diversity_of_Semantic $ 2 $ 27 $ 3 $ CRF $ Here , we show several examples that demonstrate the diversity of the sel `` NO ' ' is a Japanese postpositiona| which can represent a wide range of semantic relations . 
Evaluation $ 3 $ 99 $ 8 $ KP $ Following Briscoe and Carroll ( 1997 ) , we calculated precision/TERM ( percentage/DEF of SCFS acquired which were also exemplified in the manual analysis )/O and recall/TERM ( percentage/DEF of the SCFs exemplified in the manual analysis which were acquired automatically )/O . 
Bridging_Natural_Language_and $ 4 $ 82 $ 3 $ CRF $ One of the most notable computational inadequacies of the finite-state model is the absence of a pushdown mechanism to suspend the processing of a constituent at a given level while using the same grammar to process an embedded constituent ( Woods , 1970 ) . 
Hyperonymy_in_lexical_semantics $ 5 $ 131 $ 34 $ PATTERN $ 2 • Given an activated concept , which more general lexical items are considered in tile choice process ; are there any restrictions on . -lexical inheritance-? . . . . . . . . . o How is the eventual choice from the set of candidate lexical items being made?
Results $ 3 $ 82 $ 28 $ CRF $ For VP , on the other hand , there is an accuracy increase , probably due to a corrected inclusion/exclusion of participles into/from NPs . 
Comparing_Three_Treebank $ 4 $ 108 $ 41 $ PATTERN $ Truly unmatched templates A truly/TERM unmatched/TERM template/TERM is a/DEF template that does not match any template in the other Treebank even if we assume both Treebanks are perfectly annotated ./O 
Examples_of_YAG_in_use $ 3 $ 84 $ 19 $ PATTERN $ SNePS/TERM is a/DEF semantic network processing system (/O Shapiro and Rapaport , 1992 ) . 
Abstract $ 0 $ 4 $ 2 $ KP $ GoDiS/TERM is a/DERF prototype dialogue system for information-seeking dialogue , capable of accommodating questions and tasks to enable the user to present information in any desired order , without explicitly naming the dialogue task ./O 
OUT $ -1 $ 166 $ 112 $ PATTERN $ Concerning answers , the principal measures for the AE task must be recall and precision , applied to individual answer sentences . 
The_Filtering_Problem $ 1 $ 4 $ 0 $ KP $ How do people quickly determine whether a particular foreign language text document is relevant to their interest when they do not understand that foreign language?
Introduction $ 1 $ 24 $ 15 $ PATTERN $ The rest of this paper is organized as follows : Section 2 describes the initial candidate grammar and the operators used to generate new candidate grammars from any given one . 
Introduction : $ 1 $ 21 $ 16 $ CRF $ The first is a sire• ple heuristic that approximates the grammar ' s uncertainty in terms of sentence lengths . 
Abstract $ 0 $ 17 $ 16 $ KP $ 1 phrase structure SOC Fig . 2 dependency structure I This work was mainly done while the author visited Kodensha Ltd , Japan during 1996 . -1999 78 Sentence-1 is a pivot sentence ( ~gd ' f~3 ) , i.e. , " ~1~ " is not only the object of " i ,W " butalso the subject of " Ik~ " . 
Introduction $ 1 $ 13 $ 5 $ KP $ This corpus was derived from the Singapore Primary School Chinese Language Textbooks . 
Explaining_Probabilistic_Methods $ 3 $ 54 $ 6 $ CRF $ This section defines a learning algorithm and a class of hypotheses with some generalization properties , that capture many probabilistic learning methods used in NLP . 
Analysis_module $ 2 $ 42 $ 2 $ CRF $ SUPAR/TERM is a/DEF computational system focused on anaphora resolution ./O 
Results_and_Discussion $ 5 $ 160 $ 29 $ PATTERN $ one is the direct hypernym of the other ) . 
Word_Clustering $ 3 $ 52 $ 8 $ PATTERN $ 2 For a fixed seed word s , we take a word w as a frequently co-occurring/TERM word/TERM if/DEF the presence of s is a statistically significant indicator of the presence of w ./O Let a data sequence : ( sl ,wl ) , ( s2 ,w2 ) , . . , ( Sin ,Win ) be given where ( si , wi ) denotes the state of co-occurrence of words s and w in the i-th text in the corpus data . 
Introduction $ 1 $ 16 $ 5 $ CRF $ A textual/TERM document/TERM is a/DEF sequence of terms ./O 
Acquisition_Process $ 4 $ 140 $ 41 $ PATTERN $ First , the algorithm checks whether the conflicting dictionary head word denotes an acronym ( e . g . 
Abstract $ 0 $ 6 $ 0 $ KP $ Long sentence analysis has been a critical problem because of high complexity . 
See_ ( Chu-Carroll_and_Carberry_1998 ) _tbr_an $ 4 $ 117 $ 8 $ PATTERN $ Task efficacy is the method we have adopted in our evaluation framework . 
The_REXTOR_System $ 5 $ 141 $ 19 $ PATTERN $ A relation/TERM rule/TERM takes the following form : EntityType/DEF : => <atoml atom2 acorn3> ;/O The EntityType/TERM is the/DEF trigger for the relation ,/O i.e. , the rule is applied whenever a string of that type is extracted . 
OUT $ -1 $ 179 $ 53 $ KP $ Text classification is therefore not a solution . 
Introduction $ 1 $ 6 $ 0 $ CRF $ Pustejovsky ( Pustejovsky , 1995 ) proposed the theory of a generative lexicon as a framework by which meanings of words are expressed in one unified representation . 
_Generation_of_the_surface_phrase_from_the $ 4 $ 139 $ 102 $ PATTERN $ Therefore , we introduce the relevance score , which represents the correspondence between the subject judgement and the correct document relevance . 
The_steps_in_the_strategy_are_marked_with_the $ 7 $ 135 $ 13 $ CRF $ ( Elhadad 1992 ) investigated a general computational framework that covers all aspects of generating evaluative arguments of single entities , from content selection and structuring to fine-grained realization decisions . 
Bridging_Natural_Language_and $ 4 $ 80 $ 1 $ CRF $ We argue that a finite-state model of natural language with ternary expressions is currently the most suitable combination for this task . 
Introduction $ 1 $ 12 $ 7 $ KP $ A keyword extraction method ( e . g . , that using tf-idf ( Salton and Yang , 1973 ) ) generally extracts from a text key words which represent topics within the text , but it does not conduct segmentation . 
Abstract $ 0 $ 1 $ 0 $ KP $ Discourse markers are complex discontinuous linguistic expressions which are used to explicitly signal the discourse structure of a text . 
Introduction $ 1 $ 41 $ 33 $ PATTERN $ Subjectivity is a property which is related to the attribution of authorship as well as to author stance , but it is just one of the dimensions we consider . 
Word_Clustering $ 3 $ 48 $ 4 $ PATTERN $ For a given data sequence z m = xl . . . zm and for a fixed probability model M , 1 the stochastic complexity of x m relative to M , which we denote as SC/TERM (/TERM x/TERM m/TERM :/TERM M/TERM )/TERM , is defined as the/DEF least code length required to encode x rn with M (/O Rissanen , 1996 ) . 
Language_Modeling_using $ 4 $ 107 $ 30 $ CRF $ The lower part of Table 1 shows the comparison results of the standard bigram model and the context language model . 
Introduction $ 1 $ 4 $ 0 $ KP $ Semantically annotated linguistic data are important resources for natural language processing , and have been used in many NLP areas , e . g . , parsing , word sense disambiguation , co-reference resolution and information extraction , etc . 
Evaluation $ 4 $ 83 $ 7 $ CRF $ The best heuristic according to 145 the precision is the maximum similarity heuristic . 
Introduction $ 1 $ 13 $ 8 $ KP $ A segmentation/TERM method/TERM ( e . g . , TextTiling ( Hearst , 1997 ) ) generally segments/DEF a text into blocks ( paragraphs ) in accord with topic changes within the text ,/O but it does not identify ( or label ) by itself the topics discussed in each of the blocks . 
Introduction $ 1 $ 11 $ 8 $ KP $ EVIUS/TERM is a/DEF component of a multilingual IE system ,/O MTURBIO ( Turmo et al . , 1999 ) . 
Abstract $ 0 $ 5 $ 4 $ CRF $ The user has the option of using one or more of these specific terms to reformulate the next round of searches . 
_Instrumentalists_not_including_string_players $ 8 $ 44 $ 32 $ PATTERN $ Next section describes a clustering strategy that adequates to the Information Retrieval criterion : cluster senses if they tend to co-occur in the same Semcor documents . 
Tagged_Text $ 3 $ 201 $ 187 $ CRF $ The network contains an input layer with two groups of features . 
Conclusion $ 6 $ 184 $ 17 $ KP $ For this DM input consists of grammatical structures , rather than sets of semantic objects . 
Introduction $ 1 $ 13 $ 7 $ KP $ Our CommandTalk dialogue system was designed for a highly specialized domain with little available data , so finding ways to build a robust system with * This research was supported by the Defense Advanced Research Projects Agency under Contract N66001-94-C-6046 with the Space and Naval Warfare Systems Center . 
Abstract $ 0 $ 18 $ 16 $ CRF $ TIDES/TERM represents/DEF the pinnacle of information access and is a real challenge for MT ./O 
Conclusions $ 6 $ 139 $ 2 $ CRF $ These sets typically tend to benefit from the Modified/TERM Value/TERM Difference/TERM Metric/TERM , which creates/DEF a condensed hyperspace of features ./O 
_The_co-reference_problem_in_summarization $ 4 $ 81 $ 57 $ PATTERN $ Following is a list of various methods of creating multi-document summaries by extraction : Find the important relevant parts that the cluster of documents have in common ( their intersection ) and use that as a summary . 
Algorithms_and_Implementation $ 2 $ 46 $ 26 $ PATTERN $ The only parameters that are available in the current version are the maximum number of iterations and a value frequency threshold which is set to 2 by default ( values occurring only once are not taken into account ) . 
Introduction $ 1 $ 7 $ 3 $ CRF $ In addition , there is a lack of publicly available Chinese corpus for evaluating Chinese text categorization systems . 
The_Refinement $ 5 $ 78 $ 22 $ PATTERN $ ( 3 ) [the_DT reawakening_VBG] of_IN [the_DT abortion-rights_NNS movement_NN] Generalisation/TERM consists/DEF of accepting some sequences of elements which do no correspond to a whole structure ./O  
The_NJFun_System $ 2 $ 82 $ 68 $ PATTERN $ Finally , " history/TERM " represents/DEF whether NJFun had trouble understanding the user in the earlier part of the conversation ( bad=0 , good=l ) ./O 
Abstract $ 0 $ 4 $ 3 $ CRF $ The tool uses a query language that allows to search for tokens , syntactic categories , grammatical functions and binary relations of ( immediate ) dominance and linear precedence between nodes . 
Transformation_rule_lists $ 2 $ 25 $ 0 $ CRF $ The central idea of transformation-based learning is to learn an ordered list of rules which progressively improve upon the current state of the training set . 
Abstract $ 0 $ 3 $ 2 $ PATTERN $ An evaluative argument is presented in the context of a decision task and measures related to its effectiveness are assessed . 
Issues_and_proposals $ 3 $ 122 $ 79 $ KP $ RST in its original formulation does not cover enveloping or parallel structures or conventional forms . 
Abstract $ 0 $ 3 $ 0 $ PATTERN $ We present two measures for comparing corpora based on information theory statistics such as gain ratio as well as simple term-class ~equency counts . 
Family_weights $ 2 $ 24 $ 3 $ PATTERN $ Examples of such rudimentary weighting schemes are the use of a weight of k!
Introduction $ 1 $ 15 $ 8 $ PATTERN $ which makes the crucial distinction between nucleus/TERM , which is the/DEF most important part of a message ,/O and satellite/TERM , which is the/DEF peripheral part of the message ./O 
The_results_reported_in_this_paper_are_part_of_a_more $ 1 $ 42 $ 31 $ CRF $ Both of them try to expand a `` basic-keyword/TERM ' ' , that is a/DEF keyword direcdy derived from a natural language question ./O 
Reading_Comprehension_Tests $ 2 $ 26 $ 8 $ CRF $ Quarc/TERM (/TERM QUestion/TERM Answering/TERM for/TERM Reading/TERM Comprehension/TERM )/TERM is a/DEF rule-based system that uses lexical and semantic heuristics to look for evidence that a sentence contains the answer to a question ./O 
Abstract $ 0 $ 38 $ 7 $ CRF $ ( 1998 ) describes a method based on text coherence which models text in terms of macro-level relations between clauses or sentences to help determine the overall argumentative structure of the text . 
Introduction $ 1 $ 29 $ 22 $ KP $ The interface is intended to be intuitive for users already familiar with keyword IR but adds the extra functionality of sortal , linguistic and positional constraints that are more common in IE . 
Social_Goals_and_Conversational $ 3 $ 44 $ 0 $ KP $ In this section , we exploit the framework described above to model the complex dynamics of goals and social preferences that underlies examples like [1] . 
Conclusions $ 6 $ 107 $ 2 $ CRF $ In this paper we proposed a corpus-based technique for specializing a grammar on a domain for which a treebank exists containing all trees returned for each sentence . 
Text_Summarization $ 1 $ 21 $ 1 $ CRF $ The informative evidence associated with techniques used in summarization may also provide clues for text categorization to determine the appropriate category of the document . 
OUT $ -1 $ 230 $ 40 $ PATTERN $ 25 The attribute vtype is mandatory , vtype/TERM is a/DEF reference to a description of a guideline violation in a file which contains the different kinds of violations of the individual guidelines ./O 
Using_Equal_Probability $ 5 $ 88 $ 5 $ CRF $ Under this assumption , the number of bits of information conveyed by referring to one entity out of v possible entities is log2v . 
Conclusion $ 5 $ 215 $ 3 $ KP $ As to retrieval performance , word-based IR systems can be superseded by sense-based ones using effective techniques that are able to identify and compare meanings or senses of words . 
Tagged_Text $ 3 $ 64 $ 50 $ PATTERN $ The input is the set of story sentences and questions , such that the words in each are tagged with POS tags and the names are marked with type and gender information . 
Maximum_Entropy_Modeling $ 2 $ 55 $ 13 $ CRF $ The expected value of feature fi with respect to the empirical distribution i~ ( x , y ) is expressed as x , y and the expected value of fi with respect to the probability distribution p ( ylx ) is p ( . fi ) -~ ~ ( x ) pCylx ) . h ( x , y ) , x~y where l~/TERM (/TERM x/TERM )/TERM is the/DEF empirical distribution of x in the corpus ./O 
ALLiS $ 2 $ 18 $ 13 $ CRF $ If the formalism is powerful enough , the main problem with XFST is the number of transducers generated by ALLiS . 
The_machine_learning_method $ 3 $ 84 $ 25 $ CRF $ This corpus has been POS-tagged with the help of annotation tools developed in the MULTEXT project ( Armstrong , 1996 ) ; sentences and words are first segmented with MtSeg ; words are analyzed and lemmatized with Mmorph ( Petitpierre and Russell , 1998 ; Bouillon et al . , 1998 ) , and finally disambiguated by the Tatoo/TERM tool/TERM , a/DEF Hidden Markov Model tagger (/O Armstrong et al . , 1995 ) . 
OUT $ -1 $ 3 $ 3 $ CRF $ The Workshop on Syntactic and Semantic Complexity in Natural Language Processing Systems , held on April 30th , 2000 at the Language Technology Joint Conference on Applied Natural Language Processing and the North American Chapter of the Association of Computational Linguistics ( ANLP-NAACL2000 ) was organized around the goals of discussing , promoting , and presenting new research results regarding the question of complexity as it pertains to the syntax and semantics of natural language . 
Implementing_Embedded_MT $ 2 $ 74 $ 17 $ PATTERN $ The implemented algorithm for language/code set identification is a trainable n-graph algorithm and has been discussed in more detail elsewhere ( Reeder & Geisler , 1998 ) . 
_Tagging_NuLL-marker_CDM_pairs_ ( via $ 8 $ 115 $ 45 $ PATTERN $ The order of these attributes is : CDM , F1 , F2 , B1 , B2 , Fcom , Boom Acorn for Null marker location , and CDM , F1 , F2 , B1 , B2 , Fcom , Bcom , IsRDM for CDM classification , where IsRDM/TERM is a/DEF Boolean value ./O 
Introduction $ 1 $ 25 $ 17 $ CRF $ We use a silnple criterion called domain dependency of words as a solution and present how the i . dea of domain dependency of words can be utilized effectively to identify a topic and an event : and thus allow multi-document summarization . 
Using_linguistic_constraints $ 3 $ 70 $ 10 $ CRF $ Head OK filters out rules ' , where the LHS has a head category which is not found on the RHS . 
Abstract $ 0 $ 4 $ 2 $ KP $ In this method , Hownet was used as our information source , and a co-occurrence frequency database of sememes was constructed and then used for WSD . 
OUT $ -1 $ 0 $ 0 $ KP $ A trainable method for extracting Chinese entity names and their relations Yimin Zhang & Zhou Joe F Intel China Research Center Kerry Center 6F1 No . 
Highlight $ 3 $ 142 $ 91 $ CRF $ This set is then passed to the IE component which deals with relational constraints such as same sentence , and interactions between constraints which have to be calculated on the fly such as precedes which are not indexed during preprocessing . 
POS_Assignment $ 3 $ 79 $ 19 $ PATTERN $ In our implementation , we limited the values of Cat to Noun , Verb and Adjective , since they are the main open class categories and therefore the POSes of most new words . 
Relevant_works $ 9 $ 217 $ 7 $ CRF $ Second , Czech/TERM language/TERM is a/DEF free word-order language what/O implies that the process of recognition of the verb group structure is much more difficult . 
Examples_of_YAG_in_use $ 3 $ 69 $ 4 $ CRF $ In this representation , M2/TERM is the/DEF proposition that the discourse entity B2 is a member of class dog ./O 
Abstract $ 0 $ 2 $ 1 $ KP $ Our system , SNS/TERM ( pronounced " essence " ) , retrieves/DEF documents related to an unrestricted user query and summarizes a subset of them as selected by the user ./O 
Abstract $ 0 $ 6 $ 5 $ KP $ Retrieval effectiveness is examined utilizing query weight ratio of these three categories of phrasal terms . 
Discourse_coherence_and $ 1 $ 9 $ 1 $ KP $ In the theory of discourse structure developed by Grosz and Sidner ( 1986 ) , each discourse segment exhibits two types of coherence : local coherence among utterances inside the segment , and global coherence between this segment and other discourse segments . 
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 98 $ 63 $ PATTERN $ Then : we hypothesize that if word i is a 33 ( H ) . x x I . ~e . nt !e_ve[ . 
Abstract $ 0 $ 4 $ 0 $ KP $ Word order and accent placement are the primary linguistic means to indicate focus/background structures in German . 
Introduction $ 1 $ 30 $ 24 $ CRF $ The bedroom window was broken and broken glass was found inside the window . 
Tagged_Text $ 3 $ 88 $ 74 $ KP $ NPs have the feature types : Base/TERM ( the/DEF root word of the head word of the NP )/O , AG/TERMR ( number/person/DEF information )/O , SemType/TERM ( the/DEF semtype of the root form in the lexicon , e . g . , person , object , event , artifact , organization )/O , Label/TERM ( the/DEF role type of the word in the sentence , e . g . , subject )/O , and Gender . 
Building_Spoken_Dialo~te_Systems $ 4 $ 120 $ 15 $ CRF $ opt/TERM means/DEF an option and or means a disjunction ./O 
Search $ 2 $ 59 $ 17 $ KP $ The retrieval status value is shown in a bold black font after the URL title . 
Background $ 2 $ 85 $ 65 $ KP $ informValue/TERM (/TERM p=v/TERM )/TERM : user/DEF provides value v for parameter p . p was requested ./O 
Lexical_and_Syntactic_Closure $ 3 $ 29 $ 1 $ KP $ Moreover , while Chinese tokens can concatenate , Chinese has no extensive morphology like many Indo-European languages . 
The_Structure_of_a_Relational $ 3 $ 91 $ 17 $ PATTERN $ A link/TERM file/TERM consists/DEF of two columns only , one identifying the entity , the other identifying the filler (/O the name of the attribute is provided in the first line of the file , see figure 3 ) . 
Further_Research $ 6 $ 193 $ 20 $ PATTERN $ KSD is a long neglected area of research .  
Maximum_entropy-based_parse $ 2 $ 30 $ 1 $ PATTERN $ In the present approach , parses are ranked according to their goodness by a statistical model built using the maximum/TERM entropy/TERM technique/TERM , which involves/DEF building a distribution over events which is the most uniform possible , given constraints derived from training data ./O 
Discussion $ 5 $ 204 $ 28 $ CRF $ Such evaluations are costly , and they can not be the basis of work in stochastic generation , for which evaluation is a frequent step in research and development . 
Introduction $ 1 $ 28 $ 18 $ CRF $ Based on the assumption that most concepts and conceptual structures of the domain as well the company terminology are described in documents , applying knowledge acquisition from text for ontology design seems to be promising . 
Introduction $ 1 $ 29 $ 13 $ PATTERN $ Section 5 describes two further experiments very briefly . 
Abstract $ 0 $ 65 $ 4 $ PATTERN $ In the concrete case , this model is a grammar ; LEXICAL/TERM semantics/TERM determines/DEF the separate constraints that can go into a description and/O COMPOSITIONAL/TERM semantics/TERM determines/DEF how these constraints can share variables and so describe common objects ./O 
BOV_Stem_NE_Defaults_Qspecific_41 $ 5 $ 99 $ 69 $ CRF $ Also , the parse trees served as the language for describing very question specific techniques , such as the ones for `` where ' ' questions presented in the previous section . 
_The_co-reference_problem_in_summarization $ 4 $ 52 $ 28 $ CRF $ When a group of three people created a multi-document summarization of 10 articles about the Microsoft Trial from a given day , one summary focused on the details presented in court , one on an overall gist of the day ' s events , and the third on a high level view of the goals and outcome of the trial . 
Conclusion $ 7 $ 197 $ 8 $ PATTERN $ Returning to the question posed at the beginning of the paper what is the appropriate size of elementary discourse units the answer is twofold : first of all , coherence relations can be found to hold between phrases and the clause containing them , so one should indeed start looking for discourse units at the phrase level . 
The_WPDV_algorithm $ 1 $ 7 $ 0 $ PATTERN $ Weighted/TERM Probability/TERM Distribution/TERM Voting/TERM ( WPDV/ACR ) is a/DEF supervised learning approach to classification ./O 
The_semantic_behavior_of_the $ 5 $ 104 $ 11 $ PATTERN $ " KUMORI/TERM ( cloudiness ) " is a/DEF natural phenomenon which can be pointed to concretely ./O 
Analyzing_the_Reading $ 4 $ 137 $ 38 $ PATTERN $ the first answer among the list of possible answers for each question is the correct one ) . 
Stochastic_Topic_Model $ 2 $ 40 $ 9 $ KP $ The STMs within a text are assumed to have the same set of topics , but have different parameter values . 
Evaluation $ 3 $ 131 $ 40 $ KP $ The unconditional distribution obtained from the observed distribution of SCFs in the 20 M words of BNC is shown in figure 2 . 
Reordering_the_output $ 5 $ 100 $ 8 $ CRF $ The most/TERM likely/TERM string/TERM in the word lattice is then decoded/DEF as follows : ^ W~ = argmax ( ~T o ~WT ) = arg max P ( ~VT I ) ~T ) (/O 6 ) Where o/TERM is the/DEF composition operation defined for weighted finite-state machines (/O Pereira and Riley , 1997 ) . 
_Annotation_Guidelines_I : $ 3 $ 59 $ 13 $ PATTERN $ S/TERM is the/DEF start symbol )/O .
Corpus_comparison_based_on $ 6 $ 156 $ 47 $ CRF $ On the other hand , as the coverage for class features pairs increases , so does the part of the test set that is covered with the given feature set . 
Shortcomings $ 5 $ 140 $ 1 $ PATTERN $ Although the measure is a big step up from the measures used earlier , it has a number of shortcomings . 
Reconstructing_Centering_for_NLG $ 2 $ 21 $ 1 $ CRF $ The center/TERM in/TERM an/TERM utterance/TERM Un/TERM is the/DEF most grammatically salient entity realised in U~_i which is also realised in Un ./O 
The_Structure_of_a_Relational $ 3 $ 86 $ 12 $ CRF $ We require that each entity record provides a type for the entity in a field labelled Class . 
Introduction $ 1 $ 9 $ 2 $ KP $ Although keywords may offer some indication of " meaning , " they alone cannot capture the richness and expressiveness of natural language . 
Introduction $ 1 $ 10 $ 3 $ KP $ " 1 In Korean documents , compound nouns are represented in various forms ( shown in Table 1 ) , so there is a difficulty in indexing all types of compound nouns . 
Abstract $ 0 $ 64 $ 8 $ KP $ Many NLG modules have to be sensitive to a number of levels at once ( consider , for . . . . . . . . . . instance , -aggregatiomxeferring ,expmssion . ,generation and lexicalisation , all of which need to take into account rhetorical , semantic and syntactic constraints ) . 
OUT $ -1 $ 55 $ 13 $ CRF $ Soft decision-making is also useful when the system is one of the components in a/DEF larger decision-malting process ,/O as is the case in speech/TERM recognition/TERM systems/TERM ( Bald et al . , 1989 ) , or in an/DEF ensemble system like/O AdaBoost/TERM ( Freund and Schapire , 1997 ) . 
Conclusion $ 4 $ 175 $ 0 $ CRF $ We presented a new approach to content aggregation in the context of a very challenging and practical generation application : summarizing OLAP and data mining discoveries 13t as a few linked web pages of fluent and concise natural language . 
The_TABULATE_ILP_Method $ 3 $ 66 $ 0 $ KP $ This section discusses the ILP method used to build a committee of logical control hypotheses for each action . 
OUT $ -1 $ 197 $ 179 $ KP $ First , semantic classes were extremely useful for WHO , WHEN , and WHERE questions because they look for descriptions of people , dates , and locations . 
Validating_each_tagger_into_its_respective $ 1 $ 25 $ 1 $ KP $ The first system is specialised for tagging medical texts ( Ruch and al . , 2000 ) , while the second is a general parser ( based on FIPS , cf . 
The_CGS_system $ 4 $ 72 $ 4 $ PATTERN $ The input to CGS is a picture representation ( graphical elements and its mapping from the data set ) generated by SAGE plus its complexity metric . 
Middle $ 6 $ 65 $ 28 $ PATTERN $ Low/TERM frequency/TERM denotes the/DEF number of occurrences less than 100 ,/O middle/TERM frequency/TERM denotes the/DEF number of occurrences between 100 and 1000 ,/O and high/TERM frequency/TERM denotes the/DEF number of occurrences more than 1000 ./O 
Validating_each_tagger_into_its_respective $ 1 $ 36 $ 12 $ PATTERN $ In parallel , we chose three types of medical texts to make up the medical/TERM corpus/TERM : it/DEF represents 16024 tokens , with 3 equal thirds : discharge summaries , surgical reports , and laboratory or test results ( in this case , tables were removed ) ./O 
MALIN $ 4 $ 129 $ 23 $ PATTERN $ Figure 3 shows an example OPM which represents the request Which bus lines passes the North gate ? . 
Concepts $ 2 $ 59 $ 34 $ PATTERN $ The objective/TERM function/TERM is defined as the/DEF sum of the code length for the model ( " model description length " ) and that for the data ( " data description length " ) ./O 
Approach_for_Chunk_Identification $ 3 $ 27 $ 0 $ KP $ The chunks in the CoNLL-2000 shared task are represented with IOB based model , in which every word is to be tagged with a chunk label extended with I/TERM ( inside/DEF a chunk )/O , O/TERM ( outside/DEF a chunk )/O and B/TERM ( inside/DEF a chunk , but the preceding word is in another chunk )/O . 
Collaborative_Agents $ 1 $ 68 $ 64 $ CRF $ AGENT : `` There is a conflict of meeting with Brian at three P . M . Thursday with meeting with Irene Landoz at three P . M . 
Introduction $ 1 $ 19 $ 15 $ PATTERN $ Section 2 describes our choice of the feature selection and extraction methods . 
Introduction $ 1 $ 9 $ 0 $ KP $ Spam/TERM , or more properly Unsolicited/TERM Commercial/TERM E/TERM-mail ( UCE/ACR ) , is an/DEF increasing threat to the viability of Internet E-mail and a danger to Internet commerce ./O 
Statistics_Based_Hybrid_Approach_to $ 2 $ 24 $ 6 $ PATTERN $ Definition 3 : Boundary/TERM tag/TERM denotes the/DEF possible relative position of a word to a base phrase ./O 
_Relations_that_are_relevant_and_correct_in $ 3 $ 38 $ 4 $ PATTERN $ Therefore , a WordNet synset containing n terms defines ~11 k synonym relations . 
Empirical_Evaluation $ 4 $ 128 $ 17 $ PATTERN $ Precision/TERM ( P/ACR ) is the/DEF percentage of the predicted documents for a given category that are classifted correctly ./O 
Conclusion_&_Future_Work $ 7 $ 124 $ 2 $ PATTERN $ In Our future work includes : 1 ) Because the sparsity of collocations is a main factor of affecting the word clustering accuracy , we can use the clustering results to discover new data and enrich the thesaurus . 
Dialog_Management $ 3 $ 75 $ 14 $ PATTERN $ If the ACCUiVLVALUE of a user crosses a threshold , the accumulated user expertise level changes long term as it is assumed that there is a change in the user ' s overall understanding of the solution . 
_The_Pentagon_has_agreed_to_send_57 : 000_blankets $ 6 $ 268 $ 233 $ CRF $ ' Rec/TERM ' denotes the/DEF demonstrate~ that the criterion , domain dependency ratio of the documents judged YES that were also of words effectively employed ,/O i evaluated as YES , and Tree/TERM is the/DEF percent of the documents that were evaluated as YES which corretion Tradeoff ./O 
Optimizations $ 5 $ 88 $ 3 $ PATTERN $ Define GoodPotential/TERM 0 to I ( S ) as the/DEF number of sentences s in the training corpus for which Guess[s]=0 , Truth[s]= 1 and 3k : ( s , k ) ~ corpus_position_set ( S ) ./O 
EXOT $ 9 $ 91 $ 19 $ PATTERN $ Since our basic query unit is a paragraph , document/TERM frequencY/TERM ( df/ACR ) and inverse/TERM document/TERM frequency/TERM ( idf/ACR ) have to be redefined . 
Related_Work $ 4 $ 122 $ 31 $ PATTERN $ The key parameter affecting the level of offence is the cost 13 of the requested actions : the less the cost of the requested action , the greater the offence ; this follows the principle that low-cost actions cannot be refused ( Goffman , 1967 ) , and , if they are , requesters get offended . 
POS_Assignment $ 3 $ 61 $ 1 $ KP $ This is required for sentence analysis where every word in the sentence must have at least one POS .  
Future_Research_Issues $ 5 $ 140 $ 18 $ PATTERN $ There is a need to study , along with learning and knowledge representation , inference methods that suit this framework ( KR97 ) . 
OUT $ -1 $ 0 $ 0 $ KP $ Planning Word-order Dependent Focus Assignments* Cornelia Endriss and Ralf Klabunde University of Heidelberg Center for Computational Linguistics Karlstr . 
The_lexicon_size_of_a_typical_large-vocabulary $ 9 $ 97 $ 18 $ CRF $ This is a Chinese to English translation resource that was manually compiled by a team of linguists from more than 250 text sources , including special and general-purpose print dictionaries , and other text sources such as newspapers . 
Experiments $ 5 $ 153 $ 1 $ CRF $ We used the `` short query ' ' condition of the NACSIS/TERM NTCIR-1/TERM Test/TERM Collection/TERM ( Kando et al . , 1999 ) which consists/DEF of about 300,000 documents in Japanese , plus about 30 queries with labeled relevance judgement for training and 53 queries with relevance judgements for testing ./O 
Evaluation_Measures $ 2 $ 99 $ 74 $ PATTERN $ These inherent weaknesses in recall-based measures will be further explored in Section 4 . 
_User : _Ok $ 9 $ 215 $ 29 $ CRF $ Our future work includes conducting evaluation of the hypotheses and the system and investigating machine learning techniques for improving utility adjustments . 
Examples $ 5 $ 144 $ 2 $ PATTERN $ Text preceded by USER represents spoken utterances from the user . 
System_Overview $ 3 $ 38 $ 2 $ CRF $ The core of LexTract is an extraction/TERM algorithm/TERM that takes/DEF a Treebank sentence such as the one in Figure 5 and produces the trees ( elementary trees , derived trees and derivation trees ) such as the ones in Figure 3 ./O 
Implementing_Embedded_MT $ 2 $ 120 $ 63 $ CRF $ A good translation engine has a lexicon in the tens of thousands of entries which takes time to load up . 
Abstract $ 0 $ 3 $ 0 $ KP $ A computational/TERM framework/TERM is presented which is used/DEF to model the process by which human language learners acquire the syntactic component of their native language ./O 
Evaluation $ 4 $ 79 $ 3 $ PATTERN $ We also define ' coverage/TERM ' as the/DEF proportion of linked senses of Korean words to all the senses of Korean words in a test set ./O 
_Background $ 1 $ 34 $ 19 $ KP $ CGUs/TERM , which represent/DEF grounding at the ' illocutionary level ' (/O Clark 1996 ) , have been proposed as a/DEF meso-level dialogue structure roughly the same level that dialogue games ( Carletta et al , 1997 ) or adjacency pairs (/O eg . 
Statistical_Semantic_Parsing $ 4 $ 103 $ 4 $ CRF $ Suppose our learned parser has n different parsing actions , the ith/TERM action/TERM a/ is a/DEF function a/ ( s ) : ISi -+ OSi where ISi G S is the set of states to which the action is applicable and OSi C_ S is the set of states constructed by the action ./O 
Word_Sense_Dis~mbiguation $ 4 $ 198 $ 106 $ CRF $ This step tries to identify words from SAW which are linked at a distance of maximum 1 with the words from SDW . 
Robustness $ 4 $ 92 $ 1 $ KP $ Before the semantic representation is handed to microplanning , the robustness preproeessing module of the generator checks the input , inspecting its parts for known problems . 
Informational_content_of_sentences $ 2 $ 43 $ 19 $ PATTERN $ Maximal/TERM marginal/TERM relevance/TERM ( or MMR/ACR ) is a/DEF technique similar to CSIS and was introduced in ( Carbonell and Goldstein , 1998 ) ./O 
_Final_Observations $ 5 $ 132 $ 4 $ PATTERN $ More interesting is the fact that some node transitions will certainly be different from others in their practical implementation and this should probably be factored into the cost calculation . 
ADAM : _Architectural_Principles $ 3 $ 85 $ 62 $ PATTERN $ However , at the best of our knowledge ADAM/TERM is the/DEF first corpus being architecturally designed by explicitly adopting the concept of annotation modularity and metascheme at different levels ./O 
Introduction $ 1 $ 25 $ 18 $ PATTERN $ The communication/TERM channel/TERM consists/DEF of the trained classifier ./O 
Abstract $ 0 $ 3 $ 2 $ KP $ It integrates lexical , textual and world knowledge into a single hierarchical framework . 
OUT $ -1 $ 109 $ 72 $ PATTERN $ 28 The UNL/TERM system/TERM architecture/TERM consists/DEF of two main processes , the encoder and decoder , and several linguistic resources , each group of these corresponding to a NL embedded in the system ,/O as depicted in Figure 3 . 
Introduction $ 1 $ 94 $ 90 $ PATTERN $ In fact , the main idea of the semi-recursive algorithm is the separated l St-order relations LB approaches are not adapted to text generation , where lexical choices must be done in a global , holistic perspective ( Danlos , 1998 ) and ( Busemann , 1993 ) . 
Introduction $ 1 $ 4 $ 0 $ PATTERN $ Issues of evaluation have been pre-eminent in MT since its beginning , yet there are no measures or metrics which are universally accepted as standard or adequate . 
Determination_Schemes_of $ 4 $ 124 $ 9 $ KP $ with safe segmentation accuracy = ~ of actually segmented Sent . 
_Introduction $ 1 $ 21 $ 14 $ CRF $ Hownet/TERM is a/DEF knowledge base which was released recently on Intemet ./O 
Dialogue_examples $ 4 $ 207 $ 0 $ PATTERN $ The example represents a dialogue where the computer plays A ' s role and is implementing the tactic of enticement . 
Evaluation $ 4 $ 135 $ 31 $ KP $ As Levin does not classify verbs on basis of their sentential complement-taking properties , more classification work is required before we can obtain accurate SCF estimates for this type of verb . 
Experimental_Results $ 4 $ 211 $ 12 $ CRF $ When two evaluation measures produce nearly the same ranking of the summary set , the rank correlation will be near 1 and a scatterplot of the two rankings will show points nearly lying on a line with slope 1 . 
Learning_Algorithms_Tested $ 2 $ 40 $ 20 $ CRF $ When classifying a new example , SNoW/TERM is similar/DEF to a neural network which takes the input features and outputs the class with the highest activation ./O 
Mapping_a_document_collection_into $ 2 $ 120 $ 73 $ PATTERN $ The color intensity of the dot also represents the degree of relevance . 
Problem_Space_Modeling $ 2 $ 50 $ 23 $ KP $ Reward and punishment can be predetermined and then re-adjusted later when the user and the group modeling progresses . 
Using_CST_for_information_fusion $ 5 $ 155 $ 18 $ PATTERN $ In the example , the shaded area represents the summary/TERM subgraph/TERM G " of G that contains/DEF all four cross-document links and only these nodes and edges of G which are necessary to preserve the textual structure of G ' ./O 
_Proposed_method $ 2 $ 47 $ 17 $ PATTERN $ Accordingly , with an appropriate optimization function over the distance measures between all the senses of the two words , sense #2 for bank and sense # 1 for shore are assigned as the correct tags for the words , respectively . 
OUT $ -1 $ 191 $ 120 $ KP $ The first is query expansion replacing words in the query with a set of words of the same meaning . 
The_Structural_Triggers_Learner $ 2 $ 36 $ 6 $ CRF $ This is an important open research question which is the focus of a recent research endeavor here at CUNY . 
Results $ 3 $ 76 $ 24 $ CRF $ The rightmost column gives the total number of values times the number of classes . 
Abstract $ 0 $ 32 $ 2 $ PATTERN $ Since a primary goal of annotated corpora is to serve as the empirical base of linguistic investigations , it is desirable to annotate structure divisions that are the most commonly shared among theories . 
Abstract $ 0 $ 2 $ 0 $ KP $ We introduce CST/TERM (/TERM cross-document/TERM slructure/TERM theory/TERM )/TERM , a/DEF paradigm for multidocument analysis ./O 
Conclusion $ 5 $ 248 $ 0 $ CRF $ We have presented a system for grammar extraction that produces an LTAG from a Treebank . 
Abstract $ 0 $ 30 $ 5 $ PATTERN $ I believe that the strategy followed by ES99 is a good starting point for investigating how far one can go in resolving individual and abstract anaphors in dialogues on the basis of the local contexts in which the anaphors occur . 
Cross-corpora_experiments : $ 6 $ 144 $ 9 $ CRF $ There is a worrying proportion of contradicting collocations . 
Information_Structures $ 3 $ 82 $ 5 $ PATTERN $ Furthermore , the ' transport " event is a ' crime ' and the manner is ' secret ' . 
System_Overview $ 2 $ 28 $ 2 $ CRF $ 2 The Tree/TERM Chooser/TERM uses/DEF a stochastic tree model to choose syntactic properties ( expressed as trees in a Tree Adjoining Grammar ) for the nodes in the input structure ./O 
Conclusion $ 7 $ 367 $ 0 $ CRF $ In this paper , we proposed a method for extracting key paragraph for summarization based on distinction between a topic and an event . 
Abstract $ 0 $ 20 $ 17 $ PATTERN $ All of these are the research fields of phase . 
Related_works $ 4 $ 177 $ 13 $ PATTERN $ 9,124 compounds are extracted from the corpus consists of 74,404 words , with the precision of 47.43% . 
Context_Distributions $ 3 $ 22 $ 1 $ PATTERN $ that similar words occur in similar contexts , I formalise this in a slightly different way : each word defines a probability/TERM distribution/TERM over/TERM all/TERM contexts/TERM , namely the/DEF probability of the context given the word ./O 
Learning_validation_and_results $ 4 $ 147 $ 9 $ CRF $ We have first defined a measure of the theoretical generality of the clauses 16 . 
OUT $ -1 $ 87 $ 45 $ KP $ Since a decision tree does not place all the samples with the same current label into a single equivalence class , it does not get stuck in the same situation as a rule list m in which no change in the current state of corpus can be made without incurring a net loss in performance . 
Accommodation_in_GoDiS $ 3 $ 56 $ 5 $ PATTERN $ But it can be figured out since J knows that this is a relevant question . 
Dialogue_manager $ 6 $ 174 $ 66 $ KP $ The KU with the best certainty score and the KUs with the 90% or larger certainty score are called candidate KUs . 
Implementation $ 3 $ 140 $ 22 $ CRF $ The hotel/TERM Ariadne/TERM is a/DEF cheap hotel in the city centre ./O 
Comparing_Three_Treebank $ 4 $ 70 $ 3 $ CRF $ ( a ) Frequency ( b ) Coverage Figure 5 : Etree template types and template tokens in the Penn English Treebank ( X-axes : ( a ) and ( b ) template types Y-axes : ( a ) log frequency of templates ; ( b ) percentage of template token covered by template types ) from the three Treebanks . 
Approach $ 2 $ 32 $ 26 $ KP $ Unlike the voting algorithms , the classifiers do not require a uniform input . 
_Introduction $ 1 $ 9 $ 3 $ CRF $ Penn Treebank has enabled and motivated corpus and computational linguistic research based on information extractable from structurally annotated corpora . 
OUT $ -1 $ 100 $ 58 $ PATTERN $ The data used in all of these experiments is the CoNLL-2000 phrase chunking corpus ( CoNLL , 2000 ) . 
Generation_and_linguistic_representation $ 4 $ 79 $ 7 $ CRF $ Once the user ' s words have been interpreted , a layer of production rules constructs obligations for response ( Traum and Allen , 1994 ) ; then , a second layer plans to meet these obligations by deciding to present a specified kind of information about a specified object . 
Concepts $ 2 $ 55 $ 30 $ KP $ Thus these clusters of adjectives have great possibility to be combined into one cluster , while the ordinary hierarchical clustering algorithm can not do it . 
Learning_Algorithms_Tested $ 2 $ 21 $ 1 $ KP $ Naive/TERM Bayes/TERM is intended as a/DEF simple representative of statistical learning methods ./O 
Identifying_Phrase_Structure $ 1 $ 9 $ 1 $ PATTERN $ Given an input string O =< ol , 02 , . . . , On > , a phrase/TERM is a/DEF substring of consecutive input symbols oi , oi+l , . . . ,oj ./O 
Hyperonymy_in_lexical_semantics $ 5 $ 129 $ 32 $ CRF $ We thereby open the door to both ' vertical ' and ' horizontal ' lexical choice within a hierarchy , which raises a number of questions : * What is the granularity of conceptual , and that of lexical knowledge ?
Abstract $ 0 $ 17 $ 5 $ KP $ Novice users may have a specific information topic , but due to little or no training in search and retrieval , they don ' t know how to make best use of the available operators and tools . 
Abstract $ 0 $ 1 $ 0 $ CRF $ We address the issue of ' topic/TERM analysis/TERM , ' by which is determined a text ' s topic structure , which indicates/DEF what topics are included in a text , and how topics change within the text ./O 
System_Design $ 3 $ 56 $ 5 $ CRF $ Extraction/TERM Pattem/TERM Library/TERM -which contains/DEF the set of extraction patterns learned in the lab , one set per scenario template -to extract specific types of information from the input Korean documents , once parsed ./O 
Pre-processing_design $ 2 $ 55 $ 20 $ PATTERN $ The next section describes the pipeline up to tagging . 
Introduction $ 1 $ 12 $ 3 $ CRF $ A non-restrictive/TERM component/TERM gives/DEF additional information to a head that has already been viewed as unique or as a member of a class that has been independently identified , therefoee is not ' essential for the identification of the head ' (/O Quirk et al . , 1985 ) . 
Abstract $ 0 $ 132 $ 71 $ CRF $ Meanings of referring expressions should therefore appeal to a condition @ p which describes F iff there is a C for which @ cP describes F . Clearly , if @ p describes F and @ p describes F ' then @ p describes FU I ' ~ . 
Why_Reading_Comprehension $ 2 $ 55 $ 1 $ CRF $ Each story comes with a set of questions about information that is stated or implied in the text . 
Introduction $ 1 $ 11 $ 7 $ KP $ A baseline algorithm for Word Domain Disambignation is presented and then compared with a mutual/TERM help/TERM disambignation/TERM strategy/TERM , which makes/DEF use of the shared senses of parallel bilingual texts ./O 
MALIN $ 4 $ 175 $ 69 $ KP $ Domain Knowledge Management in general involves three steps . 
PAR_as_anIL $ 4 $ 26 $ 15 $ PATTERN $ Motion/TERM is a/DEF type of framing event where the path is in the main verb for VFLs and in the satellite for SFLs ./O 
Concepts $ 2 $ 57 $ 32 $ CRF $ According to MDL , the best/TERM probability/TERM model/TERM for a given set of data is a/DEF model that uses the shortest code length for encoding the model itself and the given data relative to it ./O 
